



<!DOCTYPE html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
        <link rel="canonical" href="https://johngiorgi.github.io/mathematics-for-machine-learning/linear_algebra/week_5/">
      
      
        <meta name="author" content="John Giorgi">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-3.0.4">
    
    
      
        <title>Week 5 - Mathematics for Machine Learning</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/application.451f80e5.css">
      
        <link rel="stylesheet" href="../../assets/stylesheets/application-palette.22915126.css">
      
      
        
        
        <meta name="theme-color" content="#ef5350">
      
    
    
      <script src="../../assets/javascripts/modernizr.1aa3b519.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../assets/fonts/material-icons.css">
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="red" data-md-color-accent="teal">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="../../#week-5-eigenvalues-and-eigenvectors" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://johngiorgi.github.io/mathematics-for-machine-learning/" title="Mathematics for Machine Learning" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                Mathematics for Machine Learning
              </span>
              <span class="md-header-nav__topic">
                Week 5
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  


  <a href="https://github.com/JohnGiorgi/mathematics-for-machine-learning/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      JohnGiorgi/mathematics-for-machine-learning
    </div>
  </a>

          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://johngiorgi.github.io/mathematics-for-machine-learning/" title="Mathematics for Machine Learning" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    Mathematics for Machine Learning
  </label>
  
    <div class="md-nav__source">
      


  


  <a href="https://github.com/JohnGiorgi/mathematics-for-machine-learning/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      JohnGiorgi/mathematics-for-machine-learning
    </div>
  </a>

    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../.." title="About" class="md-nav__link">
      About
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2" checked>
    
    <label class="md-nav__link" for="nav-2">
      Linear Algebra
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        Linear Algebra
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../course_resources/" title="Course Resources" class="md-nav__link">
      Course Resources
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_1/" title="Week 1" class="md-nav__link">
      Week 1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_2/" title="Week 2" class="md-nav__link">
      Week 2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_3/" title="Week 3" class="md-nav__link">
      Week 3
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_4/" title="Week 4" class="md-nav__link">
      Week 4
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Week 5
      </label>
    
    <a href="./" title="Week 5" class="md-nav__link md-nav__link--active">
      Week 5
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#learning-objectives" title="Learning Objectives" class="md-nav__link">
    Learning Objectives
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-are-eigen-things" title="What are eigen-things?" class="md-nav__link">
    What are eigen-things?
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-are-eigenvalues-and-eigenvectors" title="What are eigenvalues and eigenvectors?" class="md-nav__link">
    What are eigenvalues and eigenvectors?
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#getting-into-the-detail-of-eigenproblems" title="Getting into the detail of eigenproblems" class="md-nav__link">
    Getting into the detail of eigenproblems
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#special-eigen-cases" title="Special eigen-cases" class="md-nav__link">
    Special eigen-cases
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calculating-eigenvectors" title="Calculating eigenvectors" class="md-nav__link">
    Calculating eigenvectors
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#conclusions" title="Conclusions" class="md-nav__link">
    Conclusions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#when-changing-to-the-eigenbasis-is-really-useful" title="When changing to the eigenbasis is really useful" class="md-nav__link">
    When changing to the eigenbasis is really useful
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#changing-to-the-eigenbasis" title="Changing to the eigenbasis" class="md-nav__link">
    Changing to the eigenbasis
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#making-the-pagerank-algorithm" title="Making the PageRank algorithm" class="md-nav__link">
    Making the PageRank algorithm
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pagerank" title="PageRank" class="md-nav__link">
    PageRank
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#damping-factor" title="Damping factor" class="md-nav__link">
    Damping factor
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" title="Summary" class="md-nav__link">
    Summary
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      Multivariate Calculus
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        Multivariate Calculus
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../multivariate_calculus/course_resources/" title="Course Resources" class="md-nav__link">
      Course Resources
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../multivariate_calculus/week_1/" title="Week 1" class="md-nav__link">
      Week 1
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#learning-objectives" title="Learning Objectives" class="md-nav__link">
    Learning Objectives
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-are-eigen-things" title="What are eigen-things?" class="md-nav__link">
    What are eigen-things?
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-are-eigenvalues-and-eigenvectors" title="What are eigenvalues and eigenvectors?" class="md-nav__link">
    What are eigenvalues and eigenvectors?
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#getting-into-the-detail-of-eigenproblems" title="Getting into the detail of eigenproblems" class="md-nav__link">
    Getting into the detail of eigenproblems
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#special-eigen-cases" title="Special eigen-cases" class="md-nav__link">
    Special eigen-cases
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calculating-eigenvectors" title="Calculating eigenvectors" class="md-nav__link">
    Calculating eigenvectors
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#conclusions" title="Conclusions" class="md-nav__link">
    Conclusions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#when-changing-to-the-eigenbasis-is-really-useful" title="When changing to the eigenbasis is really useful" class="md-nav__link">
    When changing to the eigenbasis is really useful
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#changing-to-the-eigenbasis" title="Changing to the eigenbasis" class="md-nav__link">
    Changing to the eigenbasis
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#making-the-pagerank-algorithm" title="Making the PageRank algorithm" class="md-nav__link">
    Making the PageRank algorithm
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pagerank" title="PageRank" class="md-nav__link">
    PageRank
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#damping-factor" title="Damping factor" class="md-nav__link">
    Damping factor
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" title="Summary" class="md-nav__link">
    Summary
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/JohnGiorgi/mathematics-for-machine-learning/edit/master/docs/linear_algebra/week_5.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="week-5-eigenvalues-and-eigenvectors">Week 5: Eigenvalues and Eigenvectors</h1>
<p><strong>Eigenvectors</strong> are particular vectors that are unrotated by a transformation matrix (i.e., they <em>remain on their own span</em>) and <strong>eigenvalues</strong> are the amount by which the eigenvectors are scaled. These special 'eigen-things' are very useful in linear algebra and will let us examine Google's famous PageRank algorithm for presenting web search results. Then we'll apply this in code, which will wrap up the course.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>It is best to watch <a href="https://youtu.be/PFDu9oVAE-g">this</a> video first, then return to and read through this section. If you want even more exposure to these ideas, try the first three sections of the Khan Academy course <a href="https://www.khanacademy.org/math/linear-algebra/alternate-bases#eigen-everything">here</a>.</p>
</div>
<h2 id="learning-objectives">Learning Objectives</h2>
<ul>
<li>Identify geometrically what an eigenvector/value is</li>
<li>Apply mathematical formulation in simple cases</li>
<li>Build an intuition of larger dimensional eigensystems</li>
<li>Write code to solve a large dimensional eigen problem</li>
</ul>
<h2 id="what-are-eigen-things">What are eigen-things?</h2>
<h3 id="what-are-eigenvalues-and-eigenvectors">What are eigenvalues and eigenvectors?</h3>
<p>The word, "eigen" is perhaps most usefully translated from German as meaning <em>characteristic</em>. So when we talk about an <em>eigenproblem</em>, we're talking about finding the <em>characteristic properties of something</em>. But characteristic of what? This module, like the previous weeks, will try and explain this concept of <em>"eigen-ness"</em> primarily through a geometric interpretation, which allows us to discuss images rather than immediately getting tangled up in the math.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This topic is often considered by students to be quite tricky. But once you know how to sketch these problems, the rest is just algebra.</p>
</div>
<p>As you've seen from previous weeks, it's possible to express the concept of <strong>linear transformations</strong> using <strong>matrices</strong>. These operations can include <strong>scalings</strong>, <strong>rotations</strong>, and <strong>shears</strong>.</p>
<p><img alt="matrix_transformations" src="../../img/matrix_transformations.gif" /></p>
<p>Often, when applying these transformations, we are thinking about what they might do to a <em>specific vector</em>. However, it can also be useful to think about what it might look like when they are applied to every vector in this space. This is most easily visualized by drawing a square centered at the origin, and then observing how the square is <em>distorted</em> when you apply the transformation. For example, if we apply a scaling of 2 in the vertical direction, the square would become a rectangle. Whereas, if we applied a horizontal shear to this space, it would become a trapezoid:</p>
<p><img alt="scale_and_shear" src="../../img/scale_and_shear.gif" /></p>
<p>Now, here's the key concept. Notice that, after the transformation is applied, some vectors end up lying on the same line that they started on whereas, others do not. To highlight this, lets draw three specific vectors onto our initial square. Now, consider our vertical scaling again, and think about what will happen to these three vectors.</p>
<p><img alt="vertical_stretch" src="../../img/vertical_stretch.gif" /></p>
<p>As you can see, the horizontal <strong>green</strong> vector is unchanged, i.e., it is pointing in the same direction and having the same length. The vertical <strong>pink</strong> vector is also still pointing in the same direction as before but its length is doubled. Lastly, the diagonal <strong>orange</strong> vector used to be exactly 45 degrees to the axis, but it's angle has now increased as has its length.</p>
<p>Besides the horizontal and vertical vectors, any other vectors' direction would have been changed by this vertical scaling. So in some sense, the horizontal and vertical vectors are <em>special</em>, they are <em>characteristic</em> of this particular transformation. These are our <a href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors"><strong>eigenvectors</strong></a>, and the value they are scaled by is know as an <a href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors"><strong>eigenvalue</strong></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>From a conceptual perspective, that's about it for 2D eigen-problems, we simply take a transformation and we look for the vectors who are still laying on the same span as before, and then we measure how much their length has changed. This is basically what eigenvectors and their corresponding eigenvalues are.</p>
</div>
<p>Let's look at two more classic examples to make sure that we can generalize what we've learned. First, let look look at pure <strong>shear</strong>, where pure means that we aren't performing any scaling or rotation in addition, so the area is unchanged:</p>
<p><img alt="shear" src="../../img/shear.gif" /></p>
<p>Notice that it's only the <strong>green</strong> horizontal line that is still laying along its original span, and all the other vectors will be shifted</p>
<p>Finally, let's look at <strong>rotation</strong>. Clearly, this thing has got no <strong>eigenvectors</strong> at all, as all of the vectors have been rotated off their original span:</p>
<p><img alt="rotation" src="../../img/rotation.gif" /></p>
<p><strong>Conclusions</strong></p>
<p>In this lecture, we've already covered almost all of what you need to know about eigenvectors and eigenvalues. Although we've only been working in two dimensions so far, the concept is exactly the same in three or more dimensions. In the rest of the module, we'll have a look at some special cases, as well as discussing how to describe what we've observed in more mathematical terms.</p>
<h2 id="getting-into-the-detail-of-eigenproblems">Getting into the detail of eigenproblems</h2>
<h3 id="special-eigen-cases">Special eigen-cases</h3>
<p>As we saw previously, <strong>eigenvectors</strong> are those which lie along the same span both <em>before and after</em> applying a linear transform to a space. <strong>Eigenvalues</strong> are simply the amount that each of those vectors has been <em>stretched</em> in the process. In this section, we're going to look at <strong>three</strong> special cases to make sure the intuition we've built so far is robust, and then we're going to try and extend this concept into three dimensions.</p>
<p>The first example we're going to consider is that of a <em>uniform</em> scaling, which is where we scale by the same amount in each direction:</p>
<p><img alt="uniform_scaling" src="../../img/uniform_scaling.gif" /></p>
<p>As you will hopefully have spotted, not only are all three of the vectors that we've highlighted eigenvectors, but in fact, for a uniform scaling, <em>any vector</em> would be an eigenvector.</p>
<p>In this second example, we're going to look at rotation. In the previous section, we applied a small rotation, and we found that it had no eigenvectors. However, there is one case of non-zero pure rotation which does have at least some eigenvectors, and that is <span><span class="MathJax_Preview">180</span><script type="math/tex">180</script></span> degrees:</p>
<p><img alt="flipped_rotation" src="../../img/flipped_rotation.gif" /></p>
<p>As you can see, the three eigenvectors are still laying on the same spans as before, but pointing in the opposite direction. This means that once again, all vectors for this transform are eigenvectors, and they all have eigenvalues of <span><span class="MathJax_Preview">-1</span><script type="math/tex">-1</script></span>, which means that although the eigenvectors haven't changed length, they are all now pointing in the opposite direction.</p>
<p>In this third case, we're going to look at a combination of a <strong>horizontal shear</strong> and a <strong>vertical scaling</strong>, and it's slightly less obvious than some of the previous examples. Just like the pure shear case we saw previously, the <strong>green</strong> horizontal vector is an eigenvector and its eigenvalue is still <span><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span>. However, despite the fact that neither of the other two vectors shown are eigen, this transformation does have two eigenvectors:</p>
<p><img alt="shear_and_scale_two_eigens" src="../../img/shear_and_scale_two_eigens.gif" /></p>
<p>Let's apply the inverse transform and watch our parallelogram go back to its original square. But this time, with our other eigenvector visible. Hopefully, you're at least convinced that it is indeed an eigenvector as it stays on its own span:</p>
<p><img alt="sneaky_eigen_vector" src="../../img/sneaky_eigen_vector.gif" /></p>
<p>This shows us that while the concept of eigenvectors is fairly straightforward, eigenvectors aren't always easy to spot. This problem is even tougher in three or more dimensions, and many of the uses of eigen theory in machine learning frame the system as being composed of hundreds of dimensions or more. So, clearly, we're going to need a more robust mathematical description of this concept to allow us to proceed.</p>
<p>Before we do, let's take a quick look at one example in 3D. Clearly, scaling and shear are all going to operate much the same way in 3D as they do in 2D. However, rotation does take on a neat new meaning. As you can see from the image, although both the <strong>pink</strong> and <strong>green</strong> vectors have changed direction, the <strong>orange</strong> vector has not moved. This means that the <strong>orange</strong> vector is an eigenvector, but it also tells us, as a physical interpretation, that if we find the eigenvector of a 3D rotation, it means we've also found the <strong>axis of rotation</strong>.</p>
<p><img alt="rotation_in_3d" src="../../img/rotation_in_3d.gif" /></p>
<p>In this video, we've covered a range of special cases, which I hope have prompted the questions in your mind about how we're going to go about writing a formal definition of an eigen-problem.</p>
<h3 id="calculating-eigenvectors">Calculating eigenvectors</h3>
<p>At this point, we should now have a reasonable feeling for what an eigen-problem looks like, at least geometrically. In this section, we're going to formalize this concept into an algebraic expression, which will allow us to calculate eigenvalues and eigenvectors whenever they exist.</p>
<p>Consider a transformation <span><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>. An <strong>eigenvector</strong> of this transformation is any vector that can be written as a scaled version of itself after the transformation is applied, i.e.,</p>
<div>
<div class="MathJax_Preview">Ax = \lambda x</div>
<script type="math/tex; mode=display">Ax = \lambda x</script>
</div>
<p>This expression captures the idea that applying the transformation <span><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span> to an eigenvector <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> is the same as scaling that eigenvector <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> by some number, <span><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> (the eigenvalue). In order to solve for the eigenvectors of the transformation <span><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>, we need to find values of <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> that make the two sides equal.</p>
<p>To help us find the solutions to this expression, we can rewrite it by putting all the terms on one side and then factorizing</p>
<div>
<div class="MathJax_Preview">(A - \lambda I) x = 0</div>
<script type="math/tex; mode=display">(A - \lambda I) x = 0</script>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you're wondering where the <span><span class="MathJax_Preview">I</span><script type="math/tex">I</script></span> term came from, it's just an <span><span class="MathJax_Preview">n \times n</span><script type="math/tex">n \times n</script></span> identity matrix. We didn't need this in the first expression we wrote, as multiplying vectors by scalars is defined. However, subtracting scalars from matrices is <em>not defined</em>, so the <span><span class="MathJax_Preview">I</span><script type="math/tex">I</script></span> just tidies up the math, without changing the meaning.</p>
</div>
<p>Now that we have this expression, we can see that for the left-hand side to equal <span><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span>, either the contents of the brackets must be <span><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span> or the vector <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> must be <span><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span>. As it turns out, we're not interested in the case where the vector <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> is <span><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span>, i.e., when it has no length or direction, as this represents a <a href="https://en.wikipedia.org/wiki/Triviality_(mathematics)"><em>trivial solution</em></a>. Instead, we are interested in the case where the term in brackets is <span><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span>.</p>
<p>Referring back to the material in the previous parts of the course, we can test if a matrix operation will result in a <span><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span> output by calculating its determinant</p>
<div>
<div class="MathJax_Preview">det (A - \lambda I) = 0</div>
<script type="math/tex; mode=display">det (A - \lambda I) = 0</script>
</div>
<p>Calculating the determinants manually is a lot of work for high dimensional matrices. So let's try applying this to an arbitrary <span><span class="MathJax_Preview">2 \times 2</span><script type="math/tex">2 \times 2</script></span> transformation. Let</p>
<div>
<div class="MathJax_Preview">A = \begin{pmatrix}a &amp; b \\ c &amp; d\end{pmatrix}</div>
<script type="math/tex; mode=display">A = \begin{pmatrix}a & b \\ c & d\end{pmatrix}</script>
</div>
<p>substituting this into our eigen-finding expression gives the following:</p>
<div>
<div class="MathJax_Preview">det \Biggl( \begin{pmatrix}a &amp; b \\\ c &amp; d\end{pmatrix}- \begin{pmatrix}\lambda &amp; 0 \\\ 0 &amp; \lambda\end{pmatrix} \Biggl ) = 0</div>
<script type="math/tex; mode=display">det \Biggl( \begin{pmatrix}a & b \\\ c & d\end{pmatrix}- \begin{pmatrix}\lambda & 0 \\\ 0 & \lambda\end{pmatrix} \Biggl ) = 0</script>
</div>
<p>Evaluating this determinant, we get what is referred to as the <a href="http://www.wikiwand.com/en/Characteristic_polynomial"><strong>characteristic polynomial</strong></a>, which looks like this</p>
<div>
<div class="MathJax_Preview">\lambda^2 - (a + d) \lambda + ad - bc = 0 </div>
<script type="math/tex; mode=display">\lambda^2 - (a + d) \lambda + ad - bc = 0 </script>
</div>
<p>Our eigenvalues are simply the <em>solutions</em> of this equation. Once we solve for them, we can then plug them back into the original expression to calculate our eigenvectors. Click the dropdown below for a fully-worked out solution to computing eigenvalues and eigenvectors.</p>
<details class="example"><summary>Example</summary><p>Let's take the case of a <strong>vertical scaling</strong> by a factor of <span><span class="MathJax_Preview">2</span><script type="math/tex">2</script></span>, which is represented by the transformation matrix</p><div><div class="MathJax_Preview">A = \begin{pmatrix} 1 &amp;  0 \\\ 0 &amp; 2\end{pmatrix}</div><script type="math/tex; mode=display">A = \begin{pmatrix} 1 &  0 \\\ 0 & 2\end{pmatrix}</script></div><p>We start with our equation for finding eigenvalues</p><div><div class="MathJax_Preview">Ax = \lambda x</div><script type="math/tex; mode=display">Ax = \lambda x</script></div><p>Rearranging, we get</p><div><div class="MathJax_Preview">(A-\lambda I)x = 0</div><script type="math/tex; mode=display">(A-\lambda I)x = 0</script></div><p>Solving <span><span class="MathJax_Preview">(A-\lambda I) = 0</span><script type="math/tex">(A-\lambda I) = 0</script></span> is equivalent to asking when the determinant of the matrix is <span><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span></p><div><div class="MathJax_Preview">det((A- \lambda I)) = 0</div><script type="math/tex; mode=display">det((A- \lambda I)) = 0</script></div><p>Subbing in our matrix <span><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span> and solving the resulting characteristic polynomial</p><div><div class="MathJax_Preview">det \begin{pmatrix} 1 - \lambda &amp; 0 \\\ 0 &amp; 2- \lambda \end{pmatrix} = (1 - \lambda)(2-\lambda) = 0</div><script type="math/tex; mode=display">det \begin{pmatrix} 1 - \lambda & 0 \\\ 0 & 2- \lambda \end{pmatrix} = (1 - \lambda)(2-\lambda) = 0</script></div><p>This means that our equation must have solutions at <span><span class="MathJax_Preview">\lambda = 1</span><script type="math/tex">\lambda = 1</script></span> and <span><span class="MathJax_Preview">\lambda = 2</span><script type="math/tex">\lambda = 2</script></span>. Thinking back to our original eigen-finding formula, <span><span class="MathJax_Preview">(A - \lambda I)x = 0</span><script type="math/tex">(A - \lambda I)x = 0</script></span>, we can now sub these two solutions back in. Thinking about the case where <span><span class="MathJax_Preview">\lambda = 1</span><script type="math/tex">\lambda = 1</script></span>,</p><div><div class="MathJax_Preview">@\lambda = 1: \begin{pmatrix} 1 - 1 &amp; 0 \\\ 0 &amp; 2 - 1 \end{pmatrix}\begin{pmatrix} x_1 \\\ x_2\end{pmatrix}  = \begin{pmatrix} 0 &amp; 0 \\\ 0 &amp; 1 \end{pmatrix}\begin{pmatrix} x_1 \\\ x_2\end{pmatrix} = \begin{pmatrix} 0 \\\ x_2\end{pmatrix} = 0</div><script type="math/tex; mode=display">@\lambda = 1: \begin{pmatrix} 1 - 1 & 0 \\\ 0 & 2 - 1 \end{pmatrix}\begin{pmatrix} x_1 \\\ x_2\end{pmatrix}  = \begin{pmatrix} 0 & 0 \\\ 0 & 1 \end{pmatrix}\begin{pmatrix} x_1 \\\ x_2\end{pmatrix} = \begin{pmatrix} 0 \\\ x_2\end{pmatrix} = 0</script></div><p>Now, thinking about the case where <span><span class="MathJax_Preview">\lambda = 2</span><script type="math/tex">\lambda = 2</script></span>,</p><div><div class="MathJax_Preview">@\lambda = 2: \begin{pmatrix} 1 - 2 &amp; 0 \\\ 0 &amp; 2 - 2 \end{pmatrix}\begin{pmatrix} x_1 \\\ x_2\end{pmatrix}  = \begin{pmatrix} -1 &amp; 0 \\\ 0 &amp; 0 \end{pmatrix}\begin{pmatrix} x_1 \\\ x_2\end{pmatrix} = \begin{pmatrix} -x_1 \\\ 0 \end{pmatrix} = 0</div><script type="math/tex; mode=display">@\lambda = 2: \begin{pmatrix} 1 - 2 & 0 \\\ 0 & 2 - 2 \end{pmatrix}\begin{pmatrix} x_1 \\\ x_2\end{pmatrix}  = \begin{pmatrix} -1 & 0 \\\ 0 & 0 \end{pmatrix}\begin{pmatrix} x_1 \\\ x_2\end{pmatrix} = \begin{pmatrix} -x_1 \\\ 0 \end{pmatrix} = 0</script></div><p>So what do these two expressions tell us? Well, in the case where our eigenvalue <span><span class="MathJax_Preview">\lambda = 1</span><script type="math/tex">\lambda = 1</script></span>, we've got an eigenvector where the <span><span class="MathJax_Preview">x_2</span><script type="math/tex">x_2</script></span> term must be zero. But we don't really know anything about the <span><span class="MathJax_Preview">x_1</span><script type="math/tex">x_1</script></span> term. Well, this is because <em>any vector that points along the horizontal axis could be an eigenvector of this system</em>. We say that by writing</p><div><div class="MathJax_Preview">@\lambda = 1: \begin{pmatrix} t  \\\ 0 \end{pmatrix}</div><script type="math/tex; mode=display">@\lambda = 1: \begin{pmatrix} t  \\\ 0 \end{pmatrix}</script></div><p>using an arbitrary parameter <span><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>. Similarly for the <span><span class="MathJax_Preview">\lambda = 2</span><script type="math/tex">\lambda = 2</script></span> case, we can say that our eigenvector must equal</p><div><div class="MathJax_Preview">@\lambda = 2: \begin{pmatrix} 0  \\\ t \end{pmatrix}</div><script type="math/tex; mode=display">@\lambda = 2: \begin{pmatrix} 0  \\\ t \end{pmatrix}</script></div><p>because as long as it doesn't move at all in the horizontal direction, any vector that's purely vertical would be an eigenvector of this system, as they would lie along the same span. So now we have two eigenvalues, and their two corresponding eigenvectors.</p></details><h4 id="conclusions">Conclusions</h4>
<p>Despite all the fun that we've just been having, the truth is that you will almost certainly never have to perform this calculation by hand.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Indeed, with libraries like numpy this is as easy as
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span> <span class="p">)</span>

<span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div></p>
</div>
<p>Furthermore, we saw that our approach required finding the roots of a polynomial of order <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>, i.e., the dimension of your matrix, which means that the problem will very quickly stop being possible by analytical methods alone. When a computer finds the eigensolutions of a 100 dimensional problem it's forced to employ iterative numerical methods. Therefore, developing a strong conceptual understanding of eigen problems will be much more useful than being really good at calculating them by hand.</p>
<p>In this sections, we translated our geometrical understanding of eigenvectors into a robust mathematical expression, and validated it on a few test cases. But I hope that I've also convinced you that working through lots of eigen-problems, as is often done in engineering undergraduate degrees, is not a good investment of your time if you already understand the underlying concepts. This is what computers are for. Next video, we'll be referring back to the concept of basis change to see what magic happens when you use eigenvectors as your basis. See you then.</p>
<h2 id="when-changing-to-the-eigenbasis-is-really-useful">When changing to the eigenbasis is really useful</h2>
<h3 id="changing-to-the-eigenbasis">Changing to the eigenbasis</h3>
<p>Now that we know what eigenvectors are and how to calculate them, we can combine this idea with the concept of changing basis (which was covered earlier in the course). What emerges from this synthesis is a particularly powerful tool for performing efficient matrix operations called <a href="http://www.wikiwand.com/en/Matrix_diagonalization"><strong>diagonalisation</strong></a>.</p>
<p>Sometimes, we need to apply the same matrix multiplication many times. For example, imagine a transformation matrix, <span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span>, that represents the change in location of a particle after a single time step. We can write that: our initial position, described by vector <span><span class="MathJax_Preview">v_0</span><script type="math/tex">v_0</script></span>, multiplied by the transformation <span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span> gives us our new location, <span><span class="MathJax_Preview">v_1</span><script type="math/tex">v_1</script></span>.</p>
<p><img alt="" src="../../img/v_0_to_v_1.gif" /></p>
<p>To work out where our particle will be after two time steps, we can find <span><span class="MathJax_Preview">v_2</span><script type="math/tex">v_2</script></span> by simply multiplying <span><span class="MathJax_Preview">v_1</span><script type="math/tex">v_1</script></span> by <span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span>, which is the same thing as multiplying <span><span class="MathJax_Preview">v_0</span><script type="math/tex">v_0</script></span> by <span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span> two times. So <span><span class="MathJax_Preview">v_2 = T^2 v_0</span><script type="math/tex">v_2 = T^2 v_0</script></span>.</p>
<p><img alt="" src="../../img/v_1_to_v_2.gif" /></p>
<p>Now imagine that we expect the same linear transformation to occur every time step for <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> time steps. We can write this transformation as</p>
<div>
<div class="MathJax_Preview">v_n = T^n v_0</div>
<script type="math/tex; mode=display">v_n = T^n v_0</script>
</div>
<p>You've already seen how much work it takes to apply a single 3D matrix multiplication. If we were to imagine that <span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span> tells us what happens in one second, but we'd like to know where our particle is in two weeks from now, then <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> is going to be around 1.2 million, i.e., we'd need to multiply <span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span> by itself <em>more than a million times</em>, which may take quite a while.</p>
<p><img alt="" src="../../img/v_0_to_v_n.gif" /></p>
<p>If all the terms in the matrix are zero except for those along the leading diagonal, we refer to it as a <a href="http://www.wikiwand.com/en/Diagonal_matrix"><strong>diagonal matrix</strong></a>. When raising matrices to powers, diagonal matrices make things a lot easier. All you need to do is put each of the terms on the diagonal to the power of <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> and you've got the answer. So in this case,</p>
<div>
<div class="MathJax_Preview">T^n = \begin{pmatrix} a^n &amp; 0 &amp; 0 \\\ 0 &amp; b^n &amp; 0 \\\ 0 &amp; 0 &amp; c^n\end{pmatrix}</div>
<script type="math/tex; mode=display">T^n = \begin{pmatrix} a^n & 0 & 0 \\\ 0 & b^n & 0 \\\ 0 & 0 & c^n\end{pmatrix}</script>
</div>
<p>Thats simple enough, but what if <span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span> is not a diagonal matrix? Well, as you may have guessed, the answer comes from eigen-analysis. Essentially, what we're going to do is change to a basis where our transformation <span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span> <em>becomes</em> diagonal, which is what we call an <strong>eigen-basis</strong>. We can then easily apply our power of <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> to the diagonalized form, and finally transform the resulting matrix back again, giving us <span><span class="MathJax_Preview">T^n</span><script type="math/tex">T^n</script></span>, but avoiding much of the work.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If this is confusing, watch the last 4 minutes of <a href="https://youtu.be/PFDu9oVAE-g?t=13m2s">this</a> video.</p>
</div>
<p>As we saw in the section on changing basis, each column of our transform matrix simply represents the new location of the transformed unit vectors. So, to build our eigen-basis conversion matrix, we just plug in each of our eigenvectors as columns:</p>
<div>
<div class="MathJax_Preview">C = \begin{pmatrix}x_1 &amp; x_2 &amp; x_3 \\\ \vdots &amp; \vdots &amp; \vdots\end{pmatrix}</div>
<script type="math/tex; mode=display">C = \begin{pmatrix}x_1 & x_2 & x_3 \\\ \vdots & \vdots & \vdots\end{pmatrix}</script>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The basic idea here is that the transformation <span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span> just becomes a uniform scaling (represented by a diagonal matrix) in a basis composed strictly of eigenvectors of <span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span>.</p>
</div>
<p>Applying this transform, we find ourselves in a world where multiplying by <span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span> is effectively just a pure scaling, which is another way of saying that it can now be represented by a diagonal matrix. Crucially, this diagonal matrix, <span><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span>, contains the corresponding eigenvalues of the matrix <span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span>. So,</p>
<div>
<div class="MathJax_Preview">D = \begin{pmatrix} \lambda_1 &amp; 0 &amp; 0 \\\ 0 &amp; \lambda_2 &amp; 0 \\\ 0 &amp; 0 &amp; \lambda_3\end{pmatrix}</div>
<script type="math/tex; mode=display">D = \begin{pmatrix} \lambda_1 & 0 & 0 \\\ 0 & \lambda_2 & 0 \\\ 0 & 0 & \lambda_3\end{pmatrix}</script>
</div>
<p>We're so close now to unleashing the power of eigen. The final link that we need to see is the following. Bringing together everything we've just said, it should now be clear that applying the transformation <span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span> is just the same as converting to our eigenbasis, applying the diagonalized matrix, and then converting back again. So</p>
<div>
<div class="MathJax_Preview">T = CDC^{-1}</div>
<script type="math/tex; mode=display">T = CDC^{-1}</script>
</div>
<p>which suggests that</p>
<div>
<div class="MathJax_Preview">T^2 = CDC^{-1}CDC^{-1} = CDDC^{-1} = CD^2C^{-1}</div>
<script type="math/tex; mode=display">T^2 = CDC^{-1}CDC^{-1} = CDDC^{-1} = CD^2C^{-1}</script>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span><span class="MathJax_Preview">C^{-1}C = I</span><script type="math/tex">C^{-1}C = I</script></span>, so we omit it.</p>
</div>
<p>We can then generalize this to any power of <span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span> we'd like</p>
<div>
<div class="MathJax_Preview">T^n = CD^nC^{-1}</div>
<script type="math/tex; mode=display">T^n = CD^nC^{-1}</script>
</div>
<p>We now have a method which lets us apply a transformation matrix as many times as we'd like without paying a large computational cost.</p>
<p><img alt="eigenbasis" src="../../img/eigenbasis.png" /></p>
<p><strong>Conclusions</strong></p>
<p>This result brings together many of the ideas that we've encountered so far in this course. Check out <a href="https://www.coursera.org/learn/linear-algebra-machine-learning/lecture/zYzjM/eigenbasis-example">this</a> video, where we'll work through a short example just to ensure that this approach lines up with our expectations when applied to a simple case.</p>
<h2 id="making-the-pagerank-algorithm">Making the PageRank algorithm</h2>
<h3 id="pagerank">PageRank</h3>
<p>The final topic of this module on eigenproblems, as well as the final topic of this course as a whole, will focus on an algorithm called <a href="http://www.wikiwand.com/en/PageRank"><strong>PageRank</strong></a>.</p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>This algorithm was famously published by and named after Google founder Larry Page and colleagues in 1998. And was used by Google to help them decide which order to display their websites when they returned from search.</p>
</div>
<p>The central assumption underpinning PageRank is that the importance of a website is related to its links to and from other websites. This bubble diagram represents a model mini Internet, where each bubble is a webpage and each arrow from A, B, C, and D represents a link on that webpage which takes you to one of the others.</p>
<p><img alt="page_rank" src="../../img/page_rank.png" /></p>
<p>We're trying to build an expression that tells us, based on this network structure, which of these webpages is most relevant to the person who made the search. As such, we're going to use the concept of <em>Procrastinating Pat</em> who is an imaginary person who goes on the Internet and just randomly click links to avoid doing their work. By mapping all the possible links, we can build a model to estimate the amount of time we would expect Pat to spend on each webpage.</p>
<p>We can describe the links present on page A as a vector, where each row is either a one or a zero based on whether there is a link to the corresponding page. And then normalize the vector by the total number of the links, such that they can be used to describe a probability for that page.</p>
<p>For example, the vector of links from page A will be</p>
<div>
<div class="MathJax_Preview">\begin{bmatrix} 0 &amp; 1 &amp; 1 &amp; 1\end{bmatrix}</div>
<script type="math/tex; mode=display">\begin{bmatrix} 0 & 1 & 1 & 1\end{bmatrix}</script>
</div>
<p>because vector A has links to sites B, to C, and to D, but it doesn't have a link to itself. Also, because there are three links in this page in total, we would normalize by a factor of a third. So the total click probability sums to one. We can write,</p>
<div>
<div class="MathJax_Preview">L_A =  \begin{bmatrix}0 &amp; \frac{1}{3} &amp; \frac{1}{3} &amp; \frac{1}{3}\end{bmatrix}</div>
<script type="math/tex; mode=display">L_A =  \begin{bmatrix}0 & \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\end{bmatrix}</script>
</div>
<p>Following the same logic, the link vectors in the next two sites are shown here:</p>
<div>
<div class="MathJax_Preview">L_B =  \begin{bmatrix} \frac{1}{2} &amp; 0 &amp; 0 &amp;  \frac{1}{2}\end{bmatrix}, \;  L_C =  \begin{bmatrix}0 &amp; 0 &amp; 0 &amp; 1\end{bmatrix}</div>
<script type="math/tex; mode=display">L_B =  \begin{bmatrix} \frac{1}{2} & 0 & 0 &  \frac{1}{2}\end{bmatrix}, \;  L_C =  \begin{bmatrix}0 & 0 & 0 & 1\end{bmatrix}</script>
</div>
<p>and finally, for page D, we can write</p>
<div>
<div class="MathJax_Preview">L_D =  \begin{bmatrix} 0 &amp; \frac{1}{2} &amp; \frac{1}{2} &amp; 0 \end{bmatrix}</div>
<script type="math/tex; mode=display">L_D =  \begin{bmatrix} 0 & \frac{1}{2} & \frac{1}{2} & 0 \end{bmatrix}</script>
</div>
<p>We can now build our link matrix <span><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span> by using each of our linked vectors as a column, which you can see will form a square matrix</p>
<div>
<div class="MathJax_Preview">L =  \begin{bmatrix} 0 &amp; \frac{1}{2} &amp; 0 &amp; 0 \\\ \frac{1}{3} &amp; 0 &amp; 0 &amp; \frac{1}{2} \\\ \frac{1}{3} &amp; 0 &amp; 0 &amp; \frac{1}{2}  \\\ \frac{1}{3} &amp; \frac{1}{2} &amp; 1 &amp; 0 \end{bmatrix}</div>
<script type="math/tex; mode=display">L =  \begin{bmatrix} 0 & \frac{1}{2} & 0 & 0 \\\ \frac{1}{3} & 0 & 0 & \frac{1}{2} \\\ \frac{1}{3} & 0 & 0 & \frac{1}{2}  \\\ \frac{1}{3} & \frac{1}{2} & 1 & 0 \end{bmatrix}</script>
</div>
<p>What we're trying to represent here with our matrix <span><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span> is the probability of ending up on each of the pages. For example, the only way to get to A is by being at B. So you then need to know the probability of being at B, which you could've got to from either A or D. As you can see, this problem is self-referential, as the ranks on all the pages depend on all the others. Although we built our matrix from columns of outward links, we can see that the rows of <span><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span> describe inward links normalized with respect to their page of origin.</p>
<p>We can now write an expression which summarises the approach. We're going to use the vector <span><span class="MathJax_Preview">r</span><script type="math/tex">r</script></span> to store the rank of all webpages. To calculate the rank of page A, you need to know three things about all other pages on the Internet:</p>
<ol>
<li>What's your rank?</li>
<li>Do you link to page A?</li>
<li>And how many outgoing links do you have in total?</li>
</ol>
<p>The following expression combines these three pieces of information for webpage A only</p>
<div>
<div class="MathJax_Preview">r_A = \sum_{j=1}^n L_{A, j}r_j</div>
<script type="math/tex; mode=display">r_A = \sum_{j=1}^n L_{A, j}r_j</script>
</div>
<p>This expression states that the rank of A is the <em>sum of the ranks of all the pages which link to it</em>, weighted by their specific link probability taken from matrix <span><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span>. It would be nice, however, if we could modify this expression to solve for all pages simultaneously. We can rewrite the above expression applied to all webpages as a simple matrix multiplication</p>
<div>
<div class="MathJax_Preview">r = Lr</div>
<script type="math/tex; mode=display">r = Lr</script>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If this is confusing, think of it as saying, in english: the rank of some page <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> is equal to probability that it is linked to from <span><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span> times the rank of <span><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span> for all pages <span><span class="MathJax_Preview">i, j</span><script type="math/tex">i, j</script></span>.</p>
</div>
<p>Clearly, we start off not knowing <span><span class="MathJax_Preview">r</span><script type="math/tex">r</script></span>, so we simply assume that all the ranks are equal and normalize them by the total number of webpages in our analysis, which in this case is 4</p>
<div>
<div class="MathJax_Preview">r = \begin{bmatrix}\frac{1}{4} \\ \frac{1}{4} \\ \frac{1}{4} \\ \frac{1}{4}\end{bmatrix}</div>
<script type="math/tex; mode=display">r = \begin{bmatrix}\frac{1}{4} \\ \frac{1}{4} \\ \frac{1}{4} \\ \frac{1}{4}\end{bmatrix}</script>
</div>
<p>Then, each time we multiply <span><span class="MathJax_Preview">r</span><script type="math/tex">r</script></span> by our matrix <span><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span>, we get an updated value for <span><span class="MathJax_Preview">r</span><script type="math/tex">r</script></span></p>
<div>
<div class="MathJax_Preview">r^{i+1} = Lr^i</div>
<script type="math/tex; mode=display">r^{i+1} = Lr^i</script>
</div>
<p>Applying this expression repeatedly means that we are solving this problem iteratively. Each time we do this, we update the values in <span><span class="MathJax_Preview">r</span><script type="math/tex">r</script></span> until, eventually, <span><span class="MathJax_Preview">r</span><script type="math/tex">r</script></span> stops changing, i.e. <span><span class="MathJax_Preview">r = Lr</span><script type="math/tex">r = Lr</script></span>. Quite beautifully, this implies that <span><span class="MathJax_Preview">r</span><script type="math/tex">r</script></span> is now an eigenvector of matrix <span><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span>, with an eigenvalue of 1!</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>At this point, you might well be thinking, if we want to multiply <span><span class="MathJax_Preview">r</span><script type="math/tex">r</script></span> by <span><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span> many times, perhaps we should apply the diagonalization method that we saw in the last video. But don't forget, this would require us to already know all of the eigenvectors, which is what we're trying to find in the first place.</p>
</div>
<p>Now that we have an equation, and hopefully some idea of where it came from, we can ask our computer to iteratively apply it until it converges to find our rank vector:</p>
<p><img alt="page_rank" src="../../img/page_rank.gif" /></p>
<p>Although it takes about ten iterations for the numbers to settle down, the order is already established after the first iteration. However, this is just an artifact of our system being so tiny. Finally, we can read off our result, which says that as Procrastinating Pat randomly clicks around our network, we'd expect them to spend about 40% of their time on page D, but only about 12% of their time on page A and 24% on each of pages B and C</p>
<p><img alt="page_rank_2" src="../../img/page_rank_2.gif" /></p>
<p>As it turns out, although there are many approaches for efficiently calculating eigenvectors that have been developed over the years, repeatedly multiplying a randomly selected initial guest vector by a matrix, which is called the <strong>power method</strong>, is still very effective for the PageRank problem for two reasons. Firstly, although the power method will clearly only give you one eigenvector, when we know that there will be <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> for an <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> webpage system, it turns out that because of the way we've structured our link matrix, the vector it gives you will always be the one that you're looking for, with an eigenvalue of 1. Secondly, although this is not true for the full webpage mini Internet, when looking at the real Internet you can imagine that almost every entry in the link matrix will be zero, i.e,, most pages don't connect to most other pages. This is referred to as a <strong>sparse matrix</strong>. And algorithms exist such that multiplications can be performed very efficiently.</p>
<h4 id="damping-factor">Damping factor</h4>
<p>One key aspect of the PageRank algorithm that we haven't discussed so far is the <strong>damping factor</strong>, <span><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span>. This adds an additional term to our iterative formula. So <span><span class="MathJax_Preview">r^{i + 1}</span><script type="math/tex">r^{i + 1}</script></span> is now going to equal</p>
<div>
<div class="MathJax_Preview">r^{i + 1}= d Lr^i + \frac{1 - d}{n}</div>
<script type="math/tex; mode=display">r^{i + 1}= d Lr^i + \frac{1 - d}{n}</script>
</div>
<p>where <span><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span> is something between 0 and 1. You can think of it as 1 minus the probability with which procrastinating Pat suddenly, randomly types in a web address, rather than clicking on a link on his current page. The effect that this has on the actual calculation is about finding a compromise between speed and stability of the iterative convergence process. There are over one billion websites on the Internet today, compared with just a few million when the PageRank algorithm was first published in 1998, and so the methods for search and ranking have had to evolve to maximize efficiency, although the core concept has remained unchanged for many years.</p>
<p><strong>Conclusions</strong></p>
<p>This brings us to the end of our introduction to the PageRank algorithm. There are, of course, many details which we didn't cover in this video. But I hope this has allowed you to come away with some insight and understanding into how the PageRank works, and hopefully the confidence to apply this to some larger networks yourself.</p>
<h2 id="summary">Summary</h2>
<p>This brings us to the end of the fifth module and also, to the end of this course on linear algebra for machine learning.</p>
<p>We've covered a lot of ground in the past five modules, but I hope that we've managed to balance, the speed with the level of detail to ensure that you've stayed with us throughout.</p>
<p>There is a tension at the heart of mathematics teaching in the computer age. Classical teaching approaches focused around working through lots of examples by hand without much emphasis on building intuition. However, computers now do nearly all of the calculation work for us, and it's not typical for the methods appropriate to hand calculation to be the same as those employed by a computer. This can mean that, despite doing lots of work, students can come away from a classical education missing both the detailed view of the computational methods, but also the high level view of what each method is really doing. The concepts that you've been exposed to over the last five modules cover the core of linear algebra. That you will need as you progress your study of machine learning. And we hope that at the very least, when you get stuck in the future, you'll know the appropriate language. So that you can quickly look up some help when you need it. Which, after all, is the most important skill of a professional coder.</p>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../week_4/" title="Week 4" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Week 4
              </span>
            </div>
          </a>
        
        
          <a href="../../multivariate_calculus/course_resources/" title="Course Resources" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Course Resources
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
        
  <div class="md-footer-social">
    <link rel="stylesheet" href="../../assets/fonts/font-awesome.css">
    
      <a href="https://github.com/JohnGiorgi" class="md-footer-social__link fa fa-github"></a>
    
  </div>

      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../assets/javascripts/application.583bbe55.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:"../.."}})</script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
    
      
    
  </body>
</html>