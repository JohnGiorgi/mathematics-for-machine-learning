



<!DOCTYPE html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-3.0.4">
    
    
      
        <title>Linear Algebra - Mathematics for Machine Learning</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.451f80e5.css">
      
      
    
    
      <script src="../assets/javascripts/modernizr.1aa3b519.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    
  </head>
  
    <body dir="ltr">
  
    <svg class="md-svg">
      <defs>
        
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="../#linear-algebra" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href=".." title="Mathematics for Machine Learning" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                Mathematics for Machine Learning
              </span>
              <span class="md-header-nav__topic">
                Linear Algebra
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href=".." title="Mathematics for Machine Learning" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    Mathematics for Machine Learning
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="About" class="md-nav__link">
      About
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Linear Algebra
      </label>
    
    <a href="./" title="Linear Algebra" class="md-nav__link md-nav__link--active">
      Linear Algebra
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#course-resources" title="Course resources" class="md-nav__link">
    Course resources
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-1-introduction-to-linear-algebra" title="Week 1: Introduction to Linear Algebra" class="md-nav__link">
    Week 1: Introduction to Linear Algebra
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-relationship-between-machine-learning-linear-algebra-vectors-and-matrices" title="The relationship between machine learning, linear algebra, vectors and matrices" class="md-nav__link">
    The relationship between machine learning, linear algebra, vectors and matrices
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#motivations-for-linear-algebra" title="Motivations for linear algebra" class="md-nav__link">
    Motivations for linear algebra
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#conclusions" title="Conclusions" class="md-nav__link">
    Conclusions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#geometric-and-numeric-interpretations" title="Geometric and Numeric Interpretations" class="md-nav__link">
    Geometric and Numeric Interpretations
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vectors" title="Vectors" class="md-nav__link">
    Vectors
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#getting-a-handle-on-vectors" title="Getting a handle on vectors" class="md-nav__link">
    Getting a handle on vectors
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#operations-with-vectors" title="Operations with vectors" class="md-nav__link">
    Operations with vectors
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-2-vectors-are-objects-that-move-around-space" title="Week 2: Vectors are Objects that Move Around Space" class="md-nav__link">
    Week 2: Vectors are Objects that Move Around Space
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#finding-the-size-of-a-vector-its-angle-and-projection" title="Finding the size of a vector, its angle, and projection" class="md-nav__link">
    Finding the size of a vector, its angle, and projection
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#modulus-inner-product" title="Modulus &amp; inner product" class="md-nav__link">
    Modulus &amp; inner product
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cosine-dot-product" title="Cosine &amp; dot product" class="md-nav__link">
    Cosine &amp; dot product
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#projection" title="Projection" class="md-nav__link">
    Projection
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#changing-the-reference-frame" title="Changing the reference frame" class="md-nav__link">
    Changing the reference frame
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#changing-basis" title="Changing basis" class="md-nav__link">
    Changing basis
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#conclusions_1" title="Conclusions" class="md-nav__link">
    Conclusions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#basis-vector-space-and-linear-independence" title="Basis, vector space, and linear independence" class="md-nav__link">
    Basis, vector space, and linear independence
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#conclusions_2" title="Conclusions" class="md-nav__link">
    Conclusions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#applications-of-changing-basis" title="Applications of changing basis" class="md-nav__link">
    Applications of changing basis
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary-of-week-2" title="Summary of week 2" class="md-nav__link">
    Summary of week 2
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#week-3-matrices-as-objects-that-operate-on-vectors" title="Week 3: Matrices as Objects that Operate on Vectors" class="md-nav__link">
    Week 3: Matrices as Objects that Operate on Vectors
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#matrices" title="Matrices" class="md-nav__link">
    Matrices
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#introduction-to-matrices" title="Introduction to matrices" class="md-nav__link">
    Introduction to matrices
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#conclusions_3" title="Conclusions" class="md-nav__link">
    Conclusions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#matrices-as-objects-that-operate-on-vectors" title="Matrices as objects that operate on vectors" class="md-nav__link">
    Matrices as objects that operate on vectors
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-matrices-transform-space" title="How matrices transform space" class="md-nav__link">
    How matrices transform space
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#types-of-matrix-transformation" title="Types of matrix transformation" class="md-nav__link">
    Types of matrix transformation
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#conclusions_4" title="Conclusions" class="md-nav__link">
    Conclusions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#composition-or-combination-of-matrix-transformations" title="Composition or combination of matrix transformations" class="md-nav__link">
    Composition or combination of matrix transformations
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#matrix-inverses" title="Matrix inverses" class="md-nav__link">
    Matrix inverses
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gaussian-elimination-solving-the-apples-and-bananas-problem" title="Gaussian elimination: Solving the apples and bananas problem" class="md-nav__link">
    Gaussian elimination: Solving the apples and bananas problem
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#from-gaussian-elimination-to-finding-the-inverse-matrix" title="From Gaussian elimination to finding the inverse matrix" class="md-nav__link">
    From Gaussian elimination to finding the inverse matrix
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#special-matrices" title="Special matrices" class="md-nav__link">
    Special matrices
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-determinant" title="The determinant" class="md-nav__link">
    The determinant
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#a-determinant-of-zero" title="A determinant of zero" class="md-nav__link">
    A determinant of zero
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a-negative-determinant" title="A negative determinant" class="md-nav__link">
    A negative determinant
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-the-determinant-means-numerically" title="What the determinant means numerically" class="md-nav__link">
    What the determinant means numerically
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary" title="Summary" class="md-nav__link">
    Summary
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#week-4-matrices-make-linear-mappings" title="Week 4: Matrices Make Linear Mappings" class="md-nav__link">
    Week 4: Matrices Make Linear Mappings
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#matrices-as-objects-that-map-one-vector-onto-another" title="Matrices as objects that map one vector onto another" class="md-nav__link">
    Matrices as objects that map one vector onto another
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#introduction-to-einstein-summation-convention-and-the-symmetry-of-the-dot-product" title="Introduction to Einstein summation convention and the symmetry of the dot product" class="md-nav__link">
    Introduction to Einstein summation convention and the symmetry of the dot product
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#matrices-transform-into-the-new-basis-vector-set" title="Matrices transform into the new basis vector set" class="md-nav__link">
    Matrices transform into the new basis vector set
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#matrices-changing-basis" title="Matrices changing basis" class="md-nav__link">
    Matrices changing basis
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#generalizing" title="Generalizing" class="md-nav__link">
    Generalizing
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#doing-a-transformation-in-a-changed-basis" title="Doing a transformation in a changed basis" class="md-nav__link">
    Doing a transformation in a changed basis
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#generalizing_1" title="Generalizing" class="md-nav__link">
    Generalizing
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#making-multiple-mappings-deciding-if-these-are-reversible" title="Making multiple mappings, deciding if these are reversible" class="md-nav__link">
    Making multiple mappings, deciding if these are reversible
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonal-matrices" title="Orthogonal matrices" class="md-nav__link">
    Orthogonal matrices
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#recognizing-mapping-matrices-and-applying-these-to-data" title="Recognizing mapping matrices and applying these to data" class="md-nav__link">
    Recognizing mapping matrices and applying these to data
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-gramschmidt-process" title="The Gram–Schmidt process" class="md-nav__link">
    The Gram–Schmidt process
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reflecting-in-a-plane" title="Reflecting in a plane" class="md-nav__link">
    Reflecting in a plane
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#week-5-eigenvalues-and-eigenvectors" title="Week 5: Eigenvalues and Eigenvectors" class="md-nav__link">
    Week 5: Eigenvalues and Eigenvectors
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#learning-objectives" title="Learning Objectives" class="md-nav__link">
    Learning Objectives
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-are-eigen-things" title="What are eigen-things?" class="md-nav__link">
    What are eigen-things?
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-are-eigenvalues-and-eigenvectors" title="What are eigenvalues and eigenvectors?" class="md-nav__link">
    What are eigenvalues and eigenvectors?
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#getting-into-the-detail-of-eigenproblems" title="Getting into the detail of eigenproblems" class="md-nav__link">
    Getting into the detail of eigenproblems
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#special-eigen-cases" title="Special eigen-cases" class="md-nav__link">
    Special eigen-cases
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calculating-eigenvectors" title="Calculating eigenvectors" class="md-nav__link">
    Calculating eigenvectors
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#when-changing-to-the-eigenbasis-is-really-useful" title="When changing to the eigenbasis is really useful" class="md-nav__link">
    When changing to the eigenbasis is really useful
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#changing-to-the-eigenbasis" title="Changing to the eigenbasis" class="md-nav__link">
    Changing to the eigenbasis
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#making-the-pagerank-algorithm" title="Making the PageRank algorithm" class="md-nav__link">
    Making the PageRank algorithm
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pagerank" title="PageRank" class="md-nav__link">
    PageRank
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary_1" title="Summary" class="md-nav__link">
    Summary
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap-up" title="Wrap up" class="md-nav__link">
    Wrap up
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../calculus/" title="Calculus" class="md-nav__link">
      Calculus
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#course-resources" title="Course resources" class="md-nav__link">
    Course resources
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-1-introduction-to-linear-algebra" title="Week 1: Introduction to Linear Algebra" class="md-nav__link">
    Week 1: Introduction to Linear Algebra
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-relationship-between-machine-learning-linear-algebra-vectors-and-matrices" title="The relationship between machine learning, linear algebra, vectors and matrices" class="md-nav__link">
    The relationship between machine learning, linear algebra, vectors and matrices
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#motivations-for-linear-algebra" title="Motivations for linear algebra" class="md-nav__link">
    Motivations for linear algebra
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#conclusions" title="Conclusions" class="md-nav__link">
    Conclusions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#geometric-and-numeric-interpretations" title="Geometric and Numeric Interpretations" class="md-nav__link">
    Geometric and Numeric Interpretations
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vectors" title="Vectors" class="md-nav__link">
    Vectors
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#getting-a-handle-on-vectors" title="Getting a handle on vectors" class="md-nav__link">
    Getting a handle on vectors
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#operations-with-vectors" title="Operations with vectors" class="md-nav__link">
    Operations with vectors
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-2-vectors-are-objects-that-move-around-space" title="Week 2: Vectors are Objects that Move Around Space" class="md-nav__link">
    Week 2: Vectors are Objects that Move Around Space
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#finding-the-size-of-a-vector-its-angle-and-projection" title="Finding the size of a vector, its angle, and projection" class="md-nav__link">
    Finding the size of a vector, its angle, and projection
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#modulus-inner-product" title="Modulus &amp; inner product" class="md-nav__link">
    Modulus &amp; inner product
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cosine-dot-product" title="Cosine &amp; dot product" class="md-nav__link">
    Cosine &amp; dot product
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#projection" title="Projection" class="md-nav__link">
    Projection
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#changing-the-reference-frame" title="Changing the reference frame" class="md-nav__link">
    Changing the reference frame
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#changing-basis" title="Changing basis" class="md-nav__link">
    Changing basis
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#conclusions_1" title="Conclusions" class="md-nav__link">
    Conclusions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#basis-vector-space-and-linear-independence" title="Basis, vector space, and linear independence" class="md-nav__link">
    Basis, vector space, and linear independence
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#conclusions_2" title="Conclusions" class="md-nav__link">
    Conclusions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#applications-of-changing-basis" title="Applications of changing basis" class="md-nav__link">
    Applications of changing basis
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary-of-week-2" title="Summary of week 2" class="md-nav__link">
    Summary of week 2
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#week-3-matrices-as-objects-that-operate-on-vectors" title="Week 3: Matrices as Objects that Operate on Vectors" class="md-nav__link">
    Week 3: Matrices as Objects that Operate on Vectors
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#matrices" title="Matrices" class="md-nav__link">
    Matrices
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#introduction-to-matrices" title="Introduction to matrices" class="md-nav__link">
    Introduction to matrices
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#conclusions_3" title="Conclusions" class="md-nav__link">
    Conclusions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#matrices-as-objects-that-operate-on-vectors" title="Matrices as objects that operate on vectors" class="md-nav__link">
    Matrices as objects that operate on vectors
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-matrices-transform-space" title="How matrices transform space" class="md-nav__link">
    How matrices transform space
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#types-of-matrix-transformation" title="Types of matrix transformation" class="md-nav__link">
    Types of matrix transformation
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#conclusions_4" title="Conclusions" class="md-nav__link">
    Conclusions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#composition-or-combination-of-matrix-transformations" title="Composition or combination of matrix transformations" class="md-nav__link">
    Composition or combination of matrix transformations
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#matrix-inverses" title="Matrix inverses" class="md-nav__link">
    Matrix inverses
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gaussian-elimination-solving-the-apples-and-bananas-problem" title="Gaussian elimination: Solving the apples and bananas problem" class="md-nav__link">
    Gaussian elimination: Solving the apples and bananas problem
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#from-gaussian-elimination-to-finding-the-inverse-matrix" title="From Gaussian elimination to finding the inverse matrix" class="md-nav__link">
    From Gaussian elimination to finding the inverse matrix
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#special-matrices" title="Special matrices" class="md-nav__link">
    Special matrices
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-determinant" title="The determinant" class="md-nav__link">
    The determinant
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#a-determinant-of-zero" title="A determinant of zero" class="md-nav__link">
    A determinant of zero
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a-negative-determinant" title="A negative determinant" class="md-nav__link">
    A negative determinant
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-the-determinant-means-numerically" title="What the determinant means numerically" class="md-nav__link">
    What the determinant means numerically
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary" title="Summary" class="md-nav__link">
    Summary
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#week-4-matrices-make-linear-mappings" title="Week 4: Matrices Make Linear Mappings" class="md-nav__link">
    Week 4: Matrices Make Linear Mappings
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#matrices-as-objects-that-map-one-vector-onto-another" title="Matrices as objects that map one vector onto another" class="md-nav__link">
    Matrices as objects that map one vector onto another
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#introduction-to-einstein-summation-convention-and-the-symmetry-of-the-dot-product" title="Introduction to Einstein summation convention and the symmetry of the dot product" class="md-nav__link">
    Introduction to Einstein summation convention and the symmetry of the dot product
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#matrices-transform-into-the-new-basis-vector-set" title="Matrices transform into the new basis vector set" class="md-nav__link">
    Matrices transform into the new basis vector set
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#matrices-changing-basis" title="Matrices changing basis" class="md-nav__link">
    Matrices changing basis
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#generalizing" title="Generalizing" class="md-nav__link">
    Generalizing
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#doing-a-transformation-in-a-changed-basis" title="Doing a transformation in a changed basis" class="md-nav__link">
    Doing a transformation in a changed basis
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#generalizing_1" title="Generalizing" class="md-nav__link">
    Generalizing
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#making-multiple-mappings-deciding-if-these-are-reversible" title="Making multiple mappings, deciding if these are reversible" class="md-nav__link">
    Making multiple mappings, deciding if these are reversible
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonal-matrices" title="Orthogonal matrices" class="md-nav__link">
    Orthogonal matrices
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#recognizing-mapping-matrices-and-applying-these-to-data" title="Recognizing mapping matrices and applying these to data" class="md-nav__link">
    Recognizing mapping matrices and applying these to data
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-gramschmidt-process" title="The Gram–Schmidt process" class="md-nav__link">
    The Gram–Schmidt process
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reflecting-in-a-plane" title="Reflecting in a plane" class="md-nav__link">
    Reflecting in a plane
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#week-5-eigenvalues-and-eigenvectors" title="Week 5: Eigenvalues and Eigenvectors" class="md-nav__link">
    Week 5: Eigenvalues and Eigenvectors
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#learning-objectives" title="Learning Objectives" class="md-nav__link">
    Learning Objectives
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-are-eigen-things" title="What are eigen-things?" class="md-nav__link">
    What are eigen-things?
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-are-eigenvalues-and-eigenvectors" title="What are eigenvalues and eigenvectors?" class="md-nav__link">
    What are eigenvalues and eigenvectors?
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#getting-into-the-detail-of-eigenproblems" title="Getting into the detail of eigenproblems" class="md-nav__link">
    Getting into the detail of eigenproblems
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#special-eigen-cases" title="Special eigen-cases" class="md-nav__link">
    Special eigen-cases
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calculating-eigenvectors" title="Calculating eigenvectors" class="md-nav__link">
    Calculating eigenvectors
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#when-changing-to-the-eigenbasis-is-really-useful" title="When changing to the eigenbasis is really useful" class="md-nav__link">
    When changing to the eigenbasis is really useful
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#changing-to-the-eigenbasis" title="Changing to the eigenbasis" class="md-nav__link">
    Changing to the eigenbasis
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#making-the-pagerank-algorithm" title="Making the PageRank algorithm" class="md-nav__link">
    Making the PageRank algorithm
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pagerank" title="PageRank" class="md-nav__link">
    PageRank
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary_1" title="Summary" class="md-nav__link">
    Summary
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap-up" title="Wrap up" class="md-nav__link">
    Wrap up
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="linear-algebra">Linear Algebra</h1>
<h2 id="course-resources">Course resources</h2>
<p>There are lots of useful web resources on <strong>linear algebra</strong>. Typically they go a bit slower or have a different emphasis or way of explaining things, but it can be handy to see how someone else explains something.</p>
<ul>
<li><strong>Khan Academy</strong> is a great resource right up to 1st or 2nd year undergraduate material. For this course, there's a handy group of videos <a href="https://www.khanacademy.org/math/linear-algebra">here</a>.</li>
<li><strong>Grant Sanderson</strong> has a great series of videos developing mathematical intuition on YouTube, which you can reach through his site <a href="http://www.3Blue1Brown.com">here</a>.</li>
<li><strong>Wikipedia</strong> gets better every year - and the <a href="https://en.wikipedia.org/wiki/Linear_algebra">linear algebra wikipedia pages</a> are actually pretty good.</li>
</ul>
<h2 id="week-1-introduction-to-linear-algebra">Week 1: Introduction to Linear Algebra</h2>
<p>In this first module we look at how linear algebra is relevant to machine learning and data science. Then we'll wind up the module with an initial introduction to vectors. Throughout, we're focussing on developing your mathematical intuition, not of crunching through algebra or doing long pen-and-paper examples. For many of these operations, there are callable functions in Python that can do the math - the point is to appreciate what they do and how they work, so that when things go wrong or there are special cases, you can understand why and what to do.</p>
<p><em>Learning Objectives</em></p>
<ul>
<li>Recall how machine learning and vectors and matrices are related</li>
<li>Interpret how changes in the model parameters affect the quality of the fit to the training data</li>
<li>Recognize that variations in the model parameters are vectors on the response surface - that vectors are a generic concept not limited to a physical real space</li>
<li>Use substitution / elimination to solve a fairly easy linear algebra problem</li>
<li>Understand how to add vectors and multiply by a scalar number</li>
</ul>
<h3 id="the-relationship-between-machine-learning-linear-algebra-vectors-and-matrices">The relationship between machine learning, linear algebra, vectors and matrices</h3>
<h4 id="motivations-for-linear-algebra">Motivations for linear algebra</h4>
<p>Lets take a look at the types of problems we might want to solve, in order to expose what <strong>linear algebra</strong> is and how it might help us to solve them.</p>
<p><strong>Toy problem 1</strong></p>
<p>The first problem we might think of is <a href="http://www.wikiwand.com/en/Price_discovery"><strong>price discovery</strong></a>. We can illustrate this problem with a toy example.</p>
<p>Say we go shopping on two occasions, and the first time we buy two apples and three bananas and they cost eight Euros</p>
<p>
<script type="math/tex; mode=display">2a + 3b = 8</script>
</p>
<p>and the second time we buy ten apples and one banana, for a cost of 13 Euros.</p>
<p>
<script type="math/tex; mode=display">10a + 1b = 13</script>
</p>
<p>The \(a\)'s and \(b\)'s here, are the price of a single apple and a single banana. What we're going to have to do is solve these <a href="http://www.wikiwand.com/en/System_of_equations"><strong>simultaneous equations</strong></a> in order to discover the price of <em>individual apples and bananas</em>. Now in the general case, with lots of different types of items and lots of shopping trips, finding out the prices might be <em>quite hard</em>.  </p>
<p>This is an example of a linear algebra problem. I have some constant linear coefficients here (2, 10, 3, 1), that relate the <strong>input variables</strong>, \(a\) and \(b\), to the <strong>outputs</strong> 8 and 13. That is if, we think about a vector \([a,b]\) that describes the prices of apples and bananas, we can write this down as a matrix problem where the 2, 3 is my first trip, and the 10, 1 is my second trip,</p>
<p>
<script type="math/tex; mode=display">\begin{pmatrix}2 & 3 \\\ 10 & 1\end{pmatrix} \begin{bmatrix}a \\\ b\end{bmatrix} = \begin{bmatrix} 8 \\\ 3\end{bmatrix}</script>
</p>
<p>What we're going to do over the course of weeks one to three, is to look at these mathematical objects, <a href="http://www.wikiwand.com/en/Vector_(mathematics_and_physics)"><strong>vectors</strong></a> and <a href="http://www.wikiwand.com/en/Matrix_(mathematics)"><strong>matrices</strong></a>, in order to understand what they are and how to work with them.</p>
<p><strong>Toy problem 2</strong></p>
<p>Another type of problem we might be interested in solving is <em>fitting an equation to some data</em>. In fact, with neural networks and machine learning, we want the computer to, in effect, not only fit the equation to the data but to figure out <em>what equation to use</em>.</p>
<p>Let's say, we have some data like this histogram here:</p>
<p><img alt="" src="../img/simple_histogram.png" /></p>
<p>This looks like a <strong>population</strong> with an <strong>average</strong> and some <strong>variation</strong>. A common problem we  might want to solve is how to find the <em>optimal value of the parameters</em> in the equation describing this line, i.e., the ones that fit the data in the histogram best.</p>
<p>That might be really handy, because with that fitted equation we'd have an easy "portable" description of the population we could carry around, without needing all the original data which would free us, for example, from privacy concerns.</p>
<h5 id="conclusions">Conclusions</h5>
<p>In this video, we've set up two problems that can be solved with linear algebra. First, the problem of solving <strong>simultaneous equations</strong>. And secondly, the <strong>optimization problem</strong> of fitting and equation with some parameters to data. These problems and others will motivate our work right through the course on linear algebra, and it's partner course on <a href="#multivariate-calculus">multivariate calculus</a>.</p>
<h4 id="geometric-and-numeric-interpretations">Geometric and Numeric Interpretations</h4>
<p>It is helpful to draw a distinction from the <strong>numerical operations</strong> we can perform using linear algebra, and the <strong>geometric intuitions</strong> underlying them (which are frequently not taught in may introductory courses).</p>
<p>Roughly speaking, the <em>geometric understanding or intuition</em> is what lets us judge what tools to use to solve specific problems, feel why they work, and know how to interpret the results. The <em>numerical understanding</em> is what lets us actually carry through the application of those tools.</p>
<p>If you learn linear algebra without getting a solid foundation in that geometric understanding, the problems can go unnoticed for a while, until you go deeper into whatever field you happen to pursue (e.g. computer science, engineering, statistics, economics, etc.), at which point you may feel disheartened by your lack of understanding of the fundamentals of linear algebra.</p>
<p>With linear algebra (much like trigonometry, for example), there are a handful of useful visual/geometric intuitions underlying much of the subject. When you digest these and really understand the relationship between the geometry and the numbers, the details of the subject as well as how it's used in practice start to feel a lot more reasonable.</p>
<blockquote>
<p>Full credit for this section goes to <a href="https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw">3Blue1Brown</a>. Video <a href="https://youtu.be/kjBOesZCoqc">here</a>.</p>
</blockquote>
<h3 id="vectors">Vectors</h3>
<p>The first thing we need to do in this course on linear algebra is to get a handle on <strong>vectors</strong>, which will turn out to be really useful to us in solving the linear algebra problems we introduced earlier (along with many more!). That is, problems described by equations which are <em>linear in their coefficients</em>, such as most fitting parameters.</p>
<blockquote>
<p>This section maps most closely the the set of Khan Academy courses <a href="https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/vectors/v/vector-introduction-linear-algebra">here</a>. Take these for more practice.</p>
</blockquote>
<h4 id="getting-a-handle-on-vectors">Getting a handle on vectors</h4>
<p>We're going to first step back and look in some detail at the sort of things we're trying to do with data. And why those vectors you first learned about in high school were even relevant. This will hopefully make all the work with vectors later on in the course a lot more intuitive.</p>
<blockquote>
<p>Note: this actually is not a great introduction to vectors (IMO). I recommend you first watch <a href="https://youtu.be/fNk_zzaMoSs">this</a> video, then come back and read this section.</p>
</blockquote>
<p>Let's go back to that simpler problem from the last video, the histogram distribution of heights of people in the population:</p>
<p><img alt="" src="../img/histogram_heights.png" /></p>
<p>Say we wanted to try fitting that distribution with an equation describing the variation of height in the population. It turns our that such an equation has just two parameters; one describing the center of the distribution (the <a href="http://www.wikiwand.com/en/Arithmetic_mean"><strong>average</strong></a>), which we'll call \(\mu\), and one describing how wide it is (or the <a href="http://www.wikiwand.com/en/Variance"><strong>variance</strong></a>), which we'll call \(\sigma\) .</p>
<p>This equation turns out to be the equation for the <strong>normal</strong> or <a href="http://www.wikiwand.com/en/Normal_distribution">(<strong>Gaussian</strong>) <strong>distribution</strong></a>:</p>
<p>
<script type="math/tex; mode=display">f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{\frac{-(x-\mu)^2}{2\sigma^2}}</script>
</p>
<p>So how do we arrive at the best possible values for \(\mu\) and \(\sigma\)? Well, one way is <a href="http://www.wikiwand.com/en/Gradient_descent"><strong>gradient descent</strong></a>. If we think of some <strong>goodness</strong> value which measures how well our parameters fit our data (say, the <a href="http://www.wikiwand.com/en/Mean_squared_error"><strong>mean squared error</strong></a>) we could imagine plotting this goodness value as a function of our parameters, often called a <strong>cost</strong> or <a href="http://www.wikiwand.com/en/Loss_function"><strong>loss</strong></a> function.</p>
<p>The closer our loss function is to zero, the better our parameters fit our data. Gradient descent allows us to choose values for our parameters that minimize the <strong>error</strong>, as measured by our loss function, by taking small incremental steps towards the bottom of the parameter space defined by our loss function.</p>
<p><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/6/68/Gradient_ascent_%28surface%29.png" /></p>
<blockquote>
<p>Gradient descent on a 3D surface.</p>
</blockquote>
<p>This process involves computing the partial derivative of our loss function \(w.r.t\) to all possible parameters (also known as the <a href="http://www.wikiwand.com/en/Gradient"><strong>gradient</strong></a>). If our parameters are stored in a vector, \(<script type="math/tex; mode=display">\begin{bmatrix}\mu&\sigma \end{bmatrix}</script>\), we could subtract from this vector the vector of gradients, \(<script type="math/tex; mode=display">\begin{bmatrix}\frac{\partial f}{\partial \mu} & \frac{\partial f}{\partial \sigma}\end{bmatrix}</script>\) in order to complete the computation in (effectively) one step.</p>
<p>So vectors (and calculus) give us a computational means of navigating a parameter space, in this case by determining the set of parameters for a function \(f(x)\) which best explain the data.</p>
<p><strong>Vectors as abstract lists of numbers</strong></p>
<p>We can also think of vectors as simply <em>lists of numbers</em>. For example, we could describe a car in terms of its price, top speed, safety rating, emissions performance, etc. and store these numbers in a single <em>vector</em>.</p>
<p>
<script type="math/tex; mode=display">car = \begin{bmatrix}\text{price,} & \text{top speed,} & \text{safety rating, }& ...\end{bmatrix}</script>
</p>
<blockquote>
<p>Note that this is more of a 'computer science' perspective of vectors.</p>
</blockquote>
<p>To summarize, a vector is, at the simplest level:</p>
<ul>
<li>lists of numbers</li>
<li>something which moves in a space of parameters</li>
</ul>
<h4 id="operations-with-vectors">Operations with vectors</h4>
<p>Lets now explore the <strong>operations</strong> we can do with vectors, how these mathematical operations define what vectors are in the first place, and the sort of spaces they can apply to.</p>
<p>We can think of a <strong>vector</strong> as an <strong>object</strong> that moves us about space. This could be a physical space, or a space of data (often called a <a href="http://www.wikiwand.com/en/Vector_space"><strong>vector space</strong></a>).</p>
<blockquote>
<p>At school, you probably thought of a vector as something that moved you around a physical space, but in computer and data science, we generalize that idea to think of a vector as just a list of attributes of an objects.</p>
</blockquote>
<p>More formally, mathematics generalizes the definition of a vector to be an object for which the following two operations are <em>defined</em>:</p>
<ol>
<li><strong>addition</strong></li>
<li><strong>multiplication</strong> by a scalar</li>
</ol>
<blockquote>
<p>This is really important, make sure you understand it. If a mathematical object can be added to another object of the same type, and it can be scaled (i.e. multiplied by a scaler), then its a vector!</p>
</blockquote>
<p><strong>Vector addition</strong></p>
<p>Intuitively, we can introduce <strong>vector addition</strong> as being the resulting vector of the two vectors we want to add (\(s\) and \(r\)) being placed <em>head-to-tail</em>, \(s + r\) .</p>
<p><img alt="" src="../img/vector_addition.png" /></p>
<p><strong>Multiplication by a scalar</strong></p>
<p>Multiplying a vector by a <strong>scalar</strong> is also easy to understand. In this case, we simply multiply all elements of our <strong>vector</strong> \(r\) by the scalar, \(a\) for example.</p>
<p><img alt="" src="../img/scalar_multiplication.png" /></p>
<p><strong>Coordinate systems</strong></p>
<p>At this point, it's convenient to define a <strong>coordinate system</strong>. Imagine we had two dimensions defined by the vectors:</p>
<p>
<script type="math/tex; mode=display">\hat i = \begin{bmatrix}1 \\\ 0 \end{bmatrix} \; \hat j = \begin{bmatrix} 1 \\\ 0 \end{bmatrix}</script>
</p>
<blockquote>
<p>These are known as <strong>basis vectors</strong>, and they define the <a href="http://www.wikiwand.com/en/Basis_(linear_algebra)"><strong>basis</strong></a>.</p>
</blockquote>
<p>We could define any vector in this 2D space using the vectors \(\hat i\) and \(\hat j\) . For example, the vector</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix}3 \\\ 2 \end{bmatrix} = 3 \hat i + 2 \hat j</script>
</p>
<p><img alt="" src="../img/basis_vectors.png" /></p>
<blockquote>
<p>This is also a extremely important point. A vector space is itself <em>defined by vectors</em>. We will explore this further later in the course.</p>
</blockquote>
<p>This also nicely illustrates that vectors are <strong>associative</strong>, meaning, the sum of a series of vectors is the same regardless of the order we add them in, e.g.,</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix} 3 \\\ 2 \end{bmatrix} = 3 \hat i + 2 \hat{j} = 2 \hat{j} + 3 \hat i</script>
</p>
<p><strong>Conclusions</strong></p>
<p>We've defined <em>two</em> fundamental operations that vectors satisfy: <strong>addition</strong>, (e.g. \(r + s\)), and <strong>multiplication</strong> by a <em>scalar</em>, (e.g. \(2r\)). We've noted that it can be useful to define a coordinate system in which to do our <strong>addition</strong> and <strong>scaling</strong>, e.g.,</p>
<p>
<script type="math/tex; mode=display">  r = \begin{bmatrix}3 \\\ 2 \end{bmatrix} = 3 \hat i + 2 \hat{j}</script>
</p>
<p>using these fundamental <strong>basis</strong> vectors, \(\hat i\) and \(\hat{j}\), and explored the properties that this implies, like <strong>associativity</strong> of addition and subtraction.</p>
<p>We've also seen that although, perhaps, it's <em>easiest</em> to think of vector operations <em>geometrically</em>, we don't have to do it in a real (number) space. We can also define vector operations on vectors that list different types of objects, like the <em>attributes of a house</em>.</p>
<h2 id="week-2-vectors-are-objects-that-move-around-space">Week 2: Vectors are Objects that Move Around Space</h2>
<p>In this module, we will look at the types operations we can do with vectors - finding the modulus or magnitude (size), finding the angle between vectors (dot or inner product) and projecting one vector onto another. We will then examine how the entries describing a vector will depend on what vectors we use to define the axes - the basis. That will then let us determine whether a proposed set of basis vectors are <a href="http://www.wikiwand.com/en/Linear_independence"><strong>linearly independent</strong></a>.</p>
<p>This will complete our examination of vectors, allowing us to move on to matrices and then to begin solving linear algebra problems.</p>
<p><em>Learning Objectives</em></p>
<ul>
<li>Calculate basic operations (dot product, modulus, negation) on vectors</li>
<li>Calculate a change of basis</li>
<li>Recall linear independence</li>
<li>Identify a linearly independent basis and relate this to the dimensionality of the space</li>
</ul>
<h3 id="finding-the-size-of-a-vector-its-angle-and-projection">Finding the size of a vector, its angle, and projection</h3>
<blockquote>
<p>It is probably worth it to watch <a href="https://youtu.be/LyGKycYT2v0">this</a> 3Blue1Brown video first before reading through this section. However be warned, its the most confusing one in the series. If you want even more practice, check out <a href="https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/dot-cross-products/v/vector-dot-product-and-vector-length">this</a> Khan Academy track.</p>
</blockquote>
<h4 id="modulus-inner-product">Modulus &amp; inner product</h4>
<p>Previously we looked at the two main vector operations of <strong>addition</strong> and <strong>scaling</strong> by a number (multiplication by a <strong>scalar</strong>). As it turns out, those are really the only operations we need to be able to do in order define something as a vector.</p>
<p>Now, we can move on to define two new ideas: the <strong>length of a vector</strong>, also called its <em>size</em>, and the <a href="http://www.wikiwand.com/en/Dot_product"><strong>dot product</strong></a> of a vector, also called its <em>inner</em>, <em>scalar</em> or <em>projection</em> product.</p>
<blockquote>
<p>The dot product is a huge and amazing concept in linear algebra with a huge number of implications, and we'll only be able to touch on a few parts of it here, but enjoy. It's one of the most beautiful parts of linear algebra</p>
</blockquote>
<p><strong>Length of a vector</strong></p>
<p>Lets define a vector \(r\) using the basis vectors we introduced earlier, \(i\) and \(j\),</p>
<p>
<script type="math/tex; mode=display">  r = a   i + b   j = \begin{bmatrix} a \\\ b \end{bmatrix}</script>
</p>
<p>To calculate the length of \(r\), also called the <a href="http://www.wikiwand.com/en/Norm_(mathematics)"><strong>norm</strong></a> \(\vert r\vert\) (or \(\Vert r\Vert\)), we could imagine drawing a triangle, with our vector \(r\) as the hypotenuse:</p>
<p><img alt="vector_length" src="../img/vector_length.png" /></p>
<blockquote>
<p>The length, magnitude, modulus and norm of a vector are all the same thing, and just represent a difference in terminology. If we are thinking of a vector as representing the line segment from the origin to a given point (i.e., the geometric interpretation), we may interpret the <strong>norm</strong> as the <em>length</em> of this line segment. If we are thinking of a vector as representing a physical quantity like acceleration or velocity, we may interpret the norm as the <strong>magnitude</strong> of this quantity (how "<em>large</em>" it is, regardless of its direction).</p>
</blockquote>
<p>By <a href="http://www.wikiwand.com/en/Pythagorean_theorem"><strong>Pythagorus's Theorem</strong></a>, \(\vert r \vert = \sqrt{a^2 + b^2}\)</p>
<p><strong>Vector dot product</strong></p>
<p>The <a href="http://www.wikiwand.com/en/Dot_product"><strong>dot product</strong></a> is one of several ways of multiplying two vectors together, specifically, it is an <em>algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors) and returns a single number</em>.</p>
<p>The dot product has an <strong>algebraic</strong> and <strong>geometric</strong> interpretation. <strong>Algebraically</strong>, the dot product is the <em>sum of the products of the corresponding entries of the two sequences of numbers</em>. <strong>Geometrically</strong>, it is the <em>product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them</em>.</p>
<p><strong>Algebraic definition of the dot product</strong></p>
<p>To illustrate the algebraic definition of the dot product, lets define two vectors \(r\) and \(s\):</p>
<p>
<script type="math/tex; mode=display">  r = \begin{bmatrix} r_i \\\ r_j\end{bmatrix}</script>
<script type="math/tex; mode=display">s = \begin{bmatrix} s_i \\\ s_j\end{bmatrix}</script>
</p>
<p><img alt="dot_product" src="../img/dot_product.png" /></p>
<p>The dot product is then:</p>
<p>
<script type="math/tex; mode=display">  r \cdot s = r_is_i + r_j s_j = (3)(-1) + (2)(2) = 1</script>
</p>
<p>More formally, the algebraic definition of the dot product is:</p>
<p>
<script type="math/tex; mode=display">  r \cdot s = \sum^n_{i=1}a_ib_i = a_1b_1 + a_2b_2 + ... + a_nb_n</script>
</p>
<blockquote>
<p>The definition of the dot product is a simple multiplication of each component from the both vectors added together.</p>
</blockquote>
<p><strong>Properties of the dot product</strong></p>
<p>The dot product is,</p>
<ul>
<li><a href="http://www.wikiwand.com/en/Commutative_property"><strong>commutative</strong></a>, e.g., \(r \cdot s = s \cdot r\)</li>
<li><a href="http://www.wikiwand.com/en/Distributive_property"><strong>distributive</strong></a>, e.g., \(r \cdot (s +  {t}) =   r \cdot s +   r \cdot  {t}\)</li>
<li><a href="http://www.wikiwand.com/en/Associative_property"><strong>associative</strong></a> over scalar multiplication, e.g., \(r \cdot (a s) = a (   r \cdot s)\)</li>
</ul>
<p>Lets prove the <strong>distributive</strong> property in the general case. Let:</p>
<p>
<script type="math/tex; mode=display">  r = \begin{bmatrix} r_1 \\\ r_2 \\\ . \\\ . \\\ . \\\ r_n \end{bmatrix}, \; s = \begin{bmatrix} s_1 \\\ s_2 \\\ . \\\ . \\\ . \\\ s_n \end{bmatrix}, \;  {t} = \begin{bmatrix} t_1 \\\ t_2 \\\ . \\\ . \\\ . \\\ t_n \end{bmatrix}</script>
</p>
<p>Then,</p>
<p>
<script type="math/tex; mode=display">r \cdot (s +  {t}) = r_1 (s_1 + t_1) + r_2 (s_2 + t_2) + ... + r_n (s_n + t_n)</script>
<script type="math/tex; mode=display">= r_1 s_1 + r_1t_1 + r_2s_2 + r_2t_2 + ... + r_ns_n + r_nt_n</script>
<script type="math/tex; mode=display">=   r \cdot s +   r \cdot  {t}</script>
</p>
<blockquote>
<p>Proofs for the remaining properties are left to an exercise.</p>
</blockquote>
<p><strong>Link between the dot product and the size of the vector</strong></p>
<p>If we take \(r\) and dot it with itself, we get:</p>
<p>
<script type="math/tex; mode=display">r_i r_i + r_j r_j + ... + r_nr_n = r_i^2 + r_j^2 + ... + r_n^2 = \vert r \vert^2</script>
</p>
<p>So, the size of the vector is just given by \(r\) dotted with itself and squared.</p>
<h4 id="cosine-dot-product">Cosine &amp; dot product</h4>
<p>Lets take the time to derive the <strong>geometric</strong> definition of the dot product.</p>
<blockquote>
<p>Recall, <strong>geometrically</strong>, the dot product is the <em>product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them</em>.</p>
</blockquote>
<p>We start with the <a href="http://www.wikiwand.com/en/Law_of_cosines"><strong>law of cosines</strong></a> (also known as the <strong>cosine formula</strong> or <strong>cosine rule</strong>) from algebra, which you'll remember, probably vaguely, from school.</p>
<p>The law of cosines states that if we had a triangle with sides \(a\), \(b\), and \(c\), then:</p>
<p>
<script type="math/tex; mode=display">c^2 = a^2 + b^2 - 2ab \cos \theta</script>
</p>
<p>Now, we can translate this into our vector notation:</p>
<p><img alt="cosine_rule" src="../img/cosine_rule.png" /></p>
<p>
<script type="math/tex; mode=display">\vert r - s\vert ^2 = \vert r\vert ^2 + \vert s\vert ^2 - 2\vert r\vert \vert s\vert \cos \theta</script>
</p>
<p><strong>LHS</strong></p>
<p>
<script type="math/tex; mode=display">\Rightarrow (r-s) \cdot (r-s) = r \cdot r - s \cdot r - s \cdot r - s \cdot s</script>
<script type="math/tex; mode=display">= \vert r\vert ^2 - 2 s \cdot r + \vert s\vert ^2</script>
</p>
<blockquote>
<p>\(\vert r - s\vert ^2 = (r-s) \cdot (r-s)\) comes straight from the definition of the dot product.</p>
</blockquote>
<p><strong>LHS = RHS</strong></p>
<p>
<script type="math/tex; mode=display">\Rightarrow \vert r\vert ^2 - 2 s \cdot r + \vert s\vert ^2 = \vert r\vert ^2 + \vert s\vert ^2 - 2\vert r\vert \vert s\vert \cos \theta</script>
<script type="math/tex; mode=display">\Rightarrow   r \cdot s = \vert r\vert \vert s\vert  \cos \theta</script>
</p>
<p>So what we notice is that the dot product does something quite <em>profound</em>. It takes the size of the two vectors (\(\vert r\vert , \vert s \vert\)) and multiplies them by \(\cos\) of the angle between them. It tells us <em>something</em> about the extent to which the two vectors go in the same direction.</p>
<ul>
<li>If \(\theta\) is zero, then \(\cos \theta\) is one and \(r \cdot s\) would just be the size of the two vectors multiplied together.</li>
<li>If \(\theta\) is \(90\) degrees (<em>i.e.</em> \(r\) and \(s\) are orthogonal), then \(\cos 90\), is \(0\) and \(r \cdot s\) is \(0\) .</li>
</ul>
<p>More generally,</p>
<p><img alt="cosine_similarity" src="../img/cosine_similarity.png" /></p>
<blockquote>
<p>Ignore the word "score" here, this image was taken from a <a href="http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/">blog post</a> about machine learning. The blog post is worth checking out though. Full credit to <a href="http://blog.christianperone.com">Christian S. Perone</a> for the image.</p>
</blockquote>
<p>In this way, the dot product captures whether the two vectors are pointing in similar directions (positive) or opposite directions (negative).</p>
<h4 id="projection">Projection</h4>
<blockquote>
<p>Understanding projection can be a little tricky. If you want even more practice, check out <a href="https://www.khanacademy.org/math/linear-algebra/alternate-bases#orthogonal-projections">this</a> Khan Academy series.</p>
</blockquote>
<p>The <strong>vector projection</strong> of a vector \(s\) on (or onto) a nonzero vector \(r\) (also known as the <strong>vector component</strong> or <strong>vector resolution</strong> of \(s\) in the direction of \(r\)) is the orthogonal projection of \(s\) onto a straight line parallel to \(r\) .</p>
<p>For the following triangle,</p>
<p><img alt="projection" src="../img/projection.png" /></p>
<p>Recall the geometric definition of the dot product:</p>
<p>
<script type="math/tex; mode=display">  r \cdot s = \vert r\vert  \vert s\vert  \cos \theta</script>
</p>
<p>Notice that \(\vert s\vert  \cos \theta\) is the <em>length</em> of the <strong>adjacent</strong> side (adjacent to the angle shown). This term is the projection of the vector \(s\) into (or onto) the vector \(r\) . This is why the dot product <em>is also called</em> the <strong>projection product</strong>, because it takes the projection of one vector (\(s\)) onto another (\(r\)) times the magnitude or length of the other (\(\vert r \vert\)).</p>
<blockquote>
<p>Note again that if \(s\) was orthogonal to \(r\) then \(\vert s\vert  \cos \theta = \vert s\vert  \cos 90 = 0 = r \cdot s\) . This provides a convenient way to check for orthogonality.</p>
</blockquote>
<p>Rearranging, we can compute the <a href="http://www.wikiwand.com/en/Scalar_projection"><strong>scalar projection</strong></a> of \(s\) on \(r\):</p>
<p>
<script type="math/tex; mode=display">r \cdot s = \vert r\vert\vert s\vert  \cos \theta</script>
<script type="math/tex; mode=display">\Rightarrow \frac{  r \cdot s}{\vert r\vert } = \vert s\vert  \cos \theta</script>
</p>
<p>The scalar projection is a <em>scalar</em>, equal to the length of the orthogonal projection of \(s\) on \(r\), with a negative sign if the projection has an opposite direction with respect to \(r\) .</p>
<p>We can also define the <a href="http://www.wikiwand.com/en/Vector_projection"><strong>vector projection</strong></a></p>
<p>
<script type="math/tex; mode=display">r \cdot s = \vert r\vert\vert s\vert  \cos \theta</script>
<script type="math/tex; mode=display">\Rightarrow \frac{  r \cdot s}{\vert r\vert } = \vert s\vert  \cos \theta</script>
<script type="math/tex; mode=display">\Rightarrow \frac{r}{\vert r \vert} \cdot \frac{r \cdot s}{\vert r\vert } = \vert s\vert  \cos \theta \cdot \frac{r}{\vert r \vert}</script>
</p>
<p>which is the orthogonal projection of \(s\) onto a straight line parallel to \(r\). Notice that this formula is intuitive, we take the <strong>scaler projection</strong> of \(s\) onto \(r\) (the length of the orthogonal projection of \(s\) on \(r\)) and multiply it by a unit vector in the direction of \(r\), \(\frac{r}{\vert r \vert}\).</p>
<p><strong>Conclusions</strong></p>
<p>This was really the core video for this week. We found the <strong>size</strong> of a vector and we defined the <strong>dot product</strong>. We've then found out some mathematical operations we can do with the dot product (multiplication by a scalar and the dot product). We also proved that mathematical operations with vectors obey the following properties:</p>
<ul>
<li>commutative</li>
<li>distributive over vector addition</li>
<li>associative with scalar multiplication</li>
</ul>
<p>We then found that the dot product actually captures the <em>angle</em> between two vectors, the extent to which they go in the same direction, and also finds the <em>projection</em> of one vector onto another.</p>
<h3 id="changing-the-reference-frame">Changing the reference frame</h3>
<blockquote>
<p>It is best to watch <a href="https://youtu.be/P2LTAUO1TdA">this</a> video first, then return to and read through this section. If you want even more exposure to these ideas, try the first three sections of the Khan Academy course <a href="https://www.khanacademy.org/math/linear-algebra/alternate-bases">here</a>.</p>
</blockquote>
<h4 id="changing-basis">Changing basis</h4>
<p>So far we haven't really talked about the <a href="http://www.wikiwand.com/en/Coordinate_system"><strong>coordinate system</strong></a> of our <a href="http://www.wikiwand.com/en/Vector_space"><strong>vector space</strong></a>, the coordinates in which all of our vectors exist. In this section we'll look at what we mean by coordinate systems, and walk through a few examples of changing from one coordinate system to another.</p>
<p>Remember that a vector (e.g. \(r\)) is just an object that takes us from the <em>origin</em> to <em>some point in space</em>. This could be some <em>physical</em> space or it could be some <em>data</em> space, like the attributes of a house (bedrooms, price, etc.).</p>
<p>We could use a coordinate system defined itself by vectors, such as the vectors \(\hat{i}\) and \(\hat{j}\) that we defined before. Lets give them names \(\hat{e_1}\) and \(\hat{e_2}\) instead. We will define them to be of unit length, meaning they're of length 1.</p>
<blockquote>
<p>The little hat denotes unit length.</p>
</blockquote>
<p>So,</p>
<p>
<script type="math/tex; mode=display">\hat{e_1} = \begin{bmatrix} 1 \\\ 0 \end{bmatrix}, \; \hat{e_2} = \begin{bmatrix} 0 \\\ 1 \end{bmatrix}</script>
</p>
<blockquote>
<p>if we had more dimension in our space, we could just use more one-hot encoded vectors (\(\hat{e_n}\)) of dimension equal to the dimensions in our space.</p>
</blockquote>
<p>We can then define any other vector in our space in terms of \(\hat{e_1}\) and \(\hat{e_2}\) . For example,</p>
<p>
<script type="math/tex; mode=display">r_e = 3\hat{e_1} + 3 \hat{e_2} = \begin{bmatrix} 3 \\\ 4 \end{bmatrix}</script>
</p>
<p>Here, the instruction is that \(r_e\) is going to be equal to doing a vector sum of \(3 \hat{e_1}\) and \(4 \hat{e_2}\).</p>
<p>If you think about it, our choice of \(\hat{e_1}\) and \(\hat{e_2}\) is kind of arbitrary. There's no reason we couldn't have used different vectors to define our coordinate system</p>
<blockquote>
<p>Indeed, these vectors don't even need to be at 90 degrees to each other or of the same length</p>
</blockquote>
<p>In any case, I could still have described \(r\) as being some sum of some vectors I used to define the space. We call the vectors we use to define our vector space (e.g. \(\hat{e_1}\) and \(\hat{e_2}\)) <strong><a href="http://www.wikiwand.com/en/Basis_(linear_algebra)">basis</a> vectors</strong>.</p>
<p>What we realize here, is that our vector \(r\) exists <em>independently</em> of the coordinate system we use. The vector still takes us from the origin to some point in space, even when we change the coordinate system, more specifically, even when we change the <strong>basis vectors</strong> used to describe our <a href="http://www.wikiwand.com/en/Vector_space"><strong>vector space</strong></a>.</p>
<p>It turns out, we can actually change the basis of the vector \(r\) (call this \(r_e\)) to a new set of basis vectors, i.e. \(\hat{b_1}\) and \(\hat{b_2}\), which we will denote \(r_b\) . Furthermore, we can do this using the dot product so long as</p>
<ol>
<li>The new basis vectors are orthogonal to each other, i.e. \(\hat{b_1} \cdot \hat{b_2} = 0\)</li>
<li>We know the position of \(\hat{b_1}\) and \(\hat{b_2}\) in the space defined by \(\hat{e_1}\) and \(\hat{e_2}\).</li>
</ol>
<blockquote>
<p>We can still change basis even when the new basis vectors are not orthogonal to one another, but for this we will need matrices. See later parts of the course.</p>
</blockquote>
<p>Lets define \(\hat{b_1}\) and \(\hat{b_2}\) in the space defined by \(\hat{e_1}\) and \(\hat{e_2}\):</p>
<p>
<script type="math/tex; mode=display">\hat{b_1} = \begin{bmatrix} -2 \\\ 4 \end{bmatrix}, \; \hat{b_2} = \begin{bmatrix} 2 \\\ 1 \end{bmatrix}</script>
</p>
<p>In order to determine \(r_b\), i.e. the vector \(r\) defined in terms of the basis vectors \(\hat{b_1}\) and \(\hat{b_2}\), we need to take <em>sum</em> the <strong>vector projection</strong> of \(r_e\) onto \(\hat{b_1}\) and the <strong>vector projection</strong> of \(r_e\) onto \(\hat{b_2}\)</p>
<p><img alt="changing_basis" src="../img/changing_basis.png" /></p>
<p>So, lets do it:</p>
<p><strong>Vector projection of \(r_e\) onto \(\hat{b_1}\)</strong></p>
<p>
<script type="math/tex; mode=display">\hat{b_1}\frac{r_e \cdot \hat{b_1}}{\vert \hat{b_1}\vert ^2} = \frac{3 \times 2 + 4 \times 1}{2^2 + 1^2} = \frac{10}{5} = 2 \hat{b_1}  = 2 \begin{bmatrix} 2 \\\ 1\end{bmatrix} = \begin{bmatrix} 4 \\\ 2\end{bmatrix}</script>
</p>
<p><strong>Vector projection of \(r_e\) onto \(\hat{b_2}\)</strong></p>
<p>
<script type="math/tex; mode=display">\hat{b_2}\frac{r_e \cdot \hat{b_2}}{\vert \hat{b_2}\vert ^2} = \frac{3 \times -2 + 4 \times 4}{-2^2 + 4^2} = \frac{10}{20} = \frac{1}{2} \hat{b_2}  = \frac{1}{2} \begin{bmatrix} -2 \\\ 4\end{bmatrix} = \begin{bmatrix} -1 \\\ 2\end{bmatrix}</script>
</p>
<p>Thus,</p>
<p>
<script type="math/tex; mode=display"> r_b = \hat{b_1} \frac{r_e \cdot \hat{b_1}}{\vert \hat{b_1}\vert ^2} + \hat{b_2} \frac{r_e \cdot \hat{b_2}}{\vert \hat{b_2}\vert ^2} = 2 \hat{b_1} + \frac{1}{2} \hat{b_2} = \begin{bmatrix}2 \\\ \frac{1}{2}\end{bmatrix}</script>
</p>
<p>Finally, notice that</p>
<p>
<script type="math/tex; mode=display"> r_b = 2 \hat{b_1} + \frac{1}{2} \hat{b_2} = 2 \begin{bmatrix} 2 \\\ 1\end{bmatrix} + \frac{1}{2} \begin{bmatrix} -2 \\\ 4\end{bmatrix} = \begin{bmatrix} 3 \\\ 4\end{bmatrix} =  r_e</script>
</p>
<h5 id="conclusions_1">Conclusions</h5>
<p>We've seen that our vector describing our data <em>isn't tied to the axis that we originally used to describe it</em>. We can redescribe it using <em>some other axis</em>, <em>some other basis vectors</em>.</p>
<p>It turns out that choosing basis vectors we use to describe the space of data carefully to help us solve our problem will be a very important thing in linear algebra, and in general. We can move the numbers in the vector we used to describe a data item from one basis to another. We can do that change just by taking the <em>dot</em> or <em>projection product</em> so long as the new basis factors are orthogonal to each other.</p>
<h4 id="basis-vector-space-and-linear-independence">Basis, vector space, and linear independence</h4>
<blockquote>
<p>Linear independence is really only brushed on here. To go deeper, check out <a href="https://www.khanacademy.org/math/linear-algebra/alternate-bases/change-of-basis/v/linear-algebra-coordinates-with-respect-to-a-basis">this</a> Khan Academy series.</p>
</blockquote>
<p>Previously we've seen that our basis vectors <em>do not have to be</em> the so called <a href="http://www.wikiwand.com/en/Standard_basis"><strong>standard (or natural) basis</strong></a>. We can actually choose any basis vectors we want, which redefine how we we move about space.</p>
<p><strong>Standard Basis</strong></p>
<p>The set of unit vectors pointing in the direction of the axes of a Cartesian coordinate system.</p>
<p>
<script type="math/tex; mode=display">\hat{e_x} = \begin{bmatrix} 1 \\\ 0 \\\ 0 \end{bmatrix}, \; \hat{e_y} = \begin{bmatrix} 0 \\\ 1 \\\ 0 \end{bmatrix}, \; \hat{e_z} = \begin{bmatrix} 0 \\\ 0 \\\ 1 \end{bmatrix}</script>
</p>
<blockquote>
<p>Also known as the <a href="http://www.wikiwand.com/en/Orthonormal_basis">orthonormal basis</a></p>
</blockquote>
<p>Lets formally define what we mean by a <strong>basis</strong> (vector space), and define <strong>linear independence</strong>, which is going to let us understand how many dimensions our vector space actually has.</p>
<p><strong>Basis</strong></p>
<p>The <strong>basis</strong> is a set of \(n\) vectors that:</p>
<ol>
<li>are not linear combinations of each other (linear independent)</li>
<li>span the space that they describe</li>
</ol>
<p>If these two qualities are fulfilled, then the space defined by the basis is \(n\)-dimensional.</p>
<p><strong>Linear independence</strong></p>
<p>A set of vectors is said to be <a href="http://www.wikiwand.com/en/Linear_independence"><strong>linearly dependent</strong></a> if one of the vectors in the set can be defined as a linear combination of the others; if no vector in the set can be written in this way, then the vectors are said to be <strong>linearly independent</strong>.</p>
<p>For example, imagine we had some candidate vector \({b_3}\) . If we could write \({b_3}\) as a linear combination of, say, \({b_1}\) and \({b_2}\):</p>
<p>
<script type="math/tex; mode=display"> {b_3} = c_1  {b_1} + c_2  {b_2}</script>
</p>
<p>where \(c_1\) and \(c_2\) were constants, then we would say that \({b_3}\) is <em>linearly dependent</em> on \({b_1}\) and \({b_2}\) .</p>
<p><img alt="linear_dependence" src="../img/linear_dependence.png" /></p>
<p>To drive the point home, we note that the following are true if \({b_3}\) is linearly dependent to the vectors \({b_1}\) and \({b_2}\):</p>
<ul>
<li>\({b_3}\) does <strong>not</strong> lie in the plane spanned by \({b_1}\) and \({b_2}\)</li>
<li>\({b_3} \ne c_1  {b_1} + c_2  {b_2}\) for any \(c_1\), \(c_2 \in \mathbb R\) OR, equivalently,</li>
<li>\(0 = c_1  {b_1} + c_2 {b_2} + c_3 {b_3}\) implies that \(c_1 = c_2 = c_3 = 0\)</li>
</ul>
<p>These concepts are central to the definition of <strong>dimension</strong>. As stated previously, if we have a set of \(n\) basis vectors, then these vectors describe an \(n\)-dimensional space, as we can express any \(n\)-dimensional vector as a linear combination of our \(n\) basis vectors.</p>
<p>Now, notice what our basis vectors \({b_n}\) <em>don't</em> have to be.</p>
<ul>
<li>they <em>don't</em> have to be unit vectors, by which we mean vectors of length 1 and</li>
<li>they <em>don't</em> have to be <em>orthogonal</em> (or <em>normal</em>) to each other</li>
</ul>
<p>But, as it turns out, everything is going to be much easier if they are. So if at all possible, you want to construct what's called an <strong>orthonormal basic vector set</strong>, where all vectors of the set are at \(90\) degrees to each other and are all of unit length.</p>
<p>Now, let's think about what happens when we map from one basis to another. The axes of the <em>original grid</em> are projected onto the <em>new grid</em>; and potentially have different values on that new grid, but <em>crucially</em>, the projection keeps the grid being evenly spaced.</p>
<p><img alt="changing_basis" src="../img/changing_basis.gif" /></p>
<p>Therefore, any mapping that we do from one set of basis vectors, (one coordinate system) to another set of basis vectors (another coordinate system), keeps the vector space being a <em>regularly spaced grid</em>, where our original rules of vector addition and multiplication by a scaler still work.</p>
<blockquote>
<p>Basically, it doesn't warp or fold space, which is what the linear bit in linear algebra means geometrically. Things might be stretched or rotated or inverted, but everything remains evenly spaced and linear combinations still work.</p>
</blockquote>
<p>Now, when the new basis vectors aren't orthogonal, then we won't be able to use the dot product (really, the projection) to map from one basis to another. We'll have to use matrices instead, which we'll meet in the next module.</p>
<blockquote>
<p>Honestly, this part is tricky. It might be worth it to watch the first three videos of the <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Essence of Linear Algebra</a> series. For the lazy, jumpy straight to this <a href="https://youtu.be/kYB8IZa5AuE">video</a>.</p>
</blockquote>
<h5 id="conclusions_2">Conclusions</h5>
<p>In this section, we've talked about the dimensionality of a vector space in terms of the number of independent basis factors that it has. We found a test for independence: vectors are independent if one of them is not a linear combination of the others. Finally, we discussed what that means to map vectors from one space to another and how that is going to be useful in data science and machine learning.</p>
<h4 id="applications-of-changing-basis">Applications of changing basis</h4>
<p>Say we have a bunch of 2D data points which all more or less lie on a straight line (i.e., there is a clear <em>linear</em> correlation).</p>
<p><img alt="applications_of_changing_basis" src="../img/applications_of_changing_basis.png" /></p>
<p>I can imagine re-describing that data by "mapping" them onto that line, such that I can say <em>how far along</em> the line and <em>how far from the line</em> a given data point is. Notice that <em>how far from the line</em> the data points are, collectively, is a measure of <em>noise</em> or <em>variance</em>.</p>
<blockquote>
<p>Note that there is a lively debate in the statistics community as to how exactly we calculate <em>how far from the line</em> a data point is (i.e., in a straight line perpendicular to linear model, or straight up from the linear model).</p>
</blockquote>
<p>Thought of in another way, <em>how far from the line</em> the data points are tells us something about how well our line fits the data.</p>
<blockquote>
<p>If the best fit line was not a vey good fit, we would get a much bigger number for the noisiness. And if the best fit line was as good as possible, I get the minimum possible number for the noisiness.</p>
</blockquote>
<p>The way that we have described these two directions (<em>along the line</em> and <em>away from the line</em>), they are orthogonal to each other. So I can use the dot product to do the projection to map the data from the X-Y space onto the space of the line.</p>
<blockquote>
<p>I don't really get the point of this video.</p>
</blockquote>
<h4 id="summary-of-week-2">Summary of week 2</h4>
<p>We've looked at vectors as being objects that describe where we are in space which could be a physical space, a space of data, or a parameter space of the parameters of a function. It doesn't really matter. It's just some space.</p>
<p>Then we've defined vector addition and scaling a vector by a number, making it bigger or reversing its direction. Then we've gone on to find the magnitude or modulus of a vector, and the dot scalar and vector projection product. We've defined the basis of a vector space, its dimension, and the ideas of linear independence and linear combinations. We've used projections to look at one case of changes from one basis to another, for the case where the new basis is orthogonal.</p>
<h3 id="week-3-matrices-as-objects-that-operate-on-vectors">Week 3: Matrices as Objects that Operate on Vectors</h3>
<p>Lets now turn our attention from vectors to <a href="http://www.wikiwand.com/en/Matrix_(mathematics)"><strong>matrices</strong></a>. First we will look at how to use matrices as tools to solve linear algebra problems, before introducing them as objects that <em>transform</em> vectors. We will then explain how to solve systems of linear equations using matrices, which will introduce the concept of inverse matrices and determinants. Finally, we'll look at cases of special matrices: when the determinant is zero, and where the matrix isn't invertible. Because many algorithms require us to invert a matrix as one of their steps, this special case is important.</p>
<p><strong>Learning Objectives</strong></p>
<ul>
<li>Understand what a matrix is and how it corresponds to a transformation</li>
<li>Explain and calculate inverse and determinant of matrices</li>
<li>Identify and explain how to find inverses computationally</li>
<li>Explore what it means for a matrix to be inertible</li>
</ul>
<h3 id="matrices">Matrices</h3>
<h4 id="introduction-to-matrices">Introduction to matrices</h4>
<p>At the start of the course, we encountered the "apples and bananas" problem: how to find the price of things when we only have the total bill. Now we're going to look at matrices, which can be thought of as objects that rotate and stretch vectors, and how they can be used to solve these sorts of problems.</p>
<p>Let's go back to that apples and bananas problem. Say we walk into a shop and we buy two apples, and three bananas and that costs us 8 euros:</p>
<p>
<script type="math/tex; mode=display">2a + 3b = 8</script>
</p>
<p>On another day and we buy 10 apples and 1 banana, and that costs me 13 euros:</p>
<p>
<script type="math/tex; mode=display">10a + 1b = 13</script>
</p>
<blockquote>
<p>Now you might say this is silly. What shop doesn't have sticker prices after all? But actually, businesses with complicated products and service agreements often use <a href="http://www.wikiwand.com/en/Price_discovery"><strong>price discovery</strong></a>.</p>
</blockquote>
<p>Now these are just <strong>simultaneous equations</strong> <em>but</em>, I can write them down with matrices as follows:</p>
<p>
<script type="math/tex; mode=display">\begin{pmatrix}2 & 3 \\\ 10 & 1\end{pmatrix} \begin{bmatrix}a \\\ b\end{bmatrix} = \begin{bmatrix}8 \\\ 13\end{bmatrix}</script>
</p>
<p>We can say that the <strong>matrix</strong></p>
<p>
<script type="math/tex; mode=display">\begin{pmatrix}2 & 3 \\\ 10 & 1\end{pmatrix}</script>
</p>
<p><em>operates</em> on the <strong>vector</strong></p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix}a \\\ b\end{bmatrix}</script>
</p>
<p>to give the other <strong>vector</strong></p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix}8 \\\ 13\end{bmatrix}</script>
</p>
<p>Our question, our problem to solve, is what vector</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix}a \\\ b\end{bmatrix}</script>
</p>
<p><em>transforms</em> to give us</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix}8 \\\ 13\end{bmatrix}</script>
</p>
<p>Now, what if we use our matrix</p>
<p>
<script type="math/tex; mode=display">\begin{pmatrix}2 & 3 \\ 10 & 1\end{pmatrix}</script>
</p>
<p>to transform our basis vectors \(\hat e_1\) and  \(\hat e_2\)?</p>
<p>
<script type="math/tex; mode=display">\begin{pmatrix}2 & 3 \\\ 10 & 1\end{pmatrix} \cdot \hat e_1 = \begin{pmatrix}2 & 3 \\\ 10 & 1\end{pmatrix} \begin{pmatrix}1 \\\ 0 \end{pmatrix} = \begin{pmatrix}2 \\\ 10 \end{pmatrix}</script>
</p>
<p>
<script type="math/tex; mode=display">\begin{pmatrix}2 & 3 \\\ 10 & 1\end{pmatrix} \cdot \hat e_2 = \begin{pmatrix}2 & 3 \\\ 10 & 1\end{pmatrix} \begin{pmatrix}0 \\\ 1 \end{pmatrix} = \begin{pmatrix}3 \\\ 1 \end{pmatrix}</script>
</p>
<p>It becomes clear that what this matrix is doing is actually <em>transforming</em> the basis vectors \(\hat e_1\) and  \(\hat e_2\) to give us the vectors</p>
<p>
<script type="math/tex; mode=display">\begin{pmatrix}2 \\\ 10 \end{pmatrix} \text{ , } \begin{pmatrix}3 \\\ 1 \end{pmatrix}</script>
</p>
<p>Generally speaking, we can think of the matrix</p>
<p>
<script type="math/tex; mode=display">\begin{pmatrix}2 & 3 \\\ 10 & 1\end{pmatrix}</script>
</p>
<p>as a <em>function</em> that operates on <em>input</em> vectors in order to give us <em>output</em> vectors. A set of simultaneous equations, like the ones we have here, is asking, in effect, what <em>input</em> vector I need in order to get a transformed product (the <em>output vector</em>) at position</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix}8 \\\ 13\end{bmatrix}</script>
</p>
<h5 id="conclusions_3">Conclusions</h5>
<p>Hopefully, it is a little more clear now what we mean now by the term <em>linear</em> algebra. Linear algebra is <em>linear</em>, because it just takes input values, our \(a\) and \(b\) for example, and multiplies them by <em>constants</em>. <em>Everything is linear</em>. Finally, it's algebra simply because it is a notation describing mathematical objects and a system of manipulating those notations.</p>
<p>So linear algebra is a <em>mathematical</em> system for <em>manipulating vectors</em> in the spaces <em>described by vectors</em>.</p>
<blockquote>
<p>This is important! We are noticing some kind of deep connection between simultaneous equations, these things called matrices, and the vectors we were talking about last week. It turns that the key to solving simultaneous equation problems is appreciating how vectors are transformed by matrices, which is the heart of linear algebra.</p>
</blockquote>
<h3 id="matrices-as-objects-that-operate-on-vectors">Matrices as objects that operate on vectors</h3>
<h4 id="how-matrices-transform-space">How matrices transform space</h4>
<blockquote>
<p>Watch <a href="https://youtu.be/kYB8IZa5AuE">this</a> video before reading through this section.</p>
</blockquote>
<p>So far, we have introduced the idea of a matrix and related it to the problem of solving <strong>simultaneous equations</strong>. We showed that the <strong>columns</strong> of a matrix can be thought of as the transformations applied to <strong>unit basis vector</strong> along each axis. This is a pretty profound idea, so lets flesh it out.</p>
<p>We know that we can make any (2D) vector out of a vector <em>sum</em> of the <em>scaled</em> versions of \(\hat e_1\) and \(\hat e_2\) (our basis vectors).</p>
<p>This means that the result of any <em>linear</em> transformation is just going to be some sum of the transformed basis vectors, (\(\hat e_1\) and \(\hat e_2\) here). This is a bit hard to see but what it means is that the grid lines of our space stay <em>parallel</em> and <em>evenly spaced</em>. They might be stretched or sheared, but the origin stays where it is and there isn't any curviness to the space, it doesn't get warped --- a consequence of our scalar addition and multiplication rules for vectors.</p>
<p>If we write down the matrix \(A\) and the vector it is transforming as \(r\), we can represent our apples and bananas problem introduced earlier as:</p>
<p>
<script type="math/tex; mode=display">\begin{pmatrix}2 & 3 \\\ 10 & 1\end{pmatrix} \begin{bmatrix}a \\\ b\end{bmatrix} = \begin{bmatrix}8 \\\ 13\end{bmatrix}</script>
<script type="math/tex; mode=display">A   r =    r' </script>
</p>
<p>Where \(r'\) is our <em>transformed</em> vector. We can generalize further, and add a scalar, \(n\):</p>
<p>
<script type="math/tex; mode=display">A (n  r) =  n  r' </script>
</p>
<p>We notice that:</p>
<p>
<script type="math/tex; mode=display">A (r +   s) =  A   r + A   s </script>
</p>
<p>Putting it together, we can represent any vector in 2D space as:</p>
<p>
<script type="math/tex; mode=display">A (n \hat e_1 + m \hat e_2) =  n A \hat e_1 + m A \hat e_2 </script>
<script type="math/tex; mode=display">\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;= n \hat e_1' + m \hat e_2' </script>
</p>
<p>Now let's try an example. Let,</p>
<p>
<script type="math/tex; mode=display">A = \begin{pmatrix}2 & 3 \\\ 10 & 1\end{pmatrix}</script>
<script type="math/tex; mode=display">  r = \begin{bmatrix}3 \\\ 2\end{bmatrix}</script>
</p>
<p>Then,</p>
<p>
<script type="math/tex; mode=display">A   r =    r' </script>
<script type="math/tex; mode=display">\Rightarrow \begin{pmatrix}2 & 3 \\\ 10 & 1\end{pmatrix} \begin{bmatrix}3 \\\ 2\end{bmatrix} = \begin{bmatrix}12 \\\ 32\end{bmatrix}</script>
</p>
<p>Which is no different than how we might have multiplied matrices and vectors in school. But, we can think of this another way:</p>
<p>
<script type="math/tex; mode=display">A (n \hat e_1 + m \hat e_2) =   r'</script>
<script type="math/tex; mode=display">\Rightarrow \begin{pmatrix}2 & 3 \\\ 10 & 1\end{pmatrix} \Biggl (3 \begin{bmatrix}1 \\\ 0\end{bmatrix} + 2 \begin{bmatrix}0 \\\ 1\end{bmatrix} \Biggl) = 3 \begin{pmatrix}2 & 3 \\\ 10 & 1\end{pmatrix} \begin{bmatrix}1 \\\ 0\end{bmatrix} + 2 \begin{pmatrix}2 & 3 \\\ 10 & 1\end{pmatrix} \begin{bmatrix}0 \\\ 1\end{bmatrix} =  </script>
<script type="math/tex; mode=display">= 3 \begin{bmatrix}2 \\\ 10\end{bmatrix} + 2 \begin{bmatrix}3 \\\ 1\end{bmatrix} = \begin{bmatrix}12 \\\ 32\end{bmatrix}</script>
</p>
<p>The take home idea here is that the matrix \(A\) just <em>tells us where the basis vectors go</em>. That's the <em>transformation</em> it does.</p>
<h4 id="types-of-matrix-transformation">Types of matrix transformation</h4>
<p>Lets illustrate the type of transformations we can perform with matrices with a number of examples.</p>
<blockquote>
<p>Remember, in linear algebra, linear transformations can be represented by matrices!</p>
</blockquote>
<p>We are only going to scratch the surface here and to continue to build up our intuition of viewing matrices as <em>functions</em> that apply <em>transformations</em> to some <em>input vector</em>.</p>
<p><strong>Identity matrix</strong></p>
<p>First, let's think about a matrix that doesn't change anything. Such a matrix is just composed of the <strong>basis vectors</strong> of the space,</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix} 1 & 0  \\\ 0 & 1\end{bmatrix} \begin{bmatrix} x  \\\ y\end{bmatrix} = \begin{bmatrix} x  \\\ y\end{bmatrix}</script>
</p>
<p>this is known as the <a href="http://www.wikiwand.com/en/Identity_matrix"><strong>identity matrix</strong></a>. It's the matrix that does nothing and leaves everything preserved, typically denoted \(I_m\) where \(m\) is the number of dimensions in our vector space.</p>
<p><strong>Scaling</strong></p>
<p>When our matrix contains values other than \(0\) in the diagnonal, we get a <strong>scaling</strong>:</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix} m & 0  \\\ 0 & n\end{bmatrix} \begin{bmatrix} x  \\\ y\end{bmatrix} = \begin{bmatrix} mx  \\\ ny\end{bmatrix}</script>
</p>
<p>If \(m = 3\) and \(n=2\), visually this looks like:</p>
<p><img alt="matrix_transformation_1" src="../img/matrix_transformation_1.png" /></p>
<p>This transformation simply <em>scales</em> each dimension of the space by the value at the corresponding diagonal of the matrix.</p>
<blockquote>
<p>Note that when \(m \lt 1\) and/or \(n \lt 1\), our space is actually <em>compressed</em> along the axes.</p>
</blockquote>
<p><strong>Reflections</strong></p>
<p>When one or more of our diagonal values is negative, we get a <strong>reflection</strong>:</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix} -1 & 0  \\\ 0 & k\end{bmatrix} \begin{bmatrix} x  \\\ y\end{bmatrix} = \begin{bmatrix} -x  \\\ ky\end{bmatrix}</script>
</p>
<p>In this case, our coordinate system is flipped across the <em>vertical</em> axis. When \(k = 2\), visually this looks like:</p>
<p><img alt="matrix_transformation_2" src="../img/matrix_transformation_2.png" /></p>
<p>In another example,</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix} -1 & 0  \\\ 0 & -1\end{bmatrix} \begin{bmatrix} x  \\\ y\end{bmatrix} = \begin{bmatrix} -x  \\\ -y\end{bmatrix}</script>
</p>
<p>This transformation inverts <em>all</em> axes, and is known as an <strong>inversion</strong>.</p>
<p>We can also produce mirror reflections over a straight line that crosses through the origin:</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix} 0 & 1  \\\ 1 & 0\end{bmatrix} \begin{bmatrix} x  \\\ y\end{bmatrix} = \begin{bmatrix} y  \\\ x\end{bmatrix}</script>
</p>
<p>or,</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix} 0 & -1  \\\ -1 & 0\end{bmatrix} \begin{bmatrix} x  \\\ y\end{bmatrix} = \begin{bmatrix} -y  \\\ -x\end{bmatrix}</script>
</p>
<p><img alt="matrix_transformation_4" src="../img/matrix_transformation_4.png" /></p>
<blockquote>
<p>Of course, we can also produce mirror transformations over the \(x\) or \(y\) axis as well, by making one of the diagonals \(1\) and the other \(-1\).</p>
</blockquote>
<p><strong>Shears</strong></p>
<p><a href="http://www.wikiwand.com/en/Shear_mapping"><strong>Shears</strong></a> are visually similar to <em>slanting</em>. There are two possibilities:</p>
<p>A shear parallel to the \(x\) axis looks like:</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix} 1 & k \\\ 0 & 1\end{bmatrix} \begin{bmatrix} x  \\\ y\end{bmatrix} = \begin{bmatrix} x + ky  \\\ y\end{bmatrix}</script>
</p>
<p>If \(k=1\), then visually this looks like:</p>
<p><img alt="matrix_transformation_3" src="../img/matrix_transformation_3.png" /></p>
<p>Or a shear parallel to the \(y\) axis:</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix} 1 & 0 \\\ k & 1\end{bmatrix} \begin{bmatrix} x  \\\ y\end{bmatrix} = \begin{bmatrix} x  \\\ kx + y\end{bmatrix}</script>
</p>
<p><strong>Rotations</strong></p>
<p>Finally, we can rotate the space about the orign. For example,</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix} 0 & -1 \\\ 1 & 0\end{bmatrix} \begin{bmatrix} x  \\\ y\end{bmatrix} = \begin{bmatrix} y  \\\ -x\end{bmatrix}</script>
</p>
<p>would rotate the entire space \(90^0\) counterclockwise.</p>
<p>More generally, for a rotation by a angle \(\theta\) <strong>clockwise</strong> about the orgin:</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix} \cos \theta & \sin \theta \\\ - \sin \theta & \cos \theta \end{bmatrix} \begin{bmatrix} x \\\ y\end{bmatrix} = \begin{bmatrix} x \cos \theta + y \sin \theta  \\\  - x \sin \theta + y \cos \theta\end{bmatrix}</script>
</p>
<p>and for a rotation by a angle \(\theta\) <strong>counterclockwise</strong> about the orgin</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix} \cos \theta & - \sin \theta \\\ \sin \theta & \cos \theta \end{bmatrix} \begin{bmatrix} x \\\ y\end{bmatrix} = \begin{bmatrix} x \cos \theta - y \sin \theta  \\\  x \sin \theta + y \cos \theta\end{bmatrix}</script>
</p>
<h5 id="conclusions_4">Conclusions</h5>
<p>In this section, we described the major transformations that a matrix can perform on a vector. Next, we will look at how we can combine these transformations (known as <strong>composition</strong>) to produce more complex matrix transformations</p>
<h4 id="composition-or-combination-of-matrix-transformations">Composition or combination of matrix transformations</h4>
<blockquote>
<p>Watch <a href="https://youtu.be/XkY2DOUCWMU">this</a> video before reading through this section.</p>
</blockquote>
<p>So what is the point of introducing these different geometric transformations in this class? Well, if you want to do any kind of <em>shape</em> alteration, say of all the pixels in an image, or a face, then you can always make that shape change out of some combination of <em>rotations</em>, <em>shears</em>, <em>stretches</em>, and <em>inverses</em>.</p>
<blockquote>
<p>One example where these geometric transformations may be useful is in <a href="http://www.wikiwand.com/en/Facial_recognition_system"><strong>facial recognition</strong></a>, where we may preprocess every image by transforming it so that the person(s) face(s) are directly facing the camera.</p>
</blockquote>
<p>Lets illustrate this <em>composition</em> of matrix transformations with an example. Here, we will first apply a \(90^o\) rotation clockwise about the \(x\)-axis, and then a shear parallel to the \(x\)-axis. Let the first transformation matrix be \(A_1\):</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix} 0 & 1  \\\ -1 & 0\end{bmatrix} \begin{bmatrix} x  \\\ y\end{bmatrix} = \begin{bmatrix} y  \\\ -x\end{bmatrix}</script>
</p>
<p>And our second transformation matrix \(A_2\):</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix} 1 & 1  \\\ 0 & 1\end{bmatrix} \begin{bmatrix} x  \\\ y\end{bmatrix} = \begin{bmatrix} x + y  \\\ y\end{bmatrix}</script>
</p>
<p>Applying \(A_1\) to our basis vectors \(\hat e_1\) and \(\hat e_2\) gives us \(\hat e_1'\) and \(\hat e_1'\):</p>
<p>
<script type="math/tex; mode=display">\hat e_1' = A_1 \cdot \hat e_1 = \begin{bmatrix} 0  \\\ -1\end{bmatrix}</script>
<script type="math/tex; mode=display">\hat e_2' = A_1 \cdot \hat e_2 = \begin{bmatrix} 1  \\\ 0\end{bmatrix}</script>
</p>
<p>Applying \(A_2\) to our new transformed vectors \(\hat e_1'\) and \(\hat e_2'\) gives us \(\hat e_1''\) and \(\hat e_1''\):</p>
<p>
<script type="math/tex; mode=display">\hat e_1'' = A_2 \cdot \hat e_1' = \begin{bmatrix}-1 \\\ -1\end{bmatrix}</script>
<script type="math/tex; mode=display">\hat e_2'' = A_2 \cdot \hat e_2' = \begin{bmatrix} 1  \\\ 0\end{bmatrix}</script>
</p>
<p>Notice that if we stack our final transformed vectors \(\hat e_1''\) and \(\hat e_1''\) as columns we get the matrix:</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix} -1 & 1  \\\ -1 & 0\end{bmatrix}</script>
</p>
<p>Which is equal to \(A_2 \cdot A_1\). Geometrically, this looks like:</p>
<p><img alt="composition" src="../img/composition.jpg" /></p>
<p><strong>Conclusions</strong></p>
<p>The take home message here is that the transformation \(A_2 \cdot (A_1 \cdot r)\) for some transformation matrices \(A_1\) and \(A_2\) and some vector \(r\), is equivalent to the transformation \((A_2A_1) \cdot r\)</p>
<blockquote>
<p>Note that \(A_2 \cdot (A_1 \cdot r) \ne A_1 \cdot (A_2 \cdot r)\), that is, the order in which we apply our transformations <em>matters</em>.</p>
</blockquote>
<p>As it turns out, the key to solving simultaneous equation problems is appreciating how vectors are <em>transformed</em> by matrices, which is at the the heart of linear algebra.</p>
<h3 id="matrix-inverses">Matrix inverses</h3>
<p>In this section, we are finally going to present a way to solve the <em>apples and bananas</em> problem with matrices. Along the way, we're going to find out about a thing called the <a href="http://www.wikiwand.com/en/Invertible_matrix"><strong>inverse</strong></a> of a matrix and a method for finding it.</p>
<blockquote>
<p>Watch <a href="https://youtu.be/uQhTuRlWMxw">this</a> video before reading this section. For more practice with matrix inverses, see Khan Academy sections <a href="https://www.khanacademy.org/math/linear-algebra/matrix-transformations/inverse-transformations/v/linear-algebra-introduction-to-the-inverse-of-a-function">here</a> and <a href="https://www.khanacademy.org/math/linear-algebra/matrix-transformations/inverse-of-matrices/v/linear-algebra-deriving-a-method-for-determining-inverses">here</a>.</p>
</blockquote>
<h4 id="gaussian-elimination-solving-the-apples-and-bananas-problem">Gaussian elimination: Solving the apples and bananas problem</h4>
<p>First, recall our apples and bananas problem in matrix form:</p>
<p>
<script type="math/tex; mode=display">\begin{pmatrix}2 & 3 \\\ 10 & 1\end{pmatrix} \begin{bmatrix}a \\\ b\end{bmatrix} = \begin{bmatrix} 8 \\\ 3\end{bmatrix}</script>
</p>
<p>Where we want to find the price of individual apples (\(a\)) and bananas (\(b\)). Simplifying, lets say that:</p>
<p>
<script type="math/tex; mode=display">A\cdot r =   s</script>
</p>
<blockquote>
<p>This, in effect is saying: "\(A\) <em>operates</em> on vector \(r\) to give us \(s\)".</p>
</blockquote>
<p>To solve for \(r\), we need to move \(A\) to the other side of the equation. <em>But how?</em>. Well, to "undo" a division (and isolate \(x\)), you multiply by the reciprocal:</p>
<p>
<script type="math/tex; mode=display">\frac{1}{2}x = 4</script>
<script type="math/tex; mode=display">\Rightarrow x = 8</script>
</p>
<p>Likewise, to undo a multiplication, we divide by the reciprocal:</p>
<p>
<script type="math/tex; mode=display">2x = 4</script>
<script type="math/tex; mode=display">\Rightarrow x = 2</script>
</p>
<p>How do we <em>undo</em> the transformation performed by \(A\)? The answer is to find the matrix \(A^{-1}\) (known as the <strong>inverse</strong> of \(A\)) such that:</p>
<p>\(A^{-1}A = I\)</p>
<p>where \(I\) is the <a href="http://www.wikiwand.com/en/Identity_matrix">identity matrix</a>. We call \(A^{-1}\) the <strong>inverse</strong> of \(A\) because is it <em>reverses whatever transformation</em> \(A\) does, giving us back \(I\). We note that:</p>
<p>
<script type="math/tex; mode=display">A^{-1}A\cdot r = A^{-1}   s</script>
<script type="math/tex; mode=display">\Rightarrow\ I \cdot r = A^{-1}   s</script>
<script type="math/tex; mode=display">\Rightarrow\   r = A^{-1}   s</script>
</p>
<p>So, if we could find the inverse of \(A\) (i.e. find \(A^{-1}\)), we can solve our problem (i.e. find \(a\) and \(b\)). We can solve for \(A^{-1}\) with a series of <em>row operations</em> or <em>substitutions</em> (known as <a href="http://www.wikiwand.com/en/Gaussian_elimination"><strong>Gaussian elimination</strong></a>).</p>
<p>Let's look at a slightly more complicated problem to see how this is done:</p>
<p>
<script type="math/tex; mode=display">\begin{pmatrix}1 & 1 & 3 \\\ 1 & 2 & 4 \\\ 1 & 1 & 2\end{pmatrix} \begin{bmatrix}a \\\ b \\\ c\end{bmatrix} = \begin{bmatrix} 15 \\\ 21 \\\ 13\end{bmatrix}</script>
</p>
<p>We start by subtracting row 1 from rows 2 and 3, which gives us a matrix in <a href="http://www.wikiwand.com/en/Row_echelon_form"><strong>row echelon form</strong></a> (technically, <a href="http://www.wikiwand.com/en/Row_echelon_form#/Reduced_row_echelon_form">reduced row echelon form</a>):</p>
<p>
<script type="math/tex; mode=display">\begin{pmatrix}1 & 1 & 3 \\\ 0 & 1 & 1 \\\ 0 & 0 & 1\end{pmatrix} \begin{bmatrix}a \\\ b \\\ c\end{bmatrix} = \begin{bmatrix} 15 \\\ 6 \\\ 2\end{bmatrix}</script>
</p>
<p>We then perform two steps of <a href="http://www.wikiwand.com/en/Triangular_matrix#/Forward_and_back_substitution"><strong>back substitution</strong></a> to get the identify matrix:</p>
<p>
<script type="math/tex; mode=display">\begin{pmatrix}1 & 1 & 0 \\\ 0 & 1 & 0 \\\ 0 & 0 & 1\end{pmatrix} \begin{bmatrix}a \\\ b \\\ c\end{bmatrix} = \begin{bmatrix} 9 \\\ 4 \\\ 2\end{bmatrix}</script>
</p>
<p>
<script type="math/tex; mode=display">\begin{pmatrix}1 & 0 & 0 \\\ 0 & 1 & 0 \\\ 0 & 0 & 1\end{pmatrix} \begin{bmatrix}a \\\ b \\\ c\end{bmatrix} = \begin{bmatrix} 5 \\\ 4 \\\ 2\end{bmatrix}</script>
</p>
<p>We can then read the solution right from the matrices \(a = 5\), \(b = 4\), \(c = 2\).</p>
<blockquote>
<p>If you are still feeling uneasy about using Gaussian elimination to solve a system of linear equations, see <a href="https://youtu.be/2j5Ic2V7wq4">here</a> for an example walked through step-by-step example. Full credit to <a href="http://patrickjmt.com">PatrickJMT</a>.</p>
</blockquote>
<p><strong>Conclusions</strong></p>
<p>As it turns out, we don't have to compute the inverse at all to solve a system of linear equations. Although we showed the process of Gaussian elimination for some vectors \(r\) and \(s\), we can use it in the general case to solve for any linear equation of the form \(A\cdot r =   s\). This actually one of the most computationally efficient ways to solve this problem, and it's going to work every time.</p>
<h4 id="from-gaussian-elimination-to-finding-the-inverse-matrix">From Gaussian elimination to finding the inverse matrix</h4>
<p>Now, let's think about how we can apply this idea of <em>elimination</em> to find the inverse matrix, which solves the more general problem no matter what vectors I write down on the right hand side.</p>
<p>Say I have a 3 \(\times\) 3 matrix \(A\) and its inverse \(B\) (\(B = A^{-1})\). By the definition of the <a href="http://www.wikiwand.com/en/Invertible_matrix"><strong>inverse</strong></a>,</p>
<p>
<script type="math/tex; mode=display">AB = BA = I_n</script>
</p>
<p>If we use our matrix \(A\) from the last section:</p>
<p>
<script type="math/tex; mode=display">\begin{pmatrix}1 & 1 & 3 \\\ 1 & 2 & 4 \\\ 1 & 1 & 2\end{pmatrix} \begin{pmatrix}b_{11} & b_{12} & b_{13} \\\ b_{21} & b_{22} & b_{23} \\\ b_{31} & b_{32} & b_{33}\end{pmatrix} = I</script>
</p>
<blockquote>
<p>Where \(b_{ij}\) is the element at the \(i^{th}\) row and \(j^{th}\) column of matrix \(B\).</p>
</blockquote>
<p>we notice that the first column of \(B\) is just a vector. It's a vector that describes what the \(B\) matrix, the inverse of \(A\), does to space.</p>
<blockquote>
<p>Actually, it's the transformation that that vector does to the x-axis.</p>
</blockquote>
<p>This means that:</p>
<p>
<script type="math/tex; mode=display">\begin{pmatrix}1 & 1 & 3 \\\ 1 & 2 & 4 \\\ 1 & 1 & 2\end{pmatrix} \begin{pmatrix}b_{11} \\\ b_{21} \\\ b_{31}\end{pmatrix} = \begin{pmatrix} 1 \\\ 0 \\\ 0\end{pmatrix}</script>
</p>
<p>Now, we could solve this by the elimination method and back substitution in just the way we did before. Then, we could do it again for the second column of \(B\), and finally the third. In this way, we would have solved for \(B\), in other words, we would have found \(A^{-1}\).</p>
<p>It turns out, we can actually solve for \(B\) all at once. So lets do that:</p>
<p>
<script type="math/tex; mode=display">\begin{pmatrix}1 & 1 & 3 \\\ 1 & 2 & 4 \\\ 1 & 1 & 2\end{pmatrix} \begin{pmatrix}b_{11} & b_{12} & b_{13} \\\ b_{21} & b_{22} & b_{23} \\\ b_{31} & b_{32} & b_{33}\end{pmatrix} = I</script>
</p>
<p>Subtract row 1 from row 2 and 3:</p>
<p>
<script type="math/tex; mode=display">\Rightarrow \begin{pmatrix}1 & 1 & 3 \\\ 0 & 1 & 1 \\\ 0 & 0 & -1\end{pmatrix} \begin{pmatrix}b_{11} & b_{12} & b_{13} \\\ b_{21} & b_{22} & b_{23} \\\ b_{31} & b_{32} & b_{33}\end{pmatrix} = \begin{pmatrix}1 & 0 & 0 \\\ -1 & 1 & 0 \\\ -1 & 0 & 1\end{pmatrix}</script>
</p>
<p>Multiply row 3 by \(-1\):</p>
<p>
<script type="math/tex; mode=display">\Rightarrow \begin{pmatrix}1 & 1 & 3 \\\ 0 & 1 & 1 \\\ 0 & 0 & 1\end{pmatrix} \begin{pmatrix}b_{11} & b_{12} & b_{13} \\\ b_{21} & b_{22} & b_{23} \\\ b_{31} & b_{32} & b_{33}\end{pmatrix} = \begin{pmatrix}1 & 0 & 0 \\\ -1 & 1 & 0 \\\ 1 & 0 & -1\end{pmatrix}</script>
</p>
<p>Now that row 3 is in <a href="http://www.wikiwand.com/en/Row_echelon_form">row echelon form</a>, we can substitute it back into row 2 and row 1:</p>
<p>
<script type="math/tex; mode=display">\Rightarrow \begin{pmatrix}1 & 1 & 0 \\\ 0 & 1 & 0 \\\ 0 & 0 & 1\end{pmatrix} \begin{pmatrix}b_{11} & b_{12} & b_{13} \\\ b_{21} & b_{22} & b_{23} \\\ b_{31} & b_{32} & b_{33}\end{pmatrix} = \begin{pmatrix} -2 & 0 & 3 \\\ -2 & 1 & 1 \\\ 1 & 0 & -1\end{pmatrix}</script>
</p>
<p>And finally, back substitute row 2 into row 1:</p>
<p>
<script type="math/tex; mode=display">\Rightarrow \begin{pmatrix}1 & 0 & 0 \\\ 0 & 1 & 0 \\\ 0 & 0 & 1\end{pmatrix} \begin{pmatrix}b_{11} & b_{12} & b_{13} \\\ b_{21} & b_{22} & b_{23} \\\ b_{31} & b_{32} & b_{33}\end{pmatrix} = \begin{pmatrix} 0 & -1 & 2 \\\ -2 & 1 & 1 \\\ 1 & 0 & -1\end{pmatrix}</script>
</p>
<p>Because any matrix times its identity is that matrix itself:</p>
<p>
<script type="math/tex; mode=display">\begin{pmatrix}1 & 0 & 0 \\\ 0 & 1 & 0 \\\ 0 & 0 & 1\end{pmatrix} \begin{pmatrix}b_{11} & b_{12} & b_{13} \\\ b_{21} & b_{22} & b_{23} \\\ b_{31} & b_{32} & b_{33}\end{pmatrix} = \begin{pmatrix} 0 & -1 & 2 \\\ -2 & 1 & 1 \\\ 1 & 0 & -1\end{pmatrix}</script>
</p>
<p>
<script type="math/tex; mode=display">\Rightarrow \begin{pmatrix}b_{11} & b_{12} & b_{13} \\\ b_{21} & b_{22} & b_{23} \\\ b_{31} & b_{32} & b_{33}\end{pmatrix} = \begin{pmatrix} 0 & -1 & 2 \\\ -2 & 1 & 1 \\\ 1 & 0 & -1\end{pmatrix}</script>
</p>
<blockquote>
<p>You can actually prove to yourself that we got the right answer by checking that \(A \cdot B = I\).</p>
</blockquote>
<p>So that's our answer. We've found the identity matrix, \(B = A^{-1}\) for \(A\), and we did this by transforming \(A\) into it's identity matrix via elimination and back substitution. Moreover, because \(B\) could be any matrix, we have solved this in the general case. The solution is the same regardless of the number of dimensions, and this leads to a computationally efficient way to invert a matrix.</p>
<blockquote>
<p>There are computationally faster methods of computing the inverse, one such method is known as  a <strong>decomposition process</strong>. In practice, you will simply call the solver of your problem or function, something like <code>inv(A)</code>, and it will pick the best method by inspecting the matrix you give it and return the answer.</p>
</blockquote>
<p><strong>Conclusions</strong></p>
<p>We have figured out how to solve sets of linear equations in the general case, by a procedure we can implement in a computer really easily (known as <a href="http://www.wikiwand.com/en/Gaussian_elimination"><strong>Gaussian elimination</strong></a>), and we've generalized this method to the to find the <strong>inverse</strong> of a matrix, regardless of what is on the right hand side of our system of equations.</p>
<h3 id="special-matrices">Special matrices</h3>
<p>In the final section of this week, we're going to look at a property of a matrix called the <a href="http://www.wikiwand.com/en/Determinant"><strong>determinant</strong></a>.</p>
<blockquote>
<p>The <strong>determinant</strong> is a value that can be computed from the elements of a <a href="http://www.wikiwand.com/en/Square_matrix#Square_matrices"><strong>square matrix</strong></a>. The determinant of a matrix \(A\) is denoted \(det(A)\), \(det A\), or \(\vert A \vert\). Geometrically, it can be viewed as the scaling factor of the <a href="http://www.wikiwand.com/en/Linear_map"><strong>linear transformation</strong></a> described by the matrix.</p>
</blockquote>
<p>We'll also look at what happens when a matrix <em>doesn't</em> have linearly independent basis vectors.  </p>
<blockquote>
<p>Watch <a href="https://youtu.be/Ip3X9LOh2dk">this</a> video before reading this section. For more practice with matrix inverses, see Khan Academy sections <a href="https://www.khanacademy.org/math/linear-algebra/matrix-transformations/inverse-of-matrices/v/linear-algebra-deriving-a-method-for-determining-inverses">here</a> and <a href="https://www.khanacademy.org/math/linear-algebra/matrix-transformations/determinant-depth/v/linear-algebra-determinant-when-row-multiplied-by-scalar">here</a>.</p>
</blockquote>
<h4 id="the-determinant">The determinant</h4>
<p>Let's start by looking at the simple matrix:</p>
<p>
<script type="math/tex; mode=display">\begin{pmatrix} a & 0 \\\ 0 & d\end{pmatrix}</script>
</p>
<p>If we multiply this matrix by our basis vectors \(\hat e_1\) and  \(\hat e_2\), we get</p>
<p>
<script type="math/tex; mode=display">\hat e_1' = \begin{bmatrix} a \\\ 0 \end{bmatrix}</script>
<script type="math/tex; mode=display">\hat e_2' = \begin{bmatrix} 0 \\\ d \end{bmatrix}</script>
</p>
<p>So, in plain english, we have stretched our \(x\)-axis by a factor of \(a\) and our \(y\)-axis by a factor of \(d\) and, therefore, scaled the area of the grid cells of the vector space by a factor of \(ad\).</p>
<blockquote>
<p>This is a property of the fact that any linear transformation keeps grid lines parallel and evenly spaced.</p>
</blockquote>
<p>We call this number, \(ad\) the <strong>determinant</strong>.</p>
<p><img alt="determinant_1" src="../img/determinant_1.png" /></p>
<p>Now, if I instead have a matrix:</p>
<p>
<script type="math/tex; mode=display">\begin{pmatrix} a & b \\\ 0 & d\end{pmatrix}</script>
</p>
<p>then this is still going to stretch \(\hat e_1\) out by a factor of \(a\), <em>but</em> on the other axis, I am going to move \(\hat e_2\) hat to:</p>
<p>
<script type="math/tex; mode=display">\hat e_2 = \begin{pmatrix} b \\\ d\end{pmatrix}</script>
</p>
<p>What we have done is taken the original grid and stretched it along the \(x\)-axis by \(a\) and, along the \(y\)-axis by \(d\) and sheared it (parallel to the \(x\)-axis) by \(b\).</p>
<p><img alt="determinant_2" src="../img/determinant_2.png" /></p>
<blockquote>
<p>We've still changed the size, the scale of the space (which is what the determinant really is) by a factor of \(ad\).</p>
</blockquote>
<p>Notice that the area defined by our transformed vectors \(\hat e_1'\) and \(\hat e_2'\) is <em>still</em> just the base times the perpendicular height, \(ad\) (the determinant).</p>
<p>Lets flesh this out in the <em>general</em> case. Say we have the matrix:</p>
<p>
<script type="math/tex; mode=display">A = \begin{pmatrix} a & b \\\ c & d\end{pmatrix}</script>
</p>
<p>Multiplying this matrix by our basis vectors yields a parallelogram (stretched by \(a\) and \(d\) and sheared by \(b\) and \(c\)). We can actual compute the area of this parallelogram as follows:</p>
<p><img alt="determinant_3" src="../img/determinant_3.png" /></p>
<blockquote>
<p>We find the area of this parallelogram by finding the area of the whole box the encloses it, and subtracting off combined area of the the little bits around it.</p>
</blockquote>
<p>The exact method for solving the area is not important (although it is pretty trivial). What is important, is that the <strong>determinant</strong> of \(A\) can be computed as \(\vert A \vert =  ad - bc\), and that this computation has a <em>geometric interpretation</em>.</p>
<blockquote>
<p>This is the formula for the determinant of a square \(2 \times 2\) matrix. See <a href="http://www.wikiwand.com/en/Determinant#/3_×_3_matrices">here</a> for the formula for higher dimensional matrices.</p>
</blockquote>
<p>Now, in school when you looked at matrices, you probably saw that you could find the inverse in the following way. For a matrix:</p>
<p>
<script type="math/tex; mode=display">A = \begin{pmatrix} a & b \\\ c & d\end{pmatrix}</script>
</p>
<ol>
<li>Exchange \(a\) and the \(d\), and switch the sign on the \(b\) and the \(c\)</li>
<li>Multiply \(A\) by this matrix</li>
<li>Scale the transformation by \(\frac{1}{ad - bc}\)</li>
</ol>
<p>In the general case, this looks like:</p>
<p>
<script type="math/tex; mode=display">\frac{1}{ad - bc} \begin{pmatrix} a & b \\\ c & d\end{pmatrix} \begin{pmatrix} d & -b \\\ -c & a\end{pmatrix} = \frac{1}{ad - bc} \begin{pmatrix} ad - bc & 0 \\\ 0 & ad - bc\end{pmatrix} = I</script>
</p>
<p>This demonstrates that</p>
<p>
<script type="math/tex; mode=display">\frac{1}{ad - bc} \begin{pmatrix} d & -b \\\ -c & a\end{pmatrix}</script>
</p>
<p>is in fact the inverse of the matrix \(A\).</p>
<p>This helps capture what the determinant really is. It's the <em>amount</em> by which the original matrix <em>scaled</em> vector space. In the above example, dividing by the determinant normalizes the space back to its <em>original</em> size.</p>
<blockquote>
<p>We could spend a lot of time talking about how to solve for the determinant. However, knowing how to do the operations isn't really a useful skill. Many programming libraries (e.g. <code>python</code>) have linear algebra libraries (e.g. <code>Numpy</code>) which makes computing the determinant as easy, for example, as calling <code>det(A)</code>. If you really want to know how to compute determinants by hand, then look up a <a href="http://www.wikiwand.com/en/QR_decomposition"><strong>QR decomposition</strong></a> online.</p>
</blockquote>
<h5 id="a-determinant-of-zero">A determinant of zero</h5>
<p>Now, let's think about the matrix</p>
<p>
<script type="math/tex; mode=display">A = \begin{pmatrix} 1 & 2 \\\ 1 & 2\end{pmatrix}</script>
</p>
<p>If we multiply this matrix by our basis vectors \(\hat e_1\) and  \(\hat e_2\), we get</p>
<p>
<script type="math/tex; mode=display">\hat e_1' = \begin{bmatrix} 1 \\\ 1 \end{bmatrix}</script>
<script type="math/tex; mode=display">\hat e_2' = \begin{bmatrix} 2 \\\ 2 \end{bmatrix}</script>
</p>
<p>So this matrix, when applied to our vector space, actually <em>collapses it onto a line</em>. All our \(y\)'s are mapped to the vector</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix}2 \\\ 2 \end{bmatrix}</script>
</p>
<p>and our \(x\)'s to</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix}1 \\\ 1 \end{bmatrix}</script>
</p>
<p>Correspondingly, we notice that \(\vert A \vert = 0\). Therefore, the area enclosed by the new basis vectors is <em>zero</em>.</p>
<p><img alt="" src="../img/determinant_4.png" /></p>
<h5 id="a-negative-determinant">A negative determinant</h5>
<p>A negative determinant simply means that the transformation has <em>flipped</em> the orientation of our vector space. This is much easier to <em>see</em> than to <em>explain</em>; check out <a href="https://youtu.be/Ip3X9LOh2dk">this</a> video which presents some awesome visualizations of the determinant.</p>
<h5 id="what-the-determinant-means-numerically">What the determinant means numerically</h5>
<p>To drive home the numerical interpretation of the determinant, lets start with this set of simultaneous equations:</p>
<p>
<script type="math/tex; mode=display">\begin{pmatrix} 1 & 1 & 3 \\\ 1 & 2 & 4 \\\ 2 & 3 & 7\end{pmatrix} \begin{pmatrix} a \\\ b \\\ c\end{pmatrix} = \begin{pmatrix} 12 \\\ 17 \\\ 29\end{pmatrix}</script>
</p>
<p>You'll notice that both the <em>rows</em> and the <em>columns</em> are <em>linearly dependent</em>. Thinking about the columns of this matrix as the basis vectors of some 3-dimensional space, we note that this transformation collapses our vector space from being 3D to 2D (by collapsing every point in space onto a plane). Let's see what this means numerically by trying to reduce our matrix to row-echelon form:</p>
<p>
<script type="math/tex; mode=display">\begin{pmatrix} 1 & 1 & 3 \\\ 1 & 2 & 4 \\\ 2 & 3 & 7\end{pmatrix} \begin{pmatrix} a \\\ b \\\ c\end{pmatrix} = \begin{pmatrix} 12 \\\ 17 \\\ 29\end{pmatrix}</script>
</p>
<p>Subtract row 1 from row 2,</p>
<p>
<script type="math/tex; mode=display">\Rightarrow \begin{pmatrix} 1 & 1 & 3 \\\ 0 & 1 & 1 \\\ 2 & 3 & 7\end{pmatrix} \begin{pmatrix} a \\\ b \\\ c\end{pmatrix} = \begin{pmatrix} 12 \\\ 5 \\\ 29\end{pmatrix}</script>
</p>
<p>Subtract row 1 plus row 2 from row 3,</p>
<p>
<script type="math/tex; mode=display">\Rightarrow \begin{pmatrix} 1 & 1 & 3 \\\ 0 & 1 & 1 \\\ 0 & 0 & 0\end{pmatrix} \begin{pmatrix} a \\\ b \\\ c\end{pmatrix} = \begin{pmatrix} 12 \\\ 5 \\\ 0\end{pmatrix}</script>
</p>
<p>while the matrix is now is row-echelon form, <em>we don't have a final entry of the matrix</em> (\(0\cdot c = 0\)). Because we don't have a solution for \(c\) we can't back substitute, and we can't solve our system of equations.</p>
<blockquote>
<p>The reason we can't solve this system is because we don't have enough information. In keeping with our apples and bananas problem, imagine that when we went in to buy apples and bananas and carrots the third time, we ordered just a the <em>sum</em> of first two orders. Therefore, we didn't get any <em>new</em> information and thus don't have enough data to find out the solution for how much apples and bananas and carrots cost. This third order wasn't <em>linearly independent</em> from our first two, in the language of matrices and vectors.</p>
</blockquote>
<p>So, when the basis vectors describing a matrix aren't linearly independent, then the <strong>determinant</strong> is <em>zero</em>, and we can't solve the system. The loss of information when we map from \(n\)-dimensional to \((n-x)\)-dimensional space (where \(x \ge 1\)) means we cannot possibly know what the inverse matrix is (as it is impossible to map a lower dimensional space back to the original, higher dimensional space). When a matrix has no inverse, we say that it is <strong>singular</strong>.</p>
<blockquote>
<p>There are situations where we might want to do a transformation that collapses the number of dimensions in a space, but it means that we cannot possibly reverse the mapping, meaning the matrix has no inverse. This also means we cannot solve a system of linear equations defined by a singular matrix using Gaussian elimination and back substitution.</p>
</blockquote>
<h4 id="summary">Summary</h4>
<p>In the last section of this week, we took a look at the determinant, which is how much a given transformation <em>scales</em> our space. In 2-dimensions, this can be thought as the scalar multiple appleid to any <em>area</em> of our space, and in 3-dimensions any <em>volume</em> of our space. We also looked at the special case where the determinant is zero and found that this means that the basis vectors aren't linearly independent, which in turn means that the inverse doesn't exist.</p>
<p>To summarize <a href="#week-3-matrices-as-objects-that-operate-on-vectors">Week 3</a>, we</p>
<ul>
<li>introduced matrices as objects that transforms space.</li>
<li>looked at different archetypes of matrices, like <strong>rotations</strong>, <strong>inverses</strong>, <strong>stretches</strong>, and <strong>shears</strong>,</li>
<li>how to combine matrices by doing successive transformations, known as m<strong>atrix multiplication</strong> or <strong>composition</strong></li>
<li>how to solve systems of linear equations by elimination and how to find <strong>inverses</strong></li>
<li>and finally, we introduced <strong>determinants</strong> and showed how that relates to the concept of <strong>linear independence</strong>.</li>
</ul>
<h3 id="week-4-matrices-make-linear-mappings">Week 4: Matrices Make Linear Mappings</h3>
<p>In Module 4, we continue our discussion of matrices; first we think about how to code up matrix multiplication and matrix operations using the Einstein Summation Convention, which is a widely used notation in more advanced linear algebra courses. Then, we look at how matrices can transform a description of a vector from one basis (set of axes) to another. This will allow us to, for example, manipulate images. We'll also look at how to construct a convenient basis vector set in order to do such transformations. Then, we'll write some code to do these transformations and apply this work computationally.</p>
<p><strong>Learning Objectives</strong></p>
<ul>
<li>Identify matrices as operators</li>
<li>Relate the transformation matrix to a set of new basis vectors</li>
<li>Formulate code for mappings based on these transformation matrices</li>
<li>Write code to find an orthonormal basis set computationally</li>
</ul>
<h3 id="matrices-as-objects-that-map-one-vector-onto-another">Matrices as objects that map one vector onto another</h3>
<h4 id="introduction-to-einstein-summation-convention-and-the-symmetry-of-the-dot-product">Introduction to Einstein summation convention and the symmetry of the dot product</h4>
<p>There is a different, important way to write matrix transformations that we have not yet discussed. It's called the <a href="http://www.wikiwand.com/en/Einstein_notation"><strong>Einstein's Summation Convention</strong></a>. In this convention, we write down the actual operations on the elements of a matrix, which is useful when you're coding or programming. It also lets us see something neat about the dot product, and it lets us deal with <em>non-square</em> matrices.</p>
<p>When we started, we said that multiplying a matrix by a vector or with another matrix is a process of taking every element in each row of the first matrix multiplied with the corresponding element in each column of the other matrix, summing the products and putting them in place. In Einstein's Summation Convention, we represent the matrix product \(C = AB\):</p>
<p>
<script type="math/tex; mode=display">\begin{pmatrix} a_{11} & a_{12} & . & . & . & a_{1n} \\\ a_{21} & a_{22} &  &  &  & . &\\\ . & & & & & .  \\\ . & & & & & .  \\\ . & & & & & . \\\ a_{n1} & . & . & . & . & a_{nm}\end{pmatrix} \begin{pmatrix} b_{11} & b_{12} & . & . & . & b_{1m} \\\ b_{21} & b_{22} &  &  &  & . &\\\ . & & & & & .  \\\ . & & & & & .  \\\ . & & & & & . \\\ b_{m1} & . & . & . & . & b_{mp}\end{pmatrix} = AB</script>
</p>
<p>as,</p>
<p>
<script type="math/tex; mode=display">c_{ij} = a_{i1}b_{1j} + ... + a_{im}b_{mj} =  \sum_{k=1}^m a_{ik}b_{kj}</script>
</p>
<blockquote>
<p>To be clear, its the \(c_{ij}\) itself that is written Einstein's Summation Convention, not everything that comes to the right of the \(=\) sign.</p>
</blockquote>
<p>For \(i = 1, ..., n\) and \(j = 1, ..., p\), where \(A\) is a \(n \times m\) matrix, \(B\) is a \(m \times p\) matrix, and \(C\) is a \(n \times p\) matrix.</p>
<p>This is useful when we are implementing matrix multiplication in code, because it makes it obvious exactly what operations we need to perform. In this case, it should be obvious that we can run three loops over \(i\), \(j\) and \(k\), and then use an accumulator on the \(k\)'s to find the elements of the product matrix \(AB\).  </p>
<blockquote>
<p>We haven't talked about this yet, but now we can see it clearly. There's no reason, so long as the matrices have the same number of entries in \(k\), that the matrices we multiply need to be the same shape!</p>
</blockquote>
<p>Let's revisit the <strong>dot product</strong> in light of the Einstein Summation Convention. If we've got two vectors, let's call them \(u\) and \(v\), where \(u\) is a column vector having elements \(u_i\) and \(v\) is another column vector having elements \(v_i\).</p>
<p>
<script type="math/tex; mode=display">\begin{pmatrix}u_i \\\ \vdots \\\ u_{n}\end{pmatrix} \cdot \begin{pmatrix}v_i \\\ \vdots \\\ v_n \end{pmatrix}</script>
</p>
<p>When we dot them together, we are computing the following:</p>
<p>
<script type="math/tex; mode=display">  u \cdot v = \sum^n_{i=1}u_iv_i = u_1v_1 + u_2v_2 + ... + u_nv_n</script>
</p>
<p>Notice that,</p>
<p>
<script type="math/tex; mode=display">\begin{pmatrix}u_i \\\ \vdots \\\ u_{n}\end{pmatrix} \cdot \begin{pmatrix}v_i \\\ \vdots \\\ v_n \end{pmatrix} = \begin{pmatrix}u_i & \cdots & u_{n}\end{pmatrix} \cdot \begin{pmatrix}v_i \\\ \vdots \\\ v_n \end{pmatrix}</script>
</p>
<p>and so we notice that there's some equivalence between a <strong>matrix transformation</strong> (or <em>multiplication</em>) and the <strong>dot product</strong>. Lets explore this in more detail.</p>
<p><strong>Symmetry of the dot product</strong></p>
<p>Say we have the vector \(\hat u\), with components \(u_1\) and \(u_2\). Let's imagine what happens if we dot \(\hat u\) with the basis vector \(\hat e_1\). We know from previous sections that this gives us the length of the projection of \(\hat u_1\) on \(\hat e_1\) multiplied by the norm of \(\hat e_1\). But what if we do the reverse? What if we dot \(\hat e_1\) with \(\hat u\)? We already know that numerically, the result will be the same, as the dot product is <a href="http://www.wikiwand.com/en/Commutative_property"><strong>commutative</strong></a>. So geometrically, we can imagine drawing a line of symmetry between the point where the two projections cross:</p>
<p><img alt="dot_product_is_symmetric" src="../img/dot_product_is_symmetric.png" /></p>
<p>Previously, we stated (without proof, although the numerical proof is trivial) that the dot product is commutative, and now, we have shown geometrically why that is true.</p>
<p><strong>Conclusions</strong></p>
<p>In this section, we introduced <a href="http://www.wikiwand.com/en/Einstein_notation"><strong>Einstein's Summation Convention</strong></a>, which is a compact and computationally useful (but not very visually intuitive) way to write down matrix operations. This led to a discussion on the similarities between the dot product and matrix multiplication, where we noticed a connection between <strong>matrix multiplication</strong>, and the <strong>dot product</strong>, which itself has a geometric understanding as the concept of <strong>projection</strong>, i.e. projecting one vector onto another. This allows us to think about matrix multiplication <em>with a vector</em> as being the projection of that vector <em>onto the vectors composing the matrix</em> (i.e. the columns of the matrix).</p>
<h3 id="matrices-transform-into-the-new-basis-vector-set">Matrices transform into the new basis vector set</h3>
<blockquote>
<p>Watch <a href="https://youtu.be/P2LTAUO1TdA">this</a> video before reading this section. For more practice with changing basis, see <a href="https://www.khanacademy.org/math/linear-algebra/alternate-bases#change-of-basis">this</a> Khan Academy section.</p>
</blockquote>
<h4 id="matrices-changing-basis">Matrices changing basis</h4>
<p>We have said before that the <em>columns</em> of a transformation matrix are the <em>axes</em> of the new basis vectors after applying the mapping. We're now going to spend a little time looking at how to transform a vector <em>from one set of basis vectors to another</em>.</p>
<p>Let's say we have two sets of basis vectors, which define a first coordinate system (\(\text{CS}_1)\) and a second coordinate system (\(\text{CS}_2)\). Let the basis vectors of \(\text{CS}_2\), from the perspective of \(\text{CS}_1\) be:</p>
<p>
<script type="math/tex; mode=display">\hat b_1 = \begin{bmatrix}3 \\\ 1\end{bmatrix}, \hat b_2 = \begin{bmatrix}1 \\\ 1\end{bmatrix} </script>
</p>
<p>Lets package these basis vectors into a matrix \(\text{CS}_{21}\), for convenience</p>
<p>
<script type="math/tex; mode=display">\text{CS}_{21} = \begin{bmatrix}3 & 1 \\\ 1 & 1\end{bmatrix}</script>
</p>
<p>Think of these as the basis vectors of \(\text{CS}_2\) as they would appear in \(\text{CS}_1\). If we wanted to change the basis of any vectors in \(\text{CS}_2\) to \(\text{CS}_1\), we simply do:</p>
<p>
<script type="math/tex; mode=display">\text{CS}_{21} \cdot \text{vector in CS}_2 = \text{vector in CS}_1</script>
</p>
<p>E.g., for the vector</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix}\frac{3}{2} \\\ \frac{1}{2}\end{bmatrix}</script>
</p>
<p>defined in terms of \(\text{CS}_2\)</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix}3 & 1 \\\ 1 & 1\end{bmatrix} \begin{bmatrix}\frac{3}{2} \\\ \frac{1}{2}\end{bmatrix} = \begin{bmatrix}5  \\\ 2\end{bmatrix}</script>
</p>
<p>That is, a vector</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix}\frac{3}{2} \\\ \frac{1}{2}\end{bmatrix}</script>
</p>
<p>described in \(\text{CS}_2\), is described as</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix}5 \\\ 2\end{bmatrix}</script>
</p>
<p>in \(\text{CS}<em>1\). Why does this make sense? Well, you can think of \(\text{CS}</em>{21}\) as the transformation that takes the basis vectors of \(\text{CS}_1\) and moves them to the positions of the basis vectors of \(\text{CS}_2\). Applying this transformation to a vector in \(\text{CS}_2\), therefore, gives us the corresponding vector in \(\text{CS}_1\).</p>
<p>Now, how do we do the reverse? How do we <em>translate</em> a vector in \(\text{CS}_1\) to a vector in \(\text{CS}_2\)? All we need to do to change basis in the reverse case is to multiply a vector in one coordinate system by the inverse of the matrix containing the basis vectors of another:</p>
<p>
<script type="math/tex; mode=display">\text{CS}^{-1}_{21} \cdot \text{vector in CS}_1 = \text{vector in CS}_2</script>
</p>
<p>E.g.,</p>
<p>
<script type="math/tex; mode=display">\frac{1}{2}\begin{bmatrix}1 & -1 \\\ -1 & 3\end{bmatrix}\begin{bmatrix}5  \\\ 2\end{bmatrix} = \begin{bmatrix}\frac{3}{2} \\\ \frac{1}{2}\end{bmatrix}</script>
</p>
<p>Notice that this process gave us the coordinates of the vector</p>
<p>\[<script type="math/tex; mode=display">\begin{bmatrix}\frac{3}{2} \\\ \frac{1}{2}\end{bmatrix}</script>]</p>
<p>in \(\text{CS}_2\), which is equal to the vector that we started with.</p>
<h5 id="generalizing">Generalizing</h5>
<p>This is a little bit tricky, so lets generalize. Imagine we have two coordinate systems, "ours" and "theirs". We can translate vectors in "their" coordinate system to "our" coordinate system by applying a transformation \(A\), where \(A\) is a matrix whose columns contain the basis vectors of "their" coordinate system as they appear in "our coordinate system":</p>
<p>
<script type="math/tex; mode=display">\text{(1) } A \cdot \text{vector in "their" coordinate system} = \text{vector in "our" coordinate system}</script>
</p>
<p>To do the reverse, i.e. take a vector in "our" coordinate system and translate it to "their" coordinate system, we simply multiply "our" vector by the inverse of A</p>
<p>
<script type="math/tex; mode=display">\text{(2) } \text{vector in "their" coordinate system} = A^{-1} \cdot \text{vector in "our" coordinate system}</script>
</p>
<p>It should be obvious now why this is the case, we simply moved \(A\) over in the (1) to get (2)!</p>
<p><strong>Orthonormal basis set</strong></p>
<p>When we discussed vectors and projections, we said that if the new basis vectors were orthogonal then we could use projection to easily change basis.</p>
<blockquote>
<p>see <a href="#changing-basis">Changing basis</a> for a fleshed out example.</p>
</blockquote>
<p><strong>Summary</strong></p>
<p>Not orthogonal, use matrix multiplication. Orthogonal, use projection product.</p>
<h4 id="doing-a-transformation-in-a-changed-basis">Doing a transformation in a changed basis</h4>
<blockquote>
<p>Watch the last little bit of <a href="https://youtu.be/P2LTAUO1TdA?t=8m50s">this</a> video first.</p>
</blockquote>
<p>Lets discuss the process of applying a transformation in a changed basis. Say again that the basis vectors of coordinate system \(\text{CS}_2\) from the perspective of coordinate system \(\text{CS}_1\) are:</p>
<p>
<script type="math/tex; mode=display">\text{CS}_{21} = \begin{bmatrix}3 & 1 \\\ 1 & 1\end{bmatrix}</script>
</p>
<p>Say further that we have a vector in \(\text{CS}_2\) that we want to transform:</p>
<p>
<script type="math/tex; mode=display"> {c_2} = \begin{bmatrix}x \\\ y\end{bmatrix}</script>
</p>
<p>And the tranformation we want to apply is:</p>
<p>
<script type="math/tex; mode=display">N = \frac{1}{\sqrt{2}}\begin{bmatrix}1 & -1 \\\ 1 & 1\end{bmatrix}</script>
</p>
<blockquote>
<p>This rotates the vector space \(45^0\) counter-clockwise.</p>
</blockquote>
<p>How do we apply the transformation \(N\) to a vector defined by the coordinate system \(\text{CS}_2\)?</p>
<p>The first thing to do is take the vector \({c_2}\) and multiply it by \(\text{CS}_{21}\), that is, change the basis of the vector \({c_2}\) from \(\text{CS}_2\) to \(\text{CS}_1\):</p>
<p>
<script type="math/tex; mode=display"> {c_1} = \begin{bmatrix}3 & 1 \\\ 1 & 1\end{bmatrix} \begin{bmatrix}x \\\ y\end{bmatrix}</script>
</p>
<p>Then, we can apply the transformation:</p>
<p>
<script type="math/tex; mode=display"> {c_1}' = R \cdot \text{CS}_{21} \cdot  {c_2}</script>
<script type="math/tex; mode=display"> {c_1}' = \frac{1}{\sqrt{2}}\begin{bmatrix}1 & -1 \\\ 1 & 1\end{bmatrix} \begin{bmatrix}3 & 1 \\\ 1 & 1\end{bmatrix} \begin{bmatrix}x \\\ y\end{bmatrix}</script>
<script type="math/tex; mode=display">= \frac{1}{\sqrt{2}}\begin{bmatrix}2 & 0 \\\ 4 & 2\end{bmatrix} \begin{bmatrix}x \\\ y\end{bmatrix}</script>
</p>
<p>What if, once we obtained this output vector, we wanted to change its basis <em>back</em> to \(\text{CS}<em>2\)? Recall from the last section that we multiply the whole thing by the inverse of \(\text{CS}</em>{21}^{-1}\)</p>
<p>
<script type="math/tex; mode=display">{c_2}' = \text{CS}_{21}^{-1} \cdot R \cdot \text{CS}_{21} \cdot {c_2}</script>
</p>
<p>
<script type="math/tex; mode=display">{c_2}' = \frac{1}{2} \begin{bmatrix}1 & -1 \\\ -1 & 3\end{bmatrix} \frac{1}{\sqrt{2}}\begin{bmatrix}1 & -1 \\\ 1 & 1\end{bmatrix} \begin{bmatrix}3 & 1 \\\ 1 & 1\end{bmatrix} \begin{bmatrix}x \\\ y\end{bmatrix}</script>
</p>
<p>
<script type="math/tex; mode=display">= \frac{1}{2} \begin{bmatrix}1 & -1 \\\ -1 & 3\end{bmatrix}\frac{1}{\sqrt{2}}\begin{bmatrix}2 & 0 \\\ 4 & 2\end{bmatrix} \begin{bmatrix}x \\\ y\end{bmatrix}</script>
</p>
<p>
<script type="math/tex; mode=display">= \frac{1}{\sqrt{2}}\begin{bmatrix}-1 & -1 \\\ 5 & 3\end{bmatrix} \begin{bmatrix}x \\\ y\end{bmatrix}</script>
</p>
<p>This operation essentially builds on the previous operation to return the transformed output vector, \({c_1}'\) relative to \(\text{CS}_2\), that is it returns \({c_2}'\), where \({c_2}'\) is where \(c_2\) ends up after in the basis \(\text{CS}_2\) after some transformation \(M\) has been applied.</p>
<h5 id="generalizing_1">Generalizing</h5>
<p>Again, this is tricky, so lets generalize. Imagine we have two coordinate systems, "ours" and "theirs". If we are given:</p>
<ul>
<li>a vector with its basis in "their" coordinate system</li>
<li>\(A\), a matrix whose columns are the basis vectors of "their" coordinate system as they appear in "our" coordinate system</li>
<li>\(M\) some transformation, with its basis in "our" coordinate system</li>
</ul>
<p>Then,</p>
<p><strong>Change the vector in "their" basis to "ours":</strong></p>
<p>
<script type="math/tex; mode=display">A \cdot \text{vector in "their" coordinate system}</script>
</p>
<p><strong>Apply the transformation in "our" coordinate system:</strong></p>
<p>
<script type="math/tex; mode=display">M \cdot A \cdot \text{vector in "their" coordinate system}</script>
</p>
<p><strong>Change the resulting vector back to "their" coordinate system:</strong></p>
<p>
<script type="math/tex; mode=display">A^{-1} \cdot M \cdot A \cdot \text{vector in "their" coordinate system}</script>
</p>
<p>In sum, the transformation \(A^{-1}MA\) will take any vector in "their" coordinate system, apply some transformation in "our" coordinate system, and return the resulting vector in "their" coordinate system.</p>
<h3 id="making-multiple-mappings-deciding-if-these-are-reversible">Making multiple mappings, deciding if these are reversible</h3>
<h4 id="orthogonal-matrices">Orthogonal matrices</h4>
<p>It is very useful to compose a transformation matrix whose column and row vectors make up a <em>new basis</em>, with the additional constraint of making all of these component vectors orthogonal. Such a square matrix of  orthonormal columns and rows is known as an <a href="http://www.wikiwand.com/en/Orthogonal_matrix"><strong>orthogonal matrix</strong></a>.</p>
<p>In this section we are going to look at <em>how</em> to construct such a matrix, and why it's useful.</p>
<p><strong>(Aside) Transpose</strong></p>
<p>First, we need to define a new matrix operation called the <a href="http://www.wikiwand.com/en/Transpose"><strong>transpose</strong></a>. The transpose of a matrix is an <em>operator</em> which flips a matrix <em>over its diagonal</em>, that is it switches the row and column indices of the matrix by producing another matrix denoted as \(A^T\). It is achieved by any one of the following equivalent actions:</p>
<ul>
<li>reflect \(A\) over its main diagonal (which runs from top-left to bottom-right) to obtain \(A^T\),</li>
<li>write the rows of \(A\) as the columns of \(A^T\),</li>
<li>write the columns of \(A\) as the rows of \(A^T\).</li>
</ul>
<p><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/e/e4/Matrix_transpose.gif" /></p>
<p>Formally, the <em>i</em>-th row, <em>j</em>-th column element of \(A^T\) is the <em>j</em>-th row, <em>i</em>-th column element of \(A\):</p>
<p>
<script type="math/tex; mode=display">[A^T]_{ij} = [A]_{ji}</script>
</p>
<p>Now let's imagine I have a \(n \times n\) matrix \(A\), with a series of column vectors which are going to be the basis vectors of the some new transformed vector space:</p>
<p>
<script type="math/tex; mode=display">A = \begin{pmatrix} \begin{pmatrix}\vdots \\\ \hat a_1 \\\ \vdots\end{pmatrix} & \begin{pmatrix} \vdots \\\ \hat a_2\\\ \vdots \end{pmatrix} & \dots \begin{pmatrix} \vdots \\\ \hat a_n\\\ \vdots \end{pmatrix} \end{pmatrix}</script>
</p>
<p>Lets place two more constaints on this matrix \(A\):</p>
<ul>
<li>First, the column vectors \(a_i\) have unit length</li>
<li>Second, the column vectors \(a_i\) are orthogonal to each other.</li>
</ul>
<blockquote>
<p>That is, \(\hat a_i \cdot \hat a_j = 0\) for \(\forall i \ne j\) and \(\hat a_i \cdot \hat a_i = 1\).</p>
</blockquote>
<p>Lets think about what happens when we multiply \(A\) by its transpose, \(A^T\):</p>
<p>
<script type="math/tex; mode=display">A^TA =  \begin{pmatrix} \begin{pmatrix}\dots & \hat a_1 & \dots\end{pmatrix} \\\ \begin{pmatrix} \dots & \hat a_2 & \dots \end{pmatrix} \\\ \vdots \\\ \begin{pmatrix} \dots & \hat a_n & \dots \end{pmatrix} \end{pmatrix}  \begin{pmatrix} \begin{pmatrix}\vdots \\\ \hat a_1 \\\ \vdots\end{pmatrix} & \begin{pmatrix} \vdots \\\ \hat a_2\\\ \vdots \end{pmatrix} & \dots \begin{pmatrix} \vdots \\\ \hat a_n\\\ \vdots \end{pmatrix} \end{pmatrix} = I_n</script>
</p>
<p>So what we notice is that in the case where \(A\) is composed of vectors that are <em>normal</em> to each other and have <em>unit length</em>, (i.e. when they're <strong>orthonormal</strong>), then \(A^TA = I\). Stated another way, \(A^T\) in this situation is actually the <em>inverse</em> of \(A\)! This special case is known as an <strong>orthogonal matrix</strong>.</p>
<p>Another thing to note is that because all the basis vectors are of unit length, it must scale space by a <em>factor of one</em>. Stated another way, the determinant of an orthogonal matrix must be either plus or minus one.</p>
<p>
<script type="math/tex; mode=display">\vert A \vert = \pm 1</script>
</p>
<p>Where the minus one arises if the new basis vector set <em>flip space around</em> (from right-handed to left-handed or vice versa). Notice that if \(A^T\), the inverse of \(A\), then by the definition of the inverse:</p>
<p>
<script type="math/tex; mode=display">A^TA = AA^T = I_n</script>
</p>
<p>So, we could pre- or post- multiply and still get the identity. This means that the rows of the orthogonal matrix are also orthonormal to each other! So, the transpose matrix of an orthogonal basis set, is itself another orthogonal basis set.</p>
<p>Now, remember that in the last module on vectors, we said that transforming a vector onto a new coordinate system was as easy as taking the projection or dot product of that vector onto each of the new bases vectors, <em>as long as they were orthogonal to each other</em>. So, if we have a vector \(r\) and we want to project \(r\) into a new set of axes, let's call them \(\hat e_1\) and  \(\hat e_2\), as long as these vectors are orthogonal to each other, then we  can project into the new vector space just by taking the dot product of \(r\) with \(\hat e_2\), and the dot product of \(r\) with \(\hat e_1\), and then we'd have its components in the new set of axis.</p>
<blockquote>
<p>See <a href="#changing-basis">Changing basis</a> for a walked-through example.</p>
</blockquote>
<p><strong>Conclusions</strong></p>
<p>In this section, we introduced the most convenient basis vector set of all, the <strong>orthonormal bases vector set</strong>.</p>
<p>Wherever possible we want to use an orthonormal basis vector set represented as a orthogonal matrix \(A\). This set of vectors has the following properties and consequences that make it easy to work with:</p>
<ul>
<li>the <strong>transpose</strong> of such a matrix will be its <strong>inverse</strong>, which makes the inverse incredibly easy to compute,</li>
<li>the transformation will be <strong>reversible</strong> because space doesn't get collapsed by any dimensions,</li>
<li>the projections (i.e. the result of computing the projection of a vector onto the matrix \(A\)) are just the dot products.</li>
</ul>
<p>One final note. If we arrange the bases vectors in the correct order, then the <em>determinant will be one</em>.</p>
<p>
<script type="math/tex; mode=display"> \vert A \vert = 1</script>
</p>
<p>An easy way to check if they aren't in the right order, is to check if the determinant is minus one. This means we've transformed our space from right to left handed orientation. All we have to do to remedy this is to exchange a pair of vectors in \(A\) such that \(\vert A \vert = 1\).</p>
<h3 id="recognizing-mapping-matrices-and-applying-these-to-data">Recognizing mapping matrices and applying these to data</h3>
<h4 id="the-gramschmidt-process">The Gram–Schmidt process</h4>
<p>In the last section, we motivated the idea that life is much easier if we can construct an orthonormal basis vector set, but we haven't talked about how to do it. In this section, we will explore just that.</p>
<p>We'll start from the assumption that we already have some linearly independent vectors that span the space we're interested in. Say we have some such vectors \(V = {v_1, v_2, ..., v_n}\),</p>
<blockquote>
<p>If you want to check linear independence, you can write down your vectors as the the columns in a matrix and check that the determinant of that matrix isn't zero.</p>
</blockquote>
<p>but they aren't orthogonal to each other or of unit length. Our life would probably be easier if we could construct some orthonormal basis. As it turns out, there's a process for doing just that which is called the <a href="http://www.wikiwand.com/en/Gram–Schmidt_process"><strong>Gram-Schmidt process</strong></a>.</p>
<p>Let's take the first vector in the set to be \(v_1\). In this first step, we're just going to normalize \(v_1\) to get our eventual first basis vector \(e_1\)</p>
<p>
<script type="math/tex; mode=display">e_1 = \frac{v_1}{\vert v_1 \vert} </script>
</p>
<p>Now, we can think of \(v_2\) as being composed of two things: a component that is in the direction of \(e_1\) and a component that's perpendicular to \(e_1\). We can find the component that's in the direction of \(e_1\) by taking the vector projection \(v_2\) onto \(e_1\):</p>
<p>
<script type="math/tex; mode=display">v_2 = (v_2 \cdot e_1) \frac{e_1}{\vert e_1 \vert} </script>
</p>
<blockquote>
<p>\(|e_1|\) is 1 so we could actually omit it.</p>
</blockquote>
<p>If we subtract this vector projection from \(v_2\) we get \(u_2\), a vector which is orthogonal to \(e_1\):</p>
<p>
<script type="math/tex; mode=display">u_2 = v_2 - (v_2 \cdot e_1) e_1</script>
</p>
<p>Finally, dividing \(u_2\) by its length gives us \(e_2\), the unit vector orthogonal to \(e_1\):</p>
<p>
<script type="math/tex; mode=display">e_2 = \frac{u_2}{\vert u_2 \vert} </script>
</p>
<p>We could continue this process for all vectors in our set \(V\). The general formula (in pseudocode) is:</p>
<pre><code class="python"># For all vectors in our set V
for i in |V|:
  # For all vectors in our set V that come before v_i
  for j in i:
    # Subtract the component of v_i in the direction of the previous vectors v_j
    v_i = v_i - v_i.dot(v_j) * v_j
  # If |v_i| is not zero, normalize it to unit length. Otherwise it is linearly dependent on a
  # previous vector, so set it equal to the zero vector.
  if |v_i| &gt; 0:
    v_i = v_i / |v_i|
  else:
    v_i = zero_vector
</code></pre>

<p><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/e/ee/Gram-Schmidt_orthonormalization_process.gif" /></p>
<p><strong>Conclusions</strong></p>
<p>So that's how we construct an orthonormal basis set, which makes our lives much easier for all the reasons we discussed <a href="#orthogonal-matrices">here</a>.</p>
<h4 id="reflecting-in-a-plane">Reflecting in a plane</h4>
<p>This is a rather involved example, and is probably best if you just watch it yourself <a href="https://www.coursera.org/learn/linear-algebra-machine-learning/lecture/oXE0Y/example-reflecting-in-a-plane">here</a>. If I can find the time, i'll make notes for the video!</p>
<h3 id="week-5-eigenvalues-and-eigenvectors">Week 5: Eigenvalues and Eigenvectors</h3>
<p>Eigenvectors are particular vectors that are unrotated by a transformation matrix, and eigenvalues are the amount by which the eigenvectors are stretched. These special 'eigen-things' are very useful in linear algebra and will let us examine Google's famous PageRank algorithm for presenting web search results. Then we'll apply this in code, which will wrap up the course.</p>
<h4 id="learning-objectives">Learning Objectives</h4>
<ul>
<li>Identify geometrically what an eigenvector/value is</li>
<li>Apply mathematical formulation in simple cases</li>
<li>Build an intuition of larger dimensional eigensystems</li>
<li>Write code to solve a large dimensional eigen problem</li>
</ul>
<h3 id="what-are-eigen-things">What are eigen-things?</h3>
<h4 id="what-are-eigenvalues-and-eigenvectors">What are eigenvalues and eigenvectors?</h4>
<p>The word, "eigen" is perhaps most usefully translated from the German as meaning <em>characteristic</em>. So when we talk about an <em>eigenproblem</em>, we're talking about finding the <em>characteristic properties of something</em>. But characteristic of what? This module, like the previous weeks, will try and explain this concept of <em>"eigen-ness"</em> primarily through a geometric interpretation, which allows us to discuss images rather than immediately getting tangled up in the maths.</p>
<blockquote>
<p>This topic is often considered by students to be quite tricky. But once you know how to sketch these problems, the rest is just algebra.</p>
</blockquote>
<p>As you've seen from previous weeks, it's possible to express the concept of <strong>linear transformations</strong> using <strong>matrices</strong>. These operations can include <strong>scalings</strong>, <strong>rotations</strong>, and <strong>shears</strong>.</p>
<p><img alt="matrix_transformations" src="../img/matrix_transformations.gif" /></p>
<p>Often, when applying these transformations, we are thinking about what they might do to a <em>specific vector</em>. However, it can also be useful to think about what it might look like when they are applied to every vector in this space. This can be most easily visualized by drawing a square centered at the origin, and then seeing how your shape is distorted when you apply the transformation. So if we apply a scaling of 2 in the vertical direction, the square would now become a rectangle. Whereas, if we applied a horizontal shear to this space, it might look something like this.</p>
<p><img alt="scale_and_shear" src="../img/scale_and_shear.gif" /></p>
<p>Now, here's the key concept, we are using our little square to help us visualize what is happening to many vectors. But notice that some vectors end up lying on the same line that they started on whereas, others do not. To highlight this, I'm going to draw three specific vectors onto our initial square. Now, let's consider our vertical scaling again, and think about what will happen to these three vectors. As you can see, the horizontal green vector is unchanged pointing in the same direction and having the same length. The vertical pink vector is also still pointing in the same direction as before but its length is doubled. Lastly, the diagonal orange vector used to be exactly 45 degrees to the axis, it's angle has now increased as has its length.</p>
<p><img alt="vertical_stretch" src="../img/vertical_stretch.gif" /></p>
<p>I hope you can see that actually besides the horizontal and vertical vectors, any other vectors' direction would have been changed by this vertical scaling. So in some sense, the horizontal and vertical vectors are special, they are characteristic of this particular transform, which is why we refer to them as <a href=""><strong>eigenvectors</strong></a>. Furthermore, because the horizontal vectors' length was unchanged, we say that it has a corresponding <a href=""><strong>eigenvalue</strong></a> of one whereas, the vertical eigenvector doubled in length, so we say it has an eigenvalue of two. So, from a conceptual perspective, that's about it, for 2D eigen-problems, we simply take a transform and we look for the vectors who are still laying on the same span as before, and then we measure how much their length has changed. This is basically what eigenvectors and their corresponding eigenvalues are.</p>
<p>Let's look at two more classic examples to make sure that we can generalize what we've learned. First, let look look at pure <strong>shear</strong>, where pure means that we aren't performing any scaling or rotation in addition, so the area is unchanged. As I hope you've spotted, it's only the green horizontal line that is still laying along its original span, and all the other vectors will be shifted:</p>
<p><img alt="shear" src="../img/shear.gif" /></p>
<p>Finally, let's look at <strong>rotation</strong>. Clearly, this thing has got no <strong>eigenvectors</strong> at all, as all of the vectors have been rotated off their original span:</p>
<p><img alt="rotation" src="../img/rotation.gif" /></p>
<p><strong>Conclusions</strong></p>
<p>In this lecture, we've already covered almost all of what you need to know about eigenvectors and eigenvalues. Although we've only been working in two dimensions so far, the concept is exactly the same in three or more dimensions. In the rest of the module, we'll be having a look at some special cases, as well as discussing how to describe what we've observed in more mathematical terms.</p>
<h3 id="getting-into-the-detail-of-eigenproblems">Getting into the detail of eigenproblems</h3>
<h4 id="special-eigen-cases">Special eigen-cases</h4>
<p>As we saw previously, <strong>eigenvectors</strong> are those which lie along the same span both <em>before and after</em> applying a linear transform to a space. <strong>Eigenvalues</strong> are simply the amount that each of those vectors has been <em>stretched</em> in the process. In this section, we're going to look at <strong>three</strong> special cases to make sure the intuition we've built so far is robust, and then we're going to try and extend this concept into three dimensions.</p>
<p>The first example we're going to consider is that of a <em>uniform</em> scaling, which is where we scale by the same amount in each direction. As you will hopefully have spotted, not only are all three of the vectors that I've highlighted eigenvectors, but in fact, for a uniform scaling, any vector would be an eigenvector.</p>
<p><img alt="uniform_scaling" src="../img/uniform_scaling.gif" /></p>
<p>In this second example, we're going to look at rotation. In the previous section, we applied a small rotation, and we found that it had no eigenvectors. However, there is one case of non-zero pure rotation which does have at least some eigenvectors, and that is 180 degrees. As you can see, the three eigenvectors are still laying on the same spans as before, but just pointing in the opposite direction. This means that once again, all vectors for this transform are eigenvectors, and they all have eigenvalues of -1, which means that although the eigenvectors haven't changed length, they are all now pointing in the opposite direction.</p>
<p><img alt="flipped_rotation" src="../img/flipped_rotation.gif" /></p>
<p>This third case, we're going to look at a combination of a horizontal shear and a vertical scaling, and it's slightly less obvious than some of the previous examples. Just like the pure shear case we saw previously, the green horizontal vector is an eigenvector and its eigenvalue is still one. However, despite the fact that neither of the other two vectors shown are eigen, this transformation does have two eigenvectors.</p>
<p><img alt="shear_and_scale_two_eigens" src="../img/shear_and_scale_two_eigens.gif" /></p>
<p>Let's now apply the inverse transform and watch our parallelogram go back to its original square. But this time, with our eigenvector visible. Hopefully, you're at least convinced that it is indeed an eigenvector as it stays on its own span.</p>
<p><img alt="sneaky_eigen_vector" src="../img/sneaky_eigen_vector.gif" /></p>
<p>This shows us that while the concept of eigenvectors is fairly straightforward, eigenvectors aren't always easy to spot.This problem is even tougher in three or more dimensions, and many of the uses of eigen theory in machine learning frame the system as being composed of hundreds of dimensions or more. So, clearly, we're going to need a more robust mathematical description of this concept to allow us to proceed.</p>
<p>Before we do, let's just take a look at one quick example in 3D. Clearly, scaling and shear are all going to operate in much the same in 3D as they do in 2D. However, rotation does take on a neat new meaning. As you can see from the image, although both the pink and green vectors have changed direction, the orange vector has not moved. This means that the orange vector is an eigenvector, but it also tells us, as a physical interpretation, that if we find the eigenvector of a 3D rotation, it means we've also found the axis of rotation.</p>
<p><img alt="rotation_in_3d" src="../img/rotation_in_3d.gif" /></p>
<p>In this video, we've covered a range of special cases, which I hope have prompted the questions in your mind about how we're going to go about writing a formal definition of an eigen-problem. And this is exactly what we're going to be discussing next time. See you then.</p>
<h4 id="calculating-eigenvectors">Calculating eigenvectors</h4>
<p>At this point, we should now have a reasonable feeling for what an eigen-problem looks like, at least geometrically. In this section, we're going to formalize this concept into an algebraic expression, which will allow us to calculate eigenvalues and eigenvectors whenever they exist. Once you've understood this method, we'll be in a good position to see why you should be glad that computers can do this for you.</p>
<p>If we consider a transformation \(A\), what we have seen is that if it has eigenvectors at all, then these are simply the vectors which stay on the same span following a transformation. They can change length and even point in an opposite direction entirely, but if they remain in the same span, they are <strong>eigenvectors</strong>. If we call our eigenvector \(x\), then we can say the following expression.</p>
<p>
<script type="math/tex; mode=display">Ax = \lambda x</script>
</p>
<p>Where, on the left hand side, we're applying the transformation matrix \(A\) to a vector \(x\), and on the right-hand side, we are simply stretching a factor \(x\) by some scalar factor \(\lambda\). So \(\lambda\) is just some number. We're trying to find values of \(x\) that make the two sides equal. Another way of saying this is that for our eigenvectors, having \(A\) apply to them just scales their length.</p>
<blockquote>
<p>Or does nothing at all, which is the same as scaling the length by a factor of 1.</p>
</blockquote>
<p>So in this equation, \(A\) is an \(n\)-dimensional transform, meaning it must be an \(n \times n\) square matrix. The eigenvector \(x\) must therefore be an \(n\)-dimensional vector. To help us find the solutions to this expression, we can rewrite it by putting all the terms on one side and then factorizing.</p>
<p>
<script type="math/tex; mode=display">(A - \lambda I) x = 0</script>
</p>
<blockquote>
<p>If you're wondering where the \(I\) term came from, it's just an \(n \times n\) identity matrix. We didn't need this in the first expression we wrote, as multiplying vectors by scalars is defined. However, subtracting scalars from matrices is <em>not defined</em>, so the \(I\) just tidies up the math, without changing the meaning.</p>
</blockquote>
<p>Now that we have this expression, we can see that for the left-hand side to equal 0, either the contents of the brackets must be 0 or the vector \(x\) is 0. So we're actually not interested in the case where the vector \(x\) is 0. That's when it has no length or direction and is what we call a <em>trivial solution</em>. Instead, we are interested in the case where the term in brackets is 0.</p>
<p>Referring back to the material in the previous parts of the course, we can test if a matrix operation will result in a 0 output by calculating its determinant. So,</p>
<p>
<script type="math/tex; mode=display">det (A - \lambda I) = 0</script>
</p>
<p>Calculating the determinants manually is a lot of work for high dimensional matrices. So let's just try applying this to an arbitrary two by two transformation. Let's say,</p>
<p>
<script type="math/tex; mode=display">A = \begin{pmatrix}a & b \\ c & d\end{pmatrix}</script>
</p>
<p>Substituting this into our eigen-finding expression gives the following:</p>
<p>
<script type="math/tex; mode=display">det \Biggl( \begin{pmatrix}a & b \\\ c & d\end{pmatrix}- \begin{pmatrix}\lambda & 0 \\\ 0 & \lambda\end{pmatrix} \Biggl ) = 0</script>
</p>
<p>Evaluating this determinant, we get what is referred to as the <a href="http://www.wikiwand.com/en/Characteristic_polynomial"><strong>characteristic polynomial</strong></a>, which looks like this.</p>
<p>
<script type="math/tex; mode=display">\lambda^2 - (a + d) \lambda + ad - bc = 0 </script>
</p>
<p>Our eigenvalues are simply the <em>solutions of this equation</em>, and we can then plug these eigenvalues back into the original expression to calculate our eigenvectors. Rather than continuing with our generalized form, this is a good moment to apply this to a simple transformation, for which we already know the eigensolution.</p>
<p>Let's take the case of a vertical scaling by a factor of two, which is represented by the transformation matrix</p>
<p>
<script type="math/tex; mode=display">A = \begin{pmatrix} 1 &  0 \\\ 0 & 2\end{pmatrix} </script>
</p>
<p>We can then apply the method that we just described and take the determinant of \(A - \lambda I\) and then set it to zero and solve:</p>
<p>
<script type="math/tex; mode=display">det \begin{pmatrix} 1 - \lambda & 0 \\\ 0 & 2- \lambda \end{pmatrix} = (1 - \lambda)(2-\lambda) = 0</script>
</p>
<p>This means that our equation must have solutions at \(\lambda = 1\) and \(\lambda = 2\). Thinking back to our original eigen-finding formula, \((A - \lambda I)x = 0\), we can now sub these two solutions back in. So thinking about the case where \(\lambda = 1\),</p>
<p>
<script type="math/tex; mode=display">@\lambda = 1: \begin{pmatrix} 1 - 1 & 0 \\\ 0 & 2 - 1 \end{pmatrix}\begin{pmatrix} x_1 \\\ x_2\end{pmatrix}  = \begin{pmatrix} 0 & 0 \\\ 0 & 1 \end{pmatrix}\begin{pmatrix} x_1 \\\ x_2\end{pmatrix} = \begin{pmatrix} 0 \\\ x_2\end{pmatrix} = 0</script>
</p>
<p>Now, thinking about the case where \(\lambda = 2\),</p>
<p>
<script type="math/tex; mode=display">@\lambda = 2: \begin{pmatrix} 1 - 2 & 0 \\\ 0 & 2 - 2 \end{pmatrix}\begin{pmatrix} x_1 \\\ x_2\end{pmatrix}  = \begin{pmatrix} -1 & 0 \\\ 0 & 0 \end{pmatrix}\begin{pmatrix} x_1 \\\ x_2\end{pmatrix} = \begin{pmatrix} -x_1 \\\ 0 \end{pmatrix} = 0</script>
</p>
<p>So what do these two expressions tell us? Well, in the case where our eigenvalue \(\lambda = 1\), we've got an eigenvector where the \(x_2\) term must be zero. But we don't really know anything about the \(x_1\) term. Well, this is because, of course any vector that points along the horizontal axis could be an eigenvector of this system. So we write that by saying</p>
<p>
<script type="math/tex; mode=display">@\lambda = 1: \begin{pmatrix} t  \\\ 0 \end{pmatrix}</script>
</p>
<p>using an arbitrary parameter \(t\). Similarly for the \(\lambda = 2\) case, we can say that our eigenvector must equal</p>
<p>
<script type="math/tex; mode=display">@\lambda = 2: \begin{pmatrix} 0  \\\ t \end{pmatrix}</script>
</p>
<p>because as long as it doesn't move at all in the horizontal direction, any vector that's purely vertical would therefore also be an eigenvector of this system, as they all would lie along the same span. So now we have two eigenvalues, and their two corresponding eigenvectors.</p>
<p>Let's now try the case of a rotation by 90-degrees anti-clockwise, to ensure that we get the result that we expect which, if you remember, is no eigenvectors at all. The transformation matrix corresponding to a 90-degree rotation is as follows:</p>
<p>
<script type="math/tex; mode=display">A = \begin{pmatrix} 0 & -1 \\\ 1 &  0 \end{pmatrix}</script>
</p>
<p>So applying the formula once again we get</p>
<p>
<script type="math/tex; mode=display">det \begin{pmatrix}-\lambda & -1 \\\ 1 & -\lambda \end{pmatrix} = \lambda^2 + 1 = 0</script>
</p>
<p>which doesn't have any real numbered solutions at all, hence, no real eigenvectors. We can still calculate some complex eigenvectors using imaginary numbers, but this is beyond what we need for this particular course.</p>
<p><strong>Conclusions</strong></p>
<p>Despite all the fun that we've just been having, the truth is that you will almost certainly never have to perform this calculation by hand. Furthermore, we saw that our approach required finding the roots of a polynomial of order \(n\), i.e., the dimension of your matrix. Which means that the problem will very quickly stop being possible by analytical methods alone. So when a computer finds the eigensolutions of a 100 dimensional problem it's forced to employ iterative numerical methods. However, I can assure you that developing a strong conceptual understanding of eigen problems will be much more useful than being really good at calculating them by hand.</p>
<p>In this sections, we translated our geometrical understanding of eigenvectors into a robust mathematical expression, and validated it on a few test cases. But I hope that I've also convinced you that working through lots of eigen-problems, as is often done in engineering undergraduate degrees, is not a good investment of your time if you already understand the underlying concepts. This is what computers are for. Next video, we'll be referring back to the concept of basis change to see what magic happens when you use eigenvectors as your basis. See you then.</p>
<h3 id="when-changing-to-the-eigenbasis-is-really-useful">When changing to the eigenbasis is really useful</h3>
<h4 id="changing-to-the-eigenbasis">Changing to the eigenbasis</h4>
<p>So now that we know what eigenvectors are and how to calculate them, we can combine this idea with a concept of changing basis which was covered earlier in the course. What emerges from this synthesis is a particularly powerful tool for performing efficient matrix operations called <a href="http://www.wikiwand.com/en/Matrix_diagonalization"><strong>diagonalisation</strong></a>. Sometimes, we need to apply the same matrix multiplication many times.</p>
<p>For example, imagine a transformation matrix \(T\) represents the change in location of a particle after a single time step. So we can write that our initial position, described by vector \(v_0\), multiplied by the transformation \(T\) gives us our new location, \(v_1\).</p>
<p><img alt="" src="../img/v_0_to_v_1.gif" /></p>
<p>To work out where our particle will be after two time steps, we can find \(v_2\) by simply multiplying \(v_1\) by \(T\), which is of course the same thing as multiplying \(v_0\) by \(T\) two times. So \(v_2 = T^2 v_0\).</p>
<p><img alt="" src="../img/v_1_to_v_2.gif" /></p>
<p>Now imagine that we expect the same linear transformation to occur every time step for \(n\) time steps. We can write this transformation as</p>
<p>
<script type="math/tex; mode=display">v_n = T^n v_0</script>
</p>
<p>You've already seen how much work it takes to apply a single 3D matrix multiplication. So if we were to imagine that \(T\) tells us what happens in one second, but we'd like to know where our particle is in two weeks from now, then \(n\) is going to be around 1.2 million, i.e., we'd need to multiply \(T\) by itself more than a million times, which may take quite a while.</p>
<p><img alt="" src="../img/v_0_to_v_n.gif" /></p>
<p>If all the terms in the matrix are zero except for those along the leading diagonal, we refer to it as a <a href="http://www.wikiwand.com/en/Diagonal_matrix"><strong>diagonal matrix</strong></a>. When raising matrices to powers, diagonal matrices make things a lot easier. All you need to do is put each of the terms on the diagonal to the power of \(n\) and you've got the answer. So in this case,</p>
<p>
<script type="math/tex; mode=display">T^n = \begin{pmatrix} a^n & 0 & 0 \\\ 0 & b^n & 0 \\\ 0 & 0 & c^n\end{pmatrix}</script>
</p>
<p>It's simple enough, but what if \(T\) is not a diagonal matrix? Well, as you may have guessed, the answer comes from eigen-analysis. Essentially, what we're going to do is simply change to a basis where our transformation \(T\) <em>becomes</em> diagonal, which is what we call an eigen-basis. We can then easily apply our power of \(n\) to the diagonalized form, and finally transform the resulting matrix back again, giving us \(T^n\), but avoiding much of the work. As we saw in the section on changing basis, each column of our transform matrix simply represents the new location of the transformed unit vectors. So, to build our eigen-basis conversion matrix, we just plug in each of our eigenvectors as columns:</p>
<p>
<script type="math/tex; mode=display">C = \begin{pmatrix}x_1 & x_2 & x_3 \\\ \vdots & \vdots & \vdots\end{pmatrix}</script>
</p>
<blockquote>
<p>However, don't forget that some of these maybe complex, so not easy to spot using the purely geometrical approach, but they are appear in the math just like the others.</p>
</blockquote>
<p>Applying this transform, we find ourselves in a world where multiplying by \(T\) is effectively just a pure scaling, which is another way of saying that it can now be represented by a diagonal matrix. Crucially, this diagonal matrix, \(D\), contains the corresponding eigenvalues of the matrix \(T\). So,</p>
<p>
<script type="math/tex; mode=display">D = \begin{pmatrix} \lambda_1 & 0 & 0 \\\ 0 & \lambda_2 & 0 \\\ 0 & 0 & \lambda_3\end{pmatrix}</script>
</p>
<p>We're so close now to unleashing the power of eigen. The final link that we need to see is the following. Bringing together everything we've just said, it should now be clear that applying the transformation \(T\) is just the same as converting to our eigenbasis, applying the diagonalized matrix, and then converting back again. So</p>
<p>
<script type="math/tex; mode=display">T = CDC^{-1}</script>
</p>
<p>which suggests that</p>
<p>
<script type="math/tex; mode=display">T^2 = CDC^{-1}CDC^{-1} = CDDC^{-1} = CD^2C^{-1}</script>
</p>
<p>So hopefully you've spotted that in the middle of our expression on the right-hand side, you've got \(C\) multiplied by C inverse. But multiplying a matrix and then by its inverse is just the same as doing nothing at all. So we can simply remove this operation. And then we can finish this expression by saying, well this must be \(CD^2C^{-1}\). We can of course then generalize this to any power of \(T\) we'd like. So finally we can say that,</p>
<p>
<script type="math/tex; mode=display">T^n = CD^nC^{-1}</script>
</p>
<p>We now have a method which lets us apply a transformation matrix as many times as we'd like without paying a large computational cost.</p>
<p><img alt="eigenbasis" src="../img/eigenbasis.png" /></p>
<p><strong>Conclusions</strong></p>
<p>This result brings together many of the ideas that we've encountered so far in this course. Check out <a href="https://www.coursera.org/learn/linear-algebra-machine-learning/lecture/zYzjM/eigenbasis-example">this</a> video, where we'll work through a short example just to ensure that this approach lines up with our expectations when applied to a simple case.</p>
<h3 id="making-the-pagerank-algorithm">Making the PageRank algorithm</h3>
<h4 id="pagerank">PageRank</h4>
<p>The final topic of this module on Eigenproblems, as well as the final topic of this course as a whole, will focus on an algorithm called <a href="http://www.wikiwand.com/en/PageRank"><strong>PageRank</strong></a>. This algorithm was famously published by and named after Google founder Larry Page and colleagues in 1998. And was used by Google to help them decide which order to display their websites when they returned from search. The central assumption underpinning page rank is that the importance of a website is related to its links to and from other websites, and somehow Eigen theory comes up.</p>
<p><img alt="page_rank" src="../img/page_rank.png" /></p>
<p>This bubble diagram represents a model mini Internet, where each bubble is a webpage and each arrow from A, B, C, and D represents a link on that webpage which takes you to one of the others. We're trying to build an expression that tells us, based on this network structure, which of these webpages is most relevant to the person who made the search. As such, we're going to use the concept of <em>Procrastinating Pat</em> who is an imaginary person who goes on the Internet and just randomly click links to avoid doing their work. By mapping all the possible links, we can build a model to estimate the amount of time we would expect Pat to spend on each webpage. We can describe the links present on page A as a vector, where each row is either a one or a zero based on whether there is a link to the corresponding page. And then normalise the vector by the total number of the links, such that they can be used to describe a probability for that page.</p>
<p>For example, the vector of links from page A will be</p>
<p>
<script type="math/tex; mode=display">\begin{bmatrix} 0 & 1 & 1 & 1\end{bmatrix}</script>
</p>
<p>because vector A has links to sites B, to C, and to D, but it doesn't have a link to itself. Also, because there are three links in this page in total, we would normalize by a factor of a third. So the total click probability sums to one. So we can write,</p>
<p>
<script type="math/tex; mode=display">L_A =  \begin{bmatrix}0 & \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\end{bmatrix}</script>
</p>
<p>Following the same logic, the link vectors in the next two sites are shown here:</p>
<p>
<script type="math/tex; mode=display">L_B =  \begin{bmatrix} \frac{1}{2} & 0 & 0 &  \frac{1}{2}\end{bmatrix}</script>
<script type="math/tex; mode=display">L_C =  \begin{bmatrix}0 & 0 & 0 & 1\end{bmatrix}</script>
</p>
<p>and finally, for page D, we can write</p>
<p>
<script type="math/tex; mode=display">L_D =  \begin{bmatrix} 0 & \frac{1}{2} & \frac{1}{2} & 0 \end{bmatrix}</script>
</p>
<p>We can now build our link matrix \(L\) by using each of our linked vectors as a column, which you can see will form a square matrix.</p>
<p>
<script type="math/tex; mode=display">L =  \begin{bmatrix} 0 & \frac{1}{2} & 0 & 0 \\\ \frac{1}{3} & 0 & 0 & \frac{1}{2} \\\ \frac{1}{3} & 0 & 0 & \frac{1}{2}  \\\ \frac{1}{3} & \frac{1}{2} & 1 & 0 \end{bmatrix}</script>
</p>
<p>What we're trying to represent here with our matrix \(L\) is the probability of ending up on each of the pages. For example, the only way to get to A is by being at B. So you then need to know the probability of being at B, which you could've got to from either A or D. As you can see, this problem is self-referential, as the ranks on all the pages depend on all the others. Although we built our matrix from columns of outward links, we can see that the rows describe inward links normalized with respect to their page of origin.</p>
<p>We can now write an expression which summarises the approach. We're going to use the vector <script type="math/tex">r</script> to store the rank of all webpages. To calculate the rank of page A, you need to know three things about all other pages on the Internet.</p>
<ol>
<li>What's your rank?</li>
<li>Do you link to page A?</li>
<li>And how many outgoing links do you have in total?</li>
</ol>
<p>The following expression combines these three pieces of information for webpage A only.</p>
<p>
<script type="math/tex; mode=display">r_a = \sum_{j=1}^n L_{a, j}r_j</script>
</p>
<p>So this is going to scroll through each of our webpages. Which means that the rank of A is the sum of the ranks of all the pages which link to it, weighted by their specific link probability taken from matrix \(L\). Now we want to be able to write this expression for all pages and solve them simultaneously. Thinking back to our linear algebra, we can rewrite the above expression applied to all webpages as a simple matrix multiplication. So</p>
<p>
<script type="math/tex; mode=display">r = Lr</script>
</p>
<p>Clearly, we start off not knowing \(r\). So we simply assume that all the ranks are equally and normalise them by the total number of webpages in our analysis, which in this case is 4. So</p>
<p>
<script type="math/tex; mode=display">r = \begin{bmatrix}\frac{1}{4} \\ \frac{1}{4} \\ \frac{1}{4} \\ \frac{1}{4}\end{bmatrix}</script>
</p>
<p>Then, each time you multiply \(r\) by our matrix \(L\), this gives us an updated value for \(r\). So we can say that</p>
<p>
<script type="math/tex; mode=display">r^{i+1} = Lr^i</script>
</p>
<p>Applying this expression repeatedly means that we are solving this problem iteratively. Each time we do this, we update the values in \(r\) until, eventually, \(r\) stops changing. So now \(r\) really does equal \(Lr\). Thinking back to the previous videos in this module, this implies that \(r\) is now an eigenvector of matrix \(L\), with an eigenvalue of 1. At this point, you might well be thinking, if we want to multiply \(r\) by \(L\) many times, perhaps this will be best tackled by applying the diagonalization method that we saw in the last video. But don't forget, this would require us to already know all of the Eigen vectors, which is what we're trying to find in the first place. So now that we have an equation, and hopefully some idea of where it came from, we can ask our computer to iteratively apply it until it converges to find our rank vector.</p>
<p><img alt="page_rank" src="../img/page_rank.gif" /></p>
<p>You can see that although it takes about ten iterations for the numbers to settle down, the order is already established after the first iteration. However, this is just an artifact of our system being so tiny. So now we have our result, which says that as Procrastinating Pat randomly clicks around our network, we'd expect them to spend about 40% of their time on page D. But only about 12% of their time on page A, with 24% on each of pages B and C. We now have our ranking, with D at the top and A at the bottom, and B and C equal in the middle.</p>
<p><img alt="page_rank_2" src="../img/page_rank_2.gif" /></p>
<p>As it turns out, although there are many approaches for efficiently calculating eigenvectors that have been developed over the years, repeatedly multiplying a randomly selected initial guest vector by a matrix, which is called the <strong>power method</strong>, is still very effective for the page rank problem for two reasons. Firstly, although the power method will clearly only give you one eigenvector, when we know that there will be \(n\) for an \(n\) webpage system, it turns out that because of the way we've structured our link matrix, the vector it gives you will always be the one that you're looking for, with an eigenvalue of 1. Secondly, although this is not true for the full webpage mini Internet, when looking at the real Internet you can imagine that almost every entry in the link matrix will be zero, i.e,, most pages don't connect to most other pages. This is referred to as a <strong>sparse matrix</strong>. And algorithms exist such that multiplications can be performed very efficiently.</p>
<p>One key aspect of the page rank algorithm that we haven't discussed so far is called the damping factor, \(d\). This adds an additional term to our iterative formula. So \(r^{i + 1}\) is now going to equal</p>
<p>
<script type="math/tex; mode=display">r^{i + 1}= d Lr^i + \frac{1 - d}{n}</script>
</p>
<p>where \(d\) is something between 0 and 1. And you can think of it as 1 minus the probability with which procrastinating Pat suddenly, randomly types in a web address, rather than clicking on a link on his current page. The effect that this has on the actual calculation is about finding a compromise between speed and stability of the iterative convergence process. There are over one billion websites on the Internet today, compared with just a few million when the page rank algorithm was first published in 1998. And so the methods for search and ranking have had to evolve to maximize efficiency, although the core concept has remained unchanged for many years.</p>
<p><strong>Conclusions</strong></p>
<p>This brings us to the end of our introduction to the page rank algorithm. There are, of course, many details which we didn't cover in this video. But I hope this has allowed you to come away with some insight and understanding into how the page rank works, and hopefully the confidence to apply this to some larger networks yourself.</p>
<h3 id="summary_1">Summary</h3>
<p>This brings us to the end of the fifth module and also, to the end of this course on linear algebra for machine learning.</p>
<p>We've covered a lot of ground in the past five modules, but I hope that we've managed to balance, the speed with the level of detail to ensure that you've stayed with us throughout.</p>
<p>There is a tension at the heart of mathematics teaching in the computer age. Classical teaching approaches focused around working through lots of examples by hand without much emphasis on building intuition. However, computers now do nearly all of the calculation work for us, and it's not typical for the methods appropriate to hand calculation to be the same as those employed by a computer. This can mean that, despite doing lots of work, students can come away from a classical education missing both the detailed view of the computational methods, but also the high level view of what each method is really doing. The concepts that you've been exposed to over the last five modules cover the core of linear algebra. That you will need as you progress your study of machine learning. And we hope that at the very least, when you get stuck in the future, you'll know the appropriate language. So that you can quickly look up some help when you need it. Which, after all, is the most important skill of a professional coder.</p>
<h3 id="wrap-up">Wrap up</h3>
<p>Hello again. So that's it for this course of linear algebra in our little specialization on mathematics for machine learning. Our goal here was to give you some of the underpinnings of linear algebra, vectors and matrices. In order to enable to access neural networks, machine learning and data science courses more generally. By no means is this a course that a computer scientist or mathematician will be completely happy with as setting out the foundations required for more advanced work in linear algebra in a graduate course.</p>
<p>Nor is it a comprehensive course on linear algebra. We simply haven't had the time. So there are many things left out that in my own discipline say, Material Science, I would consider quite fundamental. But we think it's more than enough for a social scientist, an engineer or a physicist to develop the understanding and insight required to access other courses on machine learning tools. In order to do useful work to solve problems with machine learning in the real world.</p>
<p>But still we've done a lot of work here. We started out thinking about data and the sort of problems we might have. Primarily, optimization problems like fitting parameters that model data, the distribution of height and simultaneous equations like the apples and bananas price discovery problem.</p>
<p>We've then use those to go on a journey exploring vector properties like linear combination, vector additions and scale multiplications. We've then gone on to find the modulus of a vector and the dot product and related ideas of the scalar and vector projections.</p>
<p>That's let us get in to basis changes of vector transformations, which meant we had to define what we meant by a basis, by linear independence and the dimensionality of a vector space.</p>
<p>We then got into matrices and how to do matrix operations and how to solve systems of simultaneous equations using algorithms.</p>
<p>We then looked at basis transformations again using matrices, and looked at how to construct a basis using the Gram-Schmidt approach. Sam then took us through eigenvectors and eigenvalues, how to find them and then took us through how to construct Google's famous page rank algorithm. And through it all, you hopefully enjoyed using this knowledge and developing your understanding by trying the problems and exercises. Which we hope has given you the confidence to say that you're really getting now and developing an intuitive understanding about all this stuff.</p>
<p>So Sam and I and all of us on the team here at Imperial really hope you've enjoyed this course and it's been valuable to you. Of course, this course is intended to link very closely to its companions on multivariate calculus and on machine learning. So we hope you'll check those out as well. And of course whether you do that or not, we wish you the very best of the future, whatever your future endeavor.</p>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href=".." title="About" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                About
              </span>
            </div>
          </a>
        
        
          <a href="../calculus/" title="Calculus" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Calculus
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
        
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.583bbe55.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:".."}})</script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
    
      
    
  </body>
</html>