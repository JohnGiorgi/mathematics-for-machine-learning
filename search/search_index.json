{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A single resource, in the form of a simple website built with GitHub Pages, which will hopefully serve as a complete crash course on the various areas of mathematics essential to understanding machine learning. To do this, I will use the Mathematics for Machine Learning Specialization on Coursera as my guide, but also pull from other resources, such as Khan Academy and 3Blue1Brown 's various video playlists on Youtube. The end goal will be to produce a simple site that someone (including myself) could use to quickly bring themselves up to speed on the fundamental mathematical concepts necessary for machine learning. The target audience are those who have at least some highschool math, but who should really have taken introductory courses on Linear Algebra and Calculus in college. The website can be accessed here . Full credit to the team behind the Mathematics for Machine Learning Specialization course on Coursera for creating such an awesome resource. I highly encourage anyone who needs to brush up on their mathematics for machine learning to check that course out. Notebooks Notebooks contains Jupyter notebooks for each course in the specialization , which each contain python implementations for many of the discussed concepts. Attribution Just like the Mathematics for Machine Learning Specialization , This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License .","title":"About"},{"location":"#notebooks","text":"Notebooks contains Jupyter notebooks for each course in the specialization , which each contain python implementations for many of the discussed concepts.","title":"Notebooks"},{"location":"#attribution","text":"Just like the Mathematics for Machine Learning Specialization , This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License .","title":"Attribution"},{"location":"linear_algebra/course_resources/","text":"Course resources There are lots of useful web resources on linear algebra . Typically they go a bit slower or have a different emphasis or way of explaining things, but it can be handy to see how someone else explains something. Khan Academy is a great resource right up to 1st or 2nd year undergraduate material. For this course, there's a handy group of videos here . Grant Sanderson has a great series of videos developing mathematical intuition on YouTube, which you can reach through his site here . Wikipedia gets better every year - and the linear algebra wikipedia pages are actually pretty good.","title":"Course Resources"},{"location":"linear_algebra/course_resources/#course-resources","text":"There are lots of useful web resources on linear algebra . Typically they go a bit slower or have a different emphasis or way of explaining things, but it can be handy to see how someone else explains something. Khan Academy is a great resource right up to 1st or 2nd year undergraduate material. For this course, there's a handy group of videos here . Grant Sanderson has a great series of videos developing mathematical intuition on YouTube, which you can reach through his site here . Wikipedia gets better every year - and the linear algebra wikipedia pages are actually pretty good.","title":"Course resources"},{"location":"linear_algebra/week_1/","text":"Week 1: Introduction to Linear Algebra In this first module we look at how linear algebra is relevant to machine learning and data science. Then we'll wind up the module with an initial introduction to vectors. Throughout, we're focussing on developing your mathematical intuition, not of crunching through algebra or doing long pen-and-paper examples. For many of these operations, there are callable functions in Python that can do the math - the point is to appreciate what they do and how they work, so that when things go wrong or there are special cases, you can understand why and what to do. Learning Objectives Recall how machine learning and vectors and matrices are related Interpret how changes in the model parameters affect the quality of the fit to the training data Recognize that variations in the model parameters are vectors on the response surface - that vectors are a generic concept not limited to a physical real space Use substitution / elimination to solve a fairly easy linear algebra problem Understand how to add vectors and multiply by a scalar number The relationship between machine learning, linear algebra, vectors and matrices Motivations for linear algebra Lets take a look at the types of problems we might want to solve, in order to expose what linear algebra is and how it might help us to solve them. Toy problem 1 The first problem we might think of is price discovery . We can illustrate this problem with a toy example. Say we go shopping on two occasions, and the first time we buy two apples and three bananas and they cost eight Euros 2a + 3b = 8 2a + 3b = 8 and the second time we buy ten apples and one banana, for a cost of 13 Euros. 10a + 1b = 13 10a + 1b = 13 The a a 's and b b 's here, are the price of a single apple and a single banana. What we're going to have to do is solve these simultaneous equations in order to discover the price of individual apples and bananas . Now in the general case, with lots of different types of items and lots of shopping trips, finding out the prices might be quite hard . This is an example of a linear algebra problem. I have some constant linear coefficients here (2, 10, 3, 1), that relate the input variables , a a and b b , to the outputs 8 and 13. That is if, we think about a vector [a,b] [a,b] that describes the prices of apples and bananas, we can write this down as a matrix problem where the 2, 3 is my first trip, and the 10, 1 is my second trip, \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix} 8 \\\\\\ 3\\end{bmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix} 8 \\\\\\ 3\\end{bmatrix} What we're going to do over the course of weeks one to three, is to look at these mathematical objects, vectors and matrices , in order to understand what they are and how to work with them. Toy problem 2 Another type of problem we might be interested in solving is fitting an equation to some data . In fact, with neural networks and machine learning, we want the computer to, in effect, not only fit the equation to the data but to figure out what equation to use . Let's say, we have some data like this histogram here: This looks like a population with an average and some variation . A common problem we might want to solve is how to find the optimal value of the parameters in the equation describing this line, i.e., the ones that fit the data in the histogram best. That might be really handy, because with that fitted equation we'd have an easy \"portable\" description of the population we could carry around, without needing all the original data which would free us, for example, from privacy concerns. Conclusions In this video, we've set up two problems that can be solved with linear algebra. First, the problem of solving simultaneous equations . And secondly, the optimization problem of fitting and equation with some parameters to data. These problems and others will motivate our work right through the course on linear algebra, and it's partner course on multivariate calculus . Geometric and Numeric Interpretations It is helpful to draw a distinction from the numerical operations we can perform using linear algebra, and the geometric intuitions underlying them (which are frequently not taught in may introductory courses). Roughly speaking, the geometric understanding or intuition is what lets us judge what tools to use to solve specific problems, feel why they work, and know how to interpret the results. The numerical understanding is what lets us actually carry through the application of those tools. If you learn linear algebra without getting a solid foundation in that geometric understanding, the problems can go unnoticed for a while, until you go deeper into whatever field you happen to pursue (e.g. computer science, engineering, statistics, economics, etc.), at which point you may feel disheartened by your lack of understanding of the fundamentals of linear algebra. With linear algebra (much like trigonometry, for example), there are a handful of useful visual/geometric intuitions underlying much of the subject. When you digest these and really understand the relationship between the geometry and the numbers, the details of the subject as well as how it's used in practice start to feel a lot more reasonable. Note Full credit for this section goes to 3Blue1Brown . Video here . Vectors The first thing we need to do in this course on linear algebra is to get a handle on vectors , which will turn out to be really useful to us in solving the linear algebra problems we introduced earlier (along with many more!). That is, problems described by equations which are linear in their coefficients , such as most fitting parameters. Tip This section maps most closely the the set of Khan Academy courses here . Take these for more practice. Getting a handle on vectors We're going to first step back and look in some detail at the sort of things we're trying to do with data. And why those vectors you first learned about in high school were even relevant. This will hopefully make all the work with vectors later on in the course a lot more intuitive. Tip This is not a great introduction to vectors (IMO). I recommend you first watch this video, then come back and read this section. Let's go back to that simpler problem from the last video, the histogram distribution of heights of people in the population: Say we wanted to try fitting that distribution with an equation describing the variation of height in the population. It turns our that such an equation has just two parameters; one describing the center of the distribution (the average ), which we'll call \\mu \\mu , and one describing how wide it is (or the variance ), which we'll call \\sigma \\sigma . This equation turns out to be the equation for the normal or ( Gaussian ) distribution : f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}} f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}} So how do we arrive at the best possible values for \\mu \\mu and \\sigma \\sigma ? Well, one way is gradient descent . If we think of some goodness value which measures how well our parameters fit our data (say, the mean squared error ) we could imagine plotting this goodness value as a function of our parameters, often called a cost or loss function. The closer our loss function is to zero, the better our parameters fit our data. Gradient descent allows us to choose values for our parameters that minimize the error , as measured by our loss function, by taking small incremental steps towards the bottom of the parameter space defined by our loss function. Note Gradient descent on a 3D surface. This process involves computing the partial derivative of our loss function w.r.t w.r.t to all possible parameters (also known as the gradient ). If our parameters are stored in a vector, \\begin{bmatrix}\\mu&\\sigma \\end{bmatrix} \\begin{bmatrix}\\mu&\\sigma \\end{bmatrix} we could subtract from this vector the vector of gradients, \\begin{bmatrix}\\frac{\\partial f}{\\partial \\mu} & \\frac{\\partial f}{\\partial \\sigma}\\end{bmatrix} \\begin{bmatrix}\\frac{\\partial f}{\\partial \\mu} & \\frac{\\partial f}{\\partial \\sigma}\\end{bmatrix} in order to complete the computation in (effectively) one step. So vectors (and calculus) give us a computational means of navigating a parameter space, in this case by determining the set of parameters for a function f(x) f(x) which best explain the data. Vectors as abstract lists of numbers We can also think of vectors as simply lists of numbers . For example, we could describe a car in terms of its price, top speed, safety rating, emissions performance, etc. and store these numbers in a single vector . car = \\begin{bmatrix}\\text{price,} & \\text{top speed,} & \\text{safety rating, }& ...\\end{bmatrix} car = \\begin{bmatrix}\\text{price,} & \\text{top speed,} & \\text{safety rating, }& ...\\end{bmatrix} Note This is more of a 'computer science' perspective of vectors. To summarize, a vector is, at the simplest level: lists of numbers something which moves in a space of parameters Operations with vectors Lets now explore the operations we can do with vectors, how these mathematical operations define what vectors are in the first place, and the sort of spaces they can apply to. We can think of a vector as an object that moves us about space. This could be a physical space, or a space of data (often called a vector space ). Note At school, you probably thought of a vector as something that moved you around a physical space, but in computer and data science, we generalize that idea to think of a vector as just a list of attributes of an objects. More formally, mathematics generalizes the definition of a vector to be an object for which the following two operations are defined : addition multiplication by a scalar Tip This is really important, so make sure you understand it. If a mathematical object can be added to another object of the same type, and it can be scaled (i.e. multiplied by a scaler), then its a vector! Vector addition Intuitively, we can introduce vector addition as being the resulting vector of the two vectors we want to add ( s s and r r ) being placed head-to-tail , s + r s + r . Multiplication by a scalar Multiplying a vector by a scalar is also easy to understand. In this case, we simply multiply all elements of our vector r r by the scalar, a a for example. Coordinate systems At this point, it's convenient to define a coordinate system . Imagine we had two dimensions defined by the vectors: \\hat i = \\begin{bmatrix}1 \\\\\\ 0 \\end{bmatrix} \\; \\hat j = \\begin{bmatrix} 1 \\\\\\ 0 \\end{bmatrix} \\hat i = \\begin{bmatrix}1 \\\\\\ 0 \\end{bmatrix} \\; \\hat j = \\begin{bmatrix} 1 \\\\\\ 0 \\end{bmatrix} Note These are known as basis vectors , and they define the basis . We could define any vector in this 2D space using the vectors \\hat i \\hat i and \\hat j \\hat j . For example, the vector \\begin{bmatrix}3 \\\\\\ 2 \\end{bmatrix} = 3 \\hat i + 2 \\hat j \\begin{bmatrix}3 \\\\\\ 2 \\end{bmatrix} = 3 \\hat i + 2 \\hat j Note This is also a extremely important point. A vector space is itself defined by vectors . We will explore this further later in the course. This also nicely illustrates that vectors are associative , meaning, the sum of a series of vectors is the same regardless of the order we add them in, e.g., \\begin{bmatrix} 3 \\\\\\ 2 \\end{bmatrix} = 3 \\hat i + 2 \\hat{j} = 2 \\hat{j} + 3 \\hat i \\begin{bmatrix} 3 \\\\\\ 2 \\end{bmatrix} = 3 \\hat i + 2 \\hat{j} = 2 \\hat{j} + 3 \\hat i Conclusions We've defined two fundamental operations that vectors satisfy: addition , (e.g. r + s r + s ), and multiplication by a scalar , (e.g. 2r 2r ). We've noted that it can be useful to define a coordinate system in which to do our addition and scaling , e.g., r = \\begin{bmatrix}3 \\\\\\ 2 \\end{bmatrix} = 3 \\hat i + 2 \\hat{j} r = \\begin{bmatrix}3 \\\\\\ 2 \\end{bmatrix} = 3 \\hat i + 2 \\hat{j} using these fundamental basis vectors, \\hat i \\hat i and \\hat{j} \\hat{j} , and explored the properties that this implies, like associativity of addition and subtraction. We've also seen that although, perhaps, it's easiest to think of vector operations geometrically , we don't have to do it in a real (number) space. We can also define vector operations on vectors that list different types of objects, like the attributes of a house .","title":"Week 1"},{"location":"linear_algebra/week_1/#week-1-introduction-to-linear-algebra","text":"In this first module we look at how linear algebra is relevant to machine learning and data science. Then we'll wind up the module with an initial introduction to vectors. Throughout, we're focussing on developing your mathematical intuition, not of crunching through algebra or doing long pen-and-paper examples. For many of these operations, there are callable functions in Python that can do the math - the point is to appreciate what they do and how they work, so that when things go wrong or there are special cases, you can understand why and what to do. Learning Objectives Recall how machine learning and vectors and matrices are related Interpret how changes in the model parameters affect the quality of the fit to the training data Recognize that variations in the model parameters are vectors on the response surface - that vectors are a generic concept not limited to a physical real space Use substitution / elimination to solve a fairly easy linear algebra problem Understand how to add vectors and multiply by a scalar number","title":"Week 1: Introduction to Linear Algebra"},{"location":"linear_algebra/week_1/#the-relationship-between-machine-learning-linear-algebra-vectors-and-matrices","text":"","title":"The relationship between machine learning, linear algebra, vectors and matrices"},{"location":"linear_algebra/week_1/#motivations-for-linear-algebra","text":"Lets take a look at the types of problems we might want to solve, in order to expose what linear algebra is and how it might help us to solve them. Toy problem 1 The first problem we might think of is price discovery . We can illustrate this problem with a toy example. Say we go shopping on two occasions, and the first time we buy two apples and three bananas and they cost eight Euros 2a + 3b = 8 2a + 3b = 8 and the second time we buy ten apples and one banana, for a cost of 13 Euros. 10a + 1b = 13 10a + 1b = 13 The a a 's and b b 's here, are the price of a single apple and a single banana. What we're going to have to do is solve these simultaneous equations in order to discover the price of individual apples and bananas . Now in the general case, with lots of different types of items and lots of shopping trips, finding out the prices might be quite hard . This is an example of a linear algebra problem. I have some constant linear coefficients here (2, 10, 3, 1), that relate the input variables , a a and b b , to the outputs 8 and 13. That is if, we think about a vector [a,b] [a,b] that describes the prices of apples and bananas, we can write this down as a matrix problem where the 2, 3 is my first trip, and the 10, 1 is my second trip, \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix} 8 \\\\\\ 3\\end{bmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix} 8 \\\\\\ 3\\end{bmatrix} What we're going to do over the course of weeks one to three, is to look at these mathematical objects, vectors and matrices , in order to understand what they are and how to work with them. Toy problem 2 Another type of problem we might be interested in solving is fitting an equation to some data . In fact, with neural networks and machine learning, we want the computer to, in effect, not only fit the equation to the data but to figure out what equation to use . Let's say, we have some data like this histogram here: This looks like a population with an average and some variation . A common problem we might want to solve is how to find the optimal value of the parameters in the equation describing this line, i.e., the ones that fit the data in the histogram best. That might be really handy, because with that fitted equation we'd have an easy \"portable\" description of the population we could carry around, without needing all the original data which would free us, for example, from privacy concerns.","title":"Motivations for linear algebra"},{"location":"linear_algebra/week_1/#conclusions","text":"In this video, we've set up two problems that can be solved with linear algebra. First, the problem of solving simultaneous equations . And secondly, the optimization problem of fitting and equation with some parameters to data. These problems and others will motivate our work right through the course on linear algebra, and it's partner course on multivariate calculus .","title":"Conclusions"},{"location":"linear_algebra/week_1/#geometric-and-numeric-interpretations","text":"It is helpful to draw a distinction from the numerical operations we can perform using linear algebra, and the geometric intuitions underlying them (which are frequently not taught in may introductory courses). Roughly speaking, the geometric understanding or intuition is what lets us judge what tools to use to solve specific problems, feel why they work, and know how to interpret the results. The numerical understanding is what lets us actually carry through the application of those tools. If you learn linear algebra without getting a solid foundation in that geometric understanding, the problems can go unnoticed for a while, until you go deeper into whatever field you happen to pursue (e.g. computer science, engineering, statistics, economics, etc.), at which point you may feel disheartened by your lack of understanding of the fundamentals of linear algebra. With linear algebra (much like trigonometry, for example), there are a handful of useful visual/geometric intuitions underlying much of the subject. When you digest these and really understand the relationship between the geometry and the numbers, the details of the subject as well as how it's used in practice start to feel a lot more reasonable. Note Full credit for this section goes to 3Blue1Brown . Video here .","title":"Geometric and Numeric Interpretations"},{"location":"linear_algebra/week_1/#vectors","text":"The first thing we need to do in this course on linear algebra is to get a handle on vectors , which will turn out to be really useful to us in solving the linear algebra problems we introduced earlier (along with many more!). That is, problems described by equations which are linear in their coefficients , such as most fitting parameters. Tip This section maps most closely the the set of Khan Academy courses here . Take these for more practice.","title":"Vectors"},{"location":"linear_algebra/week_1/#getting-a-handle-on-vectors","text":"We're going to first step back and look in some detail at the sort of things we're trying to do with data. And why those vectors you first learned about in high school were even relevant. This will hopefully make all the work with vectors later on in the course a lot more intuitive. Tip This is not a great introduction to vectors (IMO). I recommend you first watch this video, then come back and read this section. Let's go back to that simpler problem from the last video, the histogram distribution of heights of people in the population: Say we wanted to try fitting that distribution with an equation describing the variation of height in the population. It turns our that such an equation has just two parameters; one describing the center of the distribution (the average ), which we'll call \\mu \\mu , and one describing how wide it is (or the variance ), which we'll call \\sigma \\sigma . This equation turns out to be the equation for the normal or ( Gaussian ) distribution : f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}} f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}} So how do we arrive at the best possible values for \\mu \\mu and \\sigma \\sigma ? Well, one way is gradient descent . If we think of some goodness value which measures how well our parameters fit our data (say, the mean squared error ) we could imagine plotting this goodness value as a function of our parameters, often called a cost or loss function. The closer our loss function is to zero, the better our parameters fit our data. Gradient descent allows us to choose values for our parameters that minimize the error , as measured by our loss function, by taking small incremental steps towards the bottom of the parameter space defined by our loss function. Note Gradient descent on a 3D surface. This process involves computing the partial derivative of our loss function w.r.t w.r.t to all possible parameters (also known as the gradient ). If our parameters are stored in a vector, \\begin{bmatrix}\\mu&\\sigma \\end{bmatrix} \\begin{bmatrix}\\mu&\\sigma \\end{bmatrix} we could subtract from this vector the vector of gradients, \\begin{bmatrix}\\frac{\\partial f}{\\partial \\mu} & \\frac{\\partial f}{\\partial \\sigma}\\end{bmatrix} \\begin{bmatrix}\\frac{\\partial f}{\\partial \\mu} & \\frac{\\partial f}{\\partial \\sigma}\\end{bmatrix} in order to complete the computation in (effectively) one step. So vectors (and calculus) give us a computational means of navigating a parameter space, in this case by determining the set of parameters for a function f(x) f(x) which best explain the data. Vectors as abstract lists of numbers We can also think of vectors as simply lists of numbers . For example, we could describe a car in terms of its price, top speed, safety rating, emissions performance, etc. and store these numbers in a single vector . car = \\begin{bmatrix}\\text{price,} & \\text{top speed,} & \\text{safety rating, }& ...\\end{bmatrix} car = \\begin{bmatrix}\\text{price,} & \\text{top speed,} & \\text{safety rating, }& ...\\end{bmatrix} Note This is more of a 'computer science' perspective of vectors. To summarize, a vector is, at the simplest level: lists of numbers something which moves in a space of parameters","title":"Getting a handle on vectors"},{"location":"linear_algebra/week_1/#operations-with-vectors","text":"Lets now explore the operations we can do with vectors, how these mathematical operations define what vectors are in the first place, and the sort of spaces they can apply to. We can think of a vector as an object that moves us about space. This could be a physical space, or a space of data (often called a vector space ). Note At school, you probably thought of a vector as something that moved you around a physical space, but in computer and data science, we generalize that idea to think of a vector as just a list of attributes of an objects. More formally, mathematics generalizes the definition of a vector to be an object for which the following two operations are defined : addition multiplication by a scalar Tip This is really important, so make sure you understand it. If a mathematical object can be added to another object of the same type, and it can be scaled (i.e. multiplied by a scaler), then its a vector! Vector addition Intuitively, we can introduce vector addition as being the resulting vector of the two vectors we want to add ( s s and r r ) being placed head-to-tail , s + r s + r . Multiplication by a scalar Multiplying a vector by a scalar is also easy to understand. In this case, we simply multiply all elements of our vector r r by the scalar, a a for example. Coordinate systems At this point, it's convenient to define a coordinate system . Imagine we had two dimensions defined by the vectors: \\hat i = \\begin{bmatrix}1 \\\\\\ 0 \\end{bmatrix} \\; \\hat j = \\begin{bmatrix} 1 \\\\\\ 0 \\end{bmatrix} \\hat i = \\begin{bmatrix}1 \\\\\\ 0 \\end{bmatrix} \\; \\hat j = \\begin{bmatrix} 1 \\\\\\ 0 \\end{bmatrix} Note These are known as basis vectors , and they define the basis . We could define any vector in this 2D space using the vectors \\hat i \\hat i and \\hat j \\hat j . For example, the vector \\begin{bmatrix}3 \\\\\\ 2 \\end{bmatrix} = 3 \\hat i + 2 \\hat j \\begin{bmatrix}3 \\\\\\ 2 \\end{bmatrix} = 3 \\hat i + 2 \\hat j Note This is also a extremely important point. A vector space is itself defined by vectors . We will explore this further later in the course. This also nicely illustrates that vectors are associative , meaning, the sum of a series of vectors is the same regardless of the order we add them in, e.g., \\begin{bmatrix} 3 \\\\\\ 2 \\end{bmatrix} = 3 \\hat i + 2 \\hat{j} = 2 \\hat{j} + 3 \\hat i \\begin{bmatrix} 3 \\\\\\ 2 \\end{bmatrix} = 3 \\hat i + 2 \\hat{j} = 2 \\hat{j} + 3 \\hat i Conclusions We've defined two fundamental operations that vectors satisfy: addition , (e.g. r + s r + s ), and multiplication by a scalar , (e.g. 2r 2r ). We've noted that it can be useful to define a coordinate system in which to do our addition and scaling , e.g., r = \\begin{bmatrix}3 \\\\\\ 2 \\end{bmatrix} = 3 \\hat i + 2 \\hat{j} r = \\begin{bmatrix}3 \\\\\\ 2 \\end{bmatrix} = 3 \\hat i + 2 \\hat{j} using these fundamental basis vectors, \\hat i \\hat i and \\hat{j} \\hat{j} , and explored the properties that this implies, like associativity of addition and subtraction. We've also seen that although, perhaps, it's easiest to think of vector operations geometrically , we don't have to do it in a real (number) space. We can also define vector operations on vectors that list different types of objects, like the attributes of a house .","title":"Operations with vectors"},{"location":"linear_algebra/week_2/","text":"Week 2: Vectors are Objects that Move Around Space In this module, we will look at the types operations we can do with vectors - finding the modulus or magnitude (size), finding the angle between vectors (dot or inner product) and projecting one vector onto another. We will then examine how the entries describing a vector will depend on what vectors we use to define the axes - the basis. That will then let us determine whether a proposed set of basis vectors are linearly independent . This will complete our examination of vectors, allowing us to move on to matrices and then to begin solving linear algebra problems. Learning Objectives Calculate basic operations (dot product, modulus, negation) on vectors Calculate a change of basis Recall linear independence Identify a linearly independent basis and relate this to the dimensionality of the space Finding the size of a vector, its angle, and projection Tip It is probably worth it to watch this 3Blue1Brown video first before reading through this section. However, be warned, it is the most confusing one in the series. If you want even more practice, check out this Khan Academy track. Modulus & inner product Previously we looked at the two main vector operations of addition and scaling by a number (multiplication by a scalar ). As it turns out, those are really the only operations we need to be able to do in order define something as a vector. Now, we can move on to define two new ideas: the length of a vector , also called its size , and the dot product of a vector, also called its inner , scalar or projection product. Note The dot product is a huge and amazing concept in linear algebra with a huge number of implications. We'll only be able to touch on a few parts of it here, but enjoy. It's one of the most beautiful parts of linear algebra. Length of a vector Lets define a vector r r using the basis vectors we introduced earlier, i i and j j , r = a i + b j = \\begin{bmatrix} a \\\\\\ b \\end{bmatrix} r = a i + b j = \\begin{bmatrix} a \\\\\\ b \\end{bmatrix} To calculate the length of r r , also called the norm \\vert r\\vert \\vert r\\vert (or \\Vert r\\Vert \\Vert r\\Vert ), we could imagine drawing a triangle, with our vector r r as the hypotenuse: Note The length, magnitude, modulus and norm of a vector are all the same thing, and just represent a difference in terminology. If we are thinking of a vector as representing the line segment from the origin to a given point (i.e., the geometric interpretation), we may interpret the norm as the length of this line segment. If we are thinking of a vector as representing a physical quantity like acceleration or velocity, we may interpret the norm as the magnitude of this quantity (how \" large \" it is, regardless of its direction). By Pythagorus's Theorem , \\vert r \\vert = \\sqrt{a^2 + b^2} \\vert r \\vert = \\sqrt{a^2 + b^2} Vector dot product The dot product is one of several ways of multiplying two vectors together, specifically, it is an algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors) and returns a single number . The dot product has an algebraic and geometric interpretation. Algebraically , the dot product is the sum of the products of the corresponding entries of the two sequences of numbers . Geometrically , it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them . Algebraic definition of the dot product To illustrate the algebraic definition of the dot product, lets define two vectors r r and s s : r = \\begin{bmatrix}r_i \\\\ r_j\\end{bmatrix}, s = \\begin{bmatrix}s_i \\\\ s_j\\end{bmatrix} r = \\begin{bmatrix}r_i \\\\ r_j\\end{bmatrix}, s = \\begin{bmatrix}s_i \\\\ s_j\\end{bmatrix} The dot product is then: r \\cdot s = r_is_i + r_j s_j = (3)(-1) + (2)(2) = 1 r \\cdot s = r_is_i + r_j s_j = (3)(-1) + (2)(2) = 1 More formally, the algebraic definition of the dot product is: r \\cdot s = \\sum^n_{i=1}a_ib_i = a_1b_1 + a_2b_2 + ... + a_nb_n r \\cdot s = \\sum^n_{i=1}a_ib_i = a_1b_1 + a_2b_2 + ... + a_nb_n Note The algebraic definition of the dot product is simply the sum of the products obtained by multiplying each component from both vectors. Properties of the dot product The dot product is, commutative , e.g., r \\cdot s = s \\cdot r r \\cdot s = s \\cdot r distributive , e.g., r \\cdot (s + {t}) = r \\cdot s + r \\cdot {t} r \\cdot (s + {t}) = r \\cdot s + r \\cdot {t} associative over scalar multiplication, e.g., r \\cdot (a s) = a ( r \\cdot s) r \\cdot (a s) = a ( r \\cdot s) Lets prove the distributive property in the general case. Let: r = \\begin{bmatrix} r_1 \\\\\\ r_2 \\\\\\ . \\\\\\ . \\\\\\ . \\\\\\ r_n \\end{bmatrix}, \\; s = \\begin{bmatrix} s_1 \\\\\\ s_2 \\\\\\ . \\\\\\ . \\\\\\ . \\\\\\ s_n \\end{bmatrix}, \\; {t} = \\begin{bmatrix} t_1 \\\\\\ t_2 \\\\\\ . \\\\\\ . \\\\\\ . \\\\\\ t_n \\end{bmatrix} r = \\begin{bmatrix} r_1 \\\\\\ r_2 \\\\\\ . \\\\\\ . \\\\\\ . \\\\\\ r_n \\end{bmatrix}, \\; s = \\begin{bmatrix} s_1 \\\\\\ s_2 \\\\\\ . \\\\\\ . \\\\\\ . \\\\\\ s_n \\end{bmatrix}, \\; {t} = \\begin{bmatrix} t_1 \\\\\\ t_2 \\\\\\ . \\\\\\ . \\\\\\ . \\\\\\ t_n \\end{bmatrix} then, r \\cdot (s + {t}) = r_1 (s_1 + t_1) + r_2 (s_2 + t_2) + ... + r_n (s_n + t_n) r \\cdot (s + {t}) = r_1 (s_1 + t_1) + r_2 (s_2 + t_2) + ... + r_n (s_n + t_n) = r_1 s_1 + r_1t_1 + r_2s_2 + r_2t_2 + ... + r_ns_n + r_nt_n = r_1 s_1 + r_1t_1 + r_2s_2 + r_2t_2 + ... + r_ns_n + r_nt_n = r \\cdot s + r \\cdot {t} = r \\cdot s + r \\cdot {t} Note Proofs for the remaining properties are left as an exercise. Link between the dot product and the size of the vector If we take r r and dot it with itself, we get: r_i r_i + r_j r_j + ... + r_nr_n = r_i^2 + r_j^2 + ... + r_n^2 = \\vert r \\vert^2 r_i r_i + r_j r_j + ... + r_nr_n = r_i^2 + r_j^2 + ... + r_n^2 = \\vert r \\vert^2 So, the size of the vector is just given by r r dotted with itself and squared. Cosine & dot product Lets take the time to derive the geometric definition of the dot product. Note Recall, geometrically , the dot product is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them . We start with the law of cosines (also known as the cosine formula or cosine rule ) from algebra, which you'll remember, probably vaguely, from school. The law of cosines states that if we had a triangle with sides a a , b b , and c c , then: c^2 = a^2 + b^2 - 2ab \\cos \\theta c^2 = a^2 + b^2 - 2ab \\cos \\theta Now, we can translate this into our vector notation: \\vert r - s\\vert ^2 = \\vert r\\vert ^2 + \\vert s\\vert ^2 - 2\\vert r\\vert \\vert s\\vert \\cos \\theta \\vert r - s\\vert ^2 = \\vert r\\vert ^2 + \\vert s\\vert ^2 - 2\\vert r\\vert \\vert s\\vert \\cos \\theta LHS \\Rightarrow (r-s) \\cdot (r-s) = r \\cdot r - s \\cdot r - s \\cdot r - s \\cdot s \\Rightarrow (r-s) \\cdot (r-s) = r \\cdot r - s \\cdot r - s \\cdot r - s \\cdot s = \\vert r\\vert ^2 - 2 s \\cdot r + \\vert s\\vert ^2 = \\vert r\\vert ^2 - 2 s \\cdot r + \\vert s\\vert ^2 Note \\vert r - s\\vert ^2 = (r-s) \\cdot (r-s) \\vert r - s\\vert ^2 = (r-s) \\cdot (r-s) comes straight from the definition of the dot product. LHS = RHS \\Rightarrow \\vert r\\vert ^2 - 2 s \\cdot r + \\vert s\\vert ^2 = \\vert r\\vert ^2 + \\vert s\\vert ^2 - 2\\vert r\\vert \\vert s\\vert \\cos \\theta \\Rightarrow \\vert r\\vert ^2 - 2 s \\cdot r + \\vert s\\vert ^2 = \\vert r\\vert ^2 + \\vert s\\vert ^2 - 2\\vert r\\vert \\vert s\\vert \\cos \\theta \\Rightarrow r \\cdot s = \\vert r\\vert \\vert s\\vert \\cos \\theta \\Rightarrow r \\cdot s = \\vert r\\vert \\vert s\\vert \\cos \\theta So what we notice is that the dot product does something quite profound . It takes the size of the two vectors ( \\vert r\\vert , \\vert s \\vert \\vert r\\vert , \\vert s \\vert ) and multiplies them by \\cos \\cos of the angle between them. It tells us something about the extent to which the two vectors go in the same direction. If \\theta \\theta is zero, then \\cos \\theta \\cos \\theta is one and r \\cdot s r \\cdot s would just be the size of the two vectors multiplied together. If \\theta \\theta is 90 90 degrees ( i.e. r r and s s are orthogonal), then \\cos 90 \\cos 90 , is 0 0 and r \\cdot s r \\cdot s is 0 0 . More generally, Note Ignore the word \"score\" here, this image was taken from a blog post about machine learning. The blog post is worth checking out though. Full credit to Christian S. Perone for the image. In this way, the dot product captures whether the two vectors are pointing in similar directions (positive) or opposite directions (negative). Projection The vector projection of a vector s s on (or onto) a nonzero vector r r (also known as the vector component or vector resolution of s s in the direction of r r ) is the orthogonal projection of s s onto a straight line parallel to r r . Tip Understanding projection can be a little tricky. If you want even more practice, check out this Khan Academy series. For the following triangle, Recall the geometric definition of the dot product: r \\cdot s = \\vert r\\vert \\vert s\\vert \\cos \\theta r \\cdot s = \\vert r\\vert \\vert s\\vert \\cos \\theta Notice that \\vert s\\vert \\cos \\theta \\vert s\\vert \\cos \\theta is the length of the adjacent side (adjacent to the angle shown). This term is the projection of the vector s s into (or onto) the vector r r . This is why the dot product is also called the projection product , because it takes the projection of one vector ( s s ) onto another ( r r ) times the magnitude or length of the other ( \\vert r \\vert \\vert r \\vert ). Note Note again that if s s was orthogonal to r r then \\vert s\\vert \\cos \\theta = \\vert s\\vert \\cos 90 = 0 = r \\cdot s \\vert s\\vert \\cos \\theta = \\vert s\\vert \\cos 90 = 0 = r \\cdot s . This provides a convenient way to check for orthogonality. Rearranging, we can compute the scalar projection of s s on r r : r \\cdot s = \\vert r\\vert\\vert s\\vert \\cos \\theta r \\cdot s = \\vert r\\vert\\vert s\\vert \\cos \\theta \\Rightarrow \\frac{ r \\cdot s}{\\vert r\\vert } = \\vert s\\vert \\cos \\theta \\Rightarrow \\frac{ r \\cdot s}{\\vert r\\vert } = \\vert s\\vert \\cos \\theta The scalar projection is a scalar , equal to the length of the orthogonal projection of s s on r r , with a negative sign if the projection has an opposite direction with respect to r r . We can also define the vector projection r \\cdot s = \\vert r\\vert\\vert s\\vert \\cos \\theta r \\cdot s = \\vert r\\vert\\vert s\\vert \\cos \\theta \\Rightarrow \\frac{ r \\cdot s}{\\vert r\\vert } = \\vert s\\vert \\cos \\theta \\Rightarrow \\frac{ r \\cdot s}{\\vert r\\vert } = \\vert s\\vert \\cos \\theta \\Rightarrow \\frac{r}{\\vert r \\vert} \\cdot \\frac{r \\cdot s}{\\vert r\\vert } = \\vert s\\vert \\cos \\theta \\cdot \\frac{r}{\\vert r \\vert} \\Rightarrow \\frac{r}{\\vert r \\vert} \\cdot \\frac{r \\cdot s}{\\vert r\\vert } = \\vert s\\vert \\cos \\theta \\cdot \\frac{r}{\\vert r \\vert} which is the orthogonal projection of s s onto a straight line parallel to r r . Notice that this formula is intuitive, we take the scaler projection of s s onto r r (the length of the orthogonal projection of s s on r r ) and multiply it by a unit vector in the direction of r r , \\frac{r}{\\vert r \\vert} \\frac{r}{\\vert r \\vert} . Conclusions This was really the core video for this week. We found the size of a vector and we defined the dot product . We've then found out some mathematical operations we can do with the dot product (multiplication by a scalar and the dot product). We also proved that mathematical operations with vectors obey the following properties: commutative distributive over vector addition associative with scalar multiplication We then found that the dot product actually captures the angle between two vectors, the extent to which they go in the same direction, and also finds the projection of one vector onto another. Changing the reference frame Tip It is best to watch this video first, then return to and read through this section. If you want even more exposure to these ideas, try the first three sections of the Khan Academy course here . Changing basis So far we haven't really talked about the coordinate system of our vector space , the coordinates in which all of our vectors exist. In this section we'll look at what we mean by coordinate systems, and walk through a few examples of changing from one coordinate system to another. Remember that a vector (e.g. r r ) is just an object that takes us from the origin to some point in space . This could be some physical space or it could be some data space, like the attributes of a house (bedrooms, price, etc.). We could use a coordinate system defined itself by vectors, such as the vectors \\hat{i} \\hat{i} and \\hat{j} \\hat{j} that we defined before. Lets give them names \\hat{e_1} \\hat{e_1} and \\hat{e_2} \\hat{e_2} instead. We will define them to be of unit length, meaning they're of length 1. Note The little hat ( \\hat i \\hat i ) denotes unit length. So, \\hat{e_1} = \\begin{bmatrix} 1 \\\\\\ 0 \\end{bmatrix}, \\; \\hat{e_2} = \\begin{bmatrix} 0 \\\\\\ 1 \\end{bmatrix} \\hat{e_1} = \\begin{bmatrix} 1 \\\\\\ 0 \\end{bmatrix}, \\; \\hat{e_2} = \\begin{bmatrix} 0 \\\\\\ 1 \\end{bmatrix} Note if we had more dimensions in our space, we could just use more one-hot encoded vectors ( \\hat{e_n} \\hat{e_n} ) of dimension equal to the dimensions in our space. We can then define any other vector in our space in terms of \\hat{e_1} \\hat{e_1} and \\hat{e_2} \\hat{e_2} . For example, r_e = 3\\hat{e_1} + 3 \\hat{e_2} = \\begin{bmatrix} 3 \\\\\\ 4 \\end{bmatrix} r_e = 3\\hat{e_1} + 3 \\hat{e_2} = \\begin{bmatrix} 3 \\\\\\ 4 \\end{bmatrix} Here, the instruction is that r_e r_e is going to be equal to doing a vector sum of 3 \\hat{e_1} 3 \\hat{e_1} and 4 \\hat{e_2} 4 \\hat{e_2} . If you think about it, our choice of \\hat{e_1} \\hat{e_1} and \\hat{e_2} \\hat{e_2} is kind of arbitrary. There's no reason we couldn't have used different vectors to define our coordinate system Note These vectors don't even need to be at 90 degrees to each other or of the same length In any case, I could still have described r r as being some sum of some vectors I used to define the space. We call the vectors we use to define our vector space (e.g. \\hat{e_1} \\hat{e_1} and \\hat{e_2} \\hat{e_2} ) basis vectors . What we realize here, is that our vector r r exists independently of the coordinate system we use. The vector still takes us from the origin to some point in space, even when we change the coordinate system, more specifically, even when we change the basis vectors used to describe our vector space . It turns out, we can actually change the basis of the vector r r (call this r_e r_e ) to a new set of basis vectors, i.e. \\hat{b_1} \\hat{b_1} and \\hat{b_2} \\hat{b_2} , which we will denote r_b r_b . Furthermore, we can do this using the dot product so long as The new basis vectors are orthogonal to each other, i.e. \\hat{b_1} \\cdot \\hat{b_2} = 0 \\hat{b_1} \\cdot \\hat{b_2} = 0 We know the position of \\hat{b_1} \\hat{b_1} and \\hat{b_2} \\hat{b_2} in the space defined by \\hat{e_1} \\hat{e_1} and \\hat{e_2} \\hat{e_2} . Note We can still change basis even when the new basis vectors are not orthogonal to one another, but for this we will need matrices. See later parts of the course. Lets define \\hat{b_1} \\hat{b_1} and \\hat{b_2} \\hat{b_2} in the space defined by \\hat{e_1} \\hat{e_1} and \\hat{e_2} \\hat{e_2} : \\hat{b_1} = \\begin{bmatrix} -2 \\\\\\ 4 \\end{bmatrix}, \\; \\hat{b_2} = \\begin{bmatrix} 2 \\\\\\ 1 \\end{bmatrix} \\hat{b_1} = \\begin{bmatrix} -2 \\\\\\ 4 \\end{bmatrix}, \\; \\hat{b_2} = \\begin{bmatrix} 2 \\\\\\ 1 \\end{bmatrix} In order to determine r_b r_b , i.e. the vector r r defined in terms of the basis vectors \\hat{b_1} \\hat{b_1} and \\hat{b_2} \\hat{b_2} , we need to take sum the vector projection of r_e r_e onto \\hat{b_1} \\hat{b_1} and the vector projection of r_e r_e onto \\hat{b_2} \\hat{b_2} So, lets do it: Vector projection of r_e r_e onto \\hat{b_1} \\hat{b_1} \\hat{b_1}\\frac{r_e \\cdot \\hat{b_1}}{\\vert \\hat{b_1}\\vert ^2} = \\frac{3 \\times 2 + 4 \\times 1}{2^2 + 1^2} = \\frac{10}{5} = 2 \\hat{b_1} = 2 \\begin{bmatrix} 2 \\\\\\ 1\\end{bmatrix} = \\begin{bmatrix} 4 \\\\\\ 2\\end{bmatrix} \\hat{b_1}\\frac{r_e \\cdot \\hat{b_1}}{\\vert \\hat{b_1}\\vert ^2} = \\frac{3 \\times 2 + 4 \\times 1}{2^2 + 1^2} = \\frac{10}{5} = 2 \\hat{b_1} = 2 \\begin{bmatrix} 2 \\\\\\ 1\\end{bmatrix} = \\begin{bmatrix} 4 \\\\\\ 2\\end{bmatrix} Vector projection of r_e r_e onto \\hat{b_2} \\hat{b_2} \\hat{b_2}\\frac{r_e \\cdot \\hat{b_2}}{\\vert \\hat{b_2}\\vert ^2} = \\frac{3 \\times -2 + 4 \\times 4}{-2^2 + 4^2} = \\frac{10}{20} = \\frac{1}{2} \\hat{b_2} = \\frac{1}{2} \\begin{bmatrix} -2 \\\\\\ 4\\end{bmatrix} = \\begin{bmatrix} -1 \\\\\\ 2\\end{bmatrix} \\hat{b_2}\\frac{r_e \\cdot \\hat{b_2}}{\\vert \\hat{b_2}\\vert ^2} = \\frac{3 \\times -2 + 4 \\times 4}{-2^2 + 4^2} = \\frac{10}{20} = \\frac{1}{2} \\hat{b_2} = \\frac{1}{2} \\begin{bmatrix} -2 \\\\\\ 4\\end{bmatrix} = \\begin{bmatrix} -1 \\\\\\ 2\\end{bmatrix} Thus, r_b = \\hat{b_1} \\frac{r_e \\cdot \\hat{b_1}}{\\vert \\hat{b_1}\\vert ^2} + \\hat{b_2} \\frac{r_e \\cdot \\hat{b_2}}{\\vert \\hat{b_2}\\vert ^2} = 2 \\hat{b_1} + \\frac{1}{2} \\hat{b_2} = \\begin{bmatrix}2 \\\\\\ \\frac{1}{2}\\end{bmatrix} r_b = \\hat{b_1} \\frac{r_e \\cdot \\hat{b_1}}{\\vert \\hat{b_1}\\vert ^2} + \\hat{b_2} \\frac{r_e \\cdot \\hat{b_2}}{\\vert \\hat{b_2}\\vert ^2} = 2 \\hat{b_1} + \\frac{1}{2} \\hat{b_2} = \\begin{bmatrix}2 \\\\\\ \\frac{1}{2}\\end{bmatrix} Finally, notice that r_b = 2 \\hat{b_1} + \\frac{1}{2} \\hat{b_2} = 2 \\begin{bmatrix} 2 \\\\\\ 1\\end{bmatrix} + \\frac{1}{2} \\begin{bmatrix} -2 \\\\\\ 4\\end{bmatrix} = \\begin{bmatrix} 3 \\\\\\ 4\\end{bmatrix} = r_e r_b = 2 \\hat{b_1} + \\frac{1}{2} \\hat{b_2} = 2 \\begin{bmatrix} 2 \\\\\\ 1\\end{bmatrix} + \\frac{1}{2} \\begin{bmatrix} -2 \\\\\\ 4\\end{bmatrix} = \\begin{bmatrix} 3 \\\\\\ 4\\end{bmatrix} = r_e Conclusions We've seen that our vector describing our data isn't tied to the axis that we originally used to describe it . We can redescribe it using some other axis , some other basis vectors . It turns out that choosing basis vectors we use to describe the space of data carefully to help us solve our problem will be a very important thing in linear algebra, and in general. We can move the numbers in the vector we used to describe a data item from one basis to another. We can do that change just by taking the dot or projection product so long as the new basis factors are orthogonal to each other. Basis, vector space, and linear independence Tip Linear independence is really only brushed on here. To go deeper, check out this Khan Academy series. Previously we've seen that our basis vectors do not have to be the so called standard (or natural) basis . We can actually choose any basis vectors we want, which redefine how we we move about space. Standard Basis The set of unit vectors pointing in the direction of the axes of a Cartesian coordinate system. \\hat{e_x} = \\begin{bmatrix} 1 \\\\\\ 0 \\\\\\ 0 \\end{bmatrix}, \\; \\hat{e_y} = \\begin{bmatrix} 0 \\\\\\ 1 \\\\\\ 0 \\end{bmatrix}, \\; \\hat{e_z} = \\begin{bmatrix} 0 \\\\\\ 0 \\\\\\ 1 \\end{bmatrix} \\hat{e_x} = \\begin{bmatrix} 1 \\\\\\ 0 \\\\\\ 0 \\end{bmatrix}, \\; \\hat{e_y} = \\begin{bmatrix} 0 \\\\\\ 1 \\\\\\ 0 \\end{bmatrix}, \\; \\hat{e_z} = \\begin{bmatrix} 0 \\\\\\ 0 \\\\\\ 1 \\end{bmatrix} Note Also known as the orthonormal basis . Lets formally define what we mean by a basis (vector space), and define linear independence , which is going to let us understand how many dimensions our vector space actually has. Basis The basis is a set of n n vectors that: are not linear combinations of each other (linear independent) span the space that they describe If these two qualities are fulfilled, then the space defined by the basis is n n -dimensional. Linear independence A set of vectors is said to be linearly dependent if one of the vectors in the set can be defined as a linear combination of the others; if no vector in the set can be written in this way, then the vectors are said to be linearly independent . For example, imagine we had some candidate vector {b_3} {b_3} . If we could write {b_3} {b_3} as a linear combination of, say, {b_1} {b_1} and {b_2} {b_2} : {b_3} = c_1 {b_1} + c_2 {b_2} {b_3} = c_1 {b_1} + c_2 {b_2} where c_1 c_1 and c_2 c_2 were constants, then we would say that {b_3} {b_3} is linearly dependent on {b_1} {b_1} and {b_2} {b_2} . To drive the point home, we note that the following are true if {b_3} {b_3} is linearly dependent to the vectors {b_1} {b_1} and {b_2} {b_2} : {b_3} {b_3} does not lie in the plane spanned by {b_1} {b_1} and {b_2} {b_2} {b_3} \\ne c_1 {b_1} + c_2 {b_2} {b_3} \\ne c_1 {b_1} + c_2 {b_2} for any c_1 c_1 , c_2 \\in \\mathbb R c_2 \\in \\mathbb R OR, equivalently, 0 = c_1 {b_1} + c_2 {b_2} + c_3 {b_3} 0 = c_1 {b_1} + c_2 {b_2} + c_3 {b_3} implies that c_1 = c_2 = c_3 = 0 c_1 = c_2 = c_3 = 0 These concepts are central to the definition of dimension . As stated previously, if we have a set of n n basis vectors, then these vectors describe an n n -dimensional space, as we can express any n n -dimensional vector as a linear combination of our n n basis vectors. Now, notice what our basis vectors {b_n} {b_n} don't have to be. they don't have to be unit vectors, by which we mean vectors of length 1 and they don't have to be orthogonal (or normal ) to each other But, as it turns out, everything is going to be much easier if they are. So if at all possible, you want to construct what's called an orthonormal basic vector set , where all vectors of the set are at 90 90 degrees to each other and are all of unit length. Now, let's think about what happens when we map from one basis to another. The axes of the original grid are projected onto the new grid ; and potentially have different values on that new grid, but crucially , the projection keeps the grid being evenly spaced. Therefore, any mapping that we do from one set of basis vectors, (one coordinate system) to another set of basis vectors (another coordinate system), keeps the vector space being a regularly spaced grid , where our original rules of vector addition and multiplication by a scaler still work. Note Basically, it doesn't warp or fold space, which is what the linear bit in linear algebra means geometrically. Things might be stretched or rotated or inverted, but everything remains evenly spaced and linear combinations still work. Now, when the new basis vectors aren't orthogonal, then we won't be able to use the dot product (really, the projection) to map from one basis to another. We'll have to use matrices instead, which we'll meet in the next module. Tip Honestly, this part is tricky. It might be worth it to watch the first three videos of the Essence of Linear Algebra series. For the lazy, jumpy straight to this video . Conclusions In this section, we've talked about the dimensionality of a vector space in terms of the number of independent basis factors that it has. We found a test for independence: vectors are independent if one of them is not a linear combination of the others. Finally, we discussed what that means to map vectors from one space to another and how that is going to be useful in data science and machine learning. Summary of week 2 We've looked at vectors as being objects that describe where we are in space which could be a physical space, a space of data, or a parameter space of the parameters of a function. It doesn't really matter. It's just some space. Then we've defined vector addition and scaling a vector by a number, making it bigger or reversing its direction. Then we've gone on to find the magnitude or modulus of a vector, and the dot scalar and vector projection product. We've defined the basis of a vector space, its dimension, and the ideas of linear independence and linear combinations. We've used projections to look at one case of changes from one basis to another, for the case where the new basis is orthogonal.","title":"Week 2"},{"location":"linear_algebra/week_2/#week-2-vectors-are-objects-that-move-around-space","text":"In this module, we will look at the types operations we can do with vectors - finding the modulus or magnitude (size), finding the angle between vectors (dot or inner product) and projecting one vector onto another. We will then examine how the entries describing a vector will depend on what vectors we use to define the axes - the basis. That will then let us determine whether a proposed set of basis vectors are linearly independent . This will complete our examination of vectors, allowing us to move on to matrices and then to begin solving linear algebra problems. Learning Objectives Calculate basic operations (dot product, modulus, negation) on vectors Calculate a change of basis Recall linear independence Identify a linearly independent basis and relate this to the dimensionality of the space","title":"Week 2: Vectors are Objects that Move Around Space"},{"location":"linear_algebra/week_2/#finding-the-size-of-a-vector-its-angle-and-projection","text":"Tip It is probably worth it to watch this 3Blue1Brown video first before reading through this section. However, be warned, it is the most confusing one in the series. If you want even more practice, check out this Khan Academy track.","title":"Finding the size of a vector, its angle, and projection"},{"location":"linear_algebra/week_2/#modulus-inner-product","text":"Previously we looked at the two main vector operations of addition and scaling by a number (multiplication by a scalar ). As it turns out, those are really the only operations we need to be able to do in order define something as a vector. Now, we can move on to define two new ideas: the length of a vector , also called its size , and the dot product of a vector, also called its inner , scalar or projection product. Note The dot product is a huge and amazing concept in linear algebra with a huge number of implications. We'll only be able to touch on a few parts of it here, but enjoy. It's one of the most beautiful parts of linear algebra. Length of a vector Lets define a vector r r using the basis vectors we introduced earlier, i i and j j , r = a i + b j = \\begin{bmatrix} a \\\\\\ b \\end{bmatrix} r = a i + b j = \\begin{bmatrix} a \\\\\\ b \\end{bmatrix} To calculate the length of r r , also called the norm \\vert r\\vert \\vert r\\vert (or \\Vert r\\Vert \\Vert r\\Vert ), we could imagine drawing a triangle, with our vector r r as the hypotenuse: Note The length, magnitude, modulus and norm of a vector are all the same thing, and just represent a difference in terminology. If we are thinking of a vector as representing the line segment from the origin to a given point (i.e., the geometric interpretation), we may interpret the norm as the length of this line segment. If we are thinking of a vector as representing a physical quantity like acceleration or velocity, we may interpret the norm as the magnitude of this quantity (how \" large \" it is, regardless of its direction). By Pythagorus's Theorem , \\vert r \\vert = \\sqrt{a^2 + b^2} \\vert r \\vert = \\sqrt{a^2 + b^2} Vector dot product The dot product is one of several ways of multiplying two vectors together, specifically, it is an algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors) and returns a single number . The dot product has an algebraic and geometric interpretation. Algebraically , the dot product is the sum of the products of the corresponding entries of the two sequences of numbers . Geometrically , it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them . Algebraic definition of the dot product To illustrate the algebraic definition of the dot product, lets define two vectors r r and s s : r = \\begin{bmatrix}r_i \\\\ r_j\\end{bmatrix}, s = \\begin{bmatrix}s_i \\\\ s_j\\end{bmatrix} r = \\begin{bmatrix}r_i \\\\ r_j\\end{bmatrix}, s = \\begin{bmatrix}s_i \\\\ s_j\\end{bmatrix} The dot product is then: r \\cdot s = r_is_i + r_j s_j = (3)(-1) + (2)(2) = 1 r \\cdot s = r_is_i + r_j s_j = (3)(-1) + (2)(2) = 1 More formally, the algebraic definition of the dot product is: r \\cdot s = \\sum^n_{i=1}a_ib_i = a_1b_1 + a_2b_2 + ... + a_nb_n r \\cdot s = \\sum^n_{i=1}a_ib_i = a_1b_1 + a_2b_2 + ... + a_nb_n Note The algebraic definition of the dot product is simply the sum of the products obtained by multiplying each component from both vectors. Properties of the dot product The dot product is, commutative , e.g., r \\cdot s = s \\cdot r r \\cdot s = s \\cdot r distributive , e.g., r \\cdot (s + {t}) = r \\cdot s + r \\cdot {t} r \\cdot (s + {t}) = r \\cdot s + r \\cdot {t} associative over scalar multiplication, e.g., r \\cdot (a s) = a ( r \\cdot s) r \\cdot (a s) = a ( r \\cdot s) Lets prove the distributive property in the general case. Let: r = \\begin{bmatrix} r_1 \\\\\\ r_2 \\\\\\ . \\\\\\ . \\\\\\ . \\\\\\ r_n \\end{bmatrix}, \\; s = \\begin{bmatrix} s_1 \\\\\\ s_2 \\\\\\ . \\\\\\ . \\\\\\ . \\\\\\ s_n \\end{bmatrix}, \\; {t} = \\begin{bmatrix} t_1 \\\\\\ t_2 \\\\\\ . \\\\\\ . \\\\\\ . \\\\\\ t_n \\end{bmatrix} r = \\begin{bmatrix} r_1 \\\\\\ r_2 \\\\\\ . \\\\\\ . \\\\\\ . \\\\\\ r_n \\end{bmatrix}, \\; s = \\begin{bmatrix} s_1 \\\\\\ s_2 \\\\\\ . \\\\\\ . \\\\\\ . \\\\\\ s_n \\end{bmatrix}, \\; {t} = \\begin{bmatrix} t_1 \\\\\\ t_2 \\\\\\ . \\\\\\ . \\\\\\ . \\\\\\ t_n \\end{bmatrix} then, r \\cdot (s + {t}) = r_1 (s_1 + t_1) + r_2 (s_2 + t_2) + ... + r_n (s_n + t_n) r \\cdot (s + {t}) = r_1 (s_1 + t_1) + r_2 (s_2 + t_2) + ... + r_n (s_n + t_n) = r_1 s_1 + r_1t_1 + r_2s_2 + r_2t_2 + ... + r_ns_n + r_nt_n = r_1 s_1 + r_1t_1 + r_2s_2 + r_2t_2 + ... + r_ns_n + r_nt_n = r \\cdot s + r \\cdot {t} = r \\cdot s + r \\cdot {t} Note Proofs for the remaining properties are left as an exercise. Link between the dot product and the size of the vector If we take r r and dot it with itself, we get: r_i r_i + r_j r_j + ... + r_nr_n = r_i^2 + r_j^2 + ... + r_n^2 = \\vert r \\vert^2 r_i r_i + r_j r_j + ... + r_nr_n = r_i^2 + r_j^2 + ... + r_n^2 = \\vert r \\vert^2 So, the size of the vector is just given by r r dotted with itself and squared.","title":"Modulus &amp; inner product"},{"location":"linear_algebra/week_2/#cosine-dot-product","text":"Lets take the time to derive the geometric definition of the dot product. Note Recall, geometrically , the dot product is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them . We start with the law of cosines (also known as the cosine formula or cosine rule ) from algebra, which you'll remember, probably vaguely, from school. The law of cosines states that if we had a triangle with sides a a , b b , and c c , then: c^2 = a^2 + b^2 - 2ab \\cos \\theta c^2 = a^2 + b^2 - 2ab \\cos \\theta Now, we can translate this into our vector notation: \\vert r - s\\vert ^2 = \\vert r\\vert ^2 + \\vert s\\vert ^2 - 2\\vert r\\vert \\vert s\\vert \\cos \\theta \\vert r - s\\vert ^2 = \\vert r\\vert ^2 + \\vert s\\vert ^2 - 2\\vert r\\vert \\vert s\\vert \\cos \\theta LHS \\Rightarrow (r-s) \\cdot (r-s) = r \\cdot r - s \\cdot r - s \\cdot r - s \\cdot s \\Rightarrow (r-s) \\cdot (r-s) = r \\cdot r - s \\cdot r - s \\cdot r - s \\cdot s = \\vert r\\vert ^2 - 2 s \\cdot r + \\vert s\\vert ^2 = \\vert r\\vert ^2 - 2 s \\cdot r + \\vert s\\vert ^2 Note \\vert r - s\\vert ^2 = (r-s) \\cdot (r-s) \\vert r - s\\vert ^2 = (r-s) \\cdot (r-s) comes straight from the definition of the dot product. LHS = RHS \\Rightarrow \\vert r\\vert ^2 - 2 s \\cdot r + \\vert s\\vert ^2 = \\vert r\\vert ^2 + \\vert s\\vert ^2 - 2\\vert r\\vert \\vert s\\vert \\cos \\theta \\Rightarrow \\vert r\\vert ^2 - 2 s \\cdot r + \\vert s\\vert ^2 = \\vert r\\vert ^2 + \\vert s\\vert ^2 - 2\\vert r\\vert \\vert s\\vert \\cos \\theta \\Rightarrow r \\cdot s = \\vert r\\vert \\vert s\\vert \\cos \\theta \\Rightarrow r \\cdot s = \\vert r\\vert \\vert s\\vert \\cos \\theta So what we notice is that the dot product does something quite profound . It takes the size of the two vectors ( \\vert r\\vert , \\vert s \\vert \\vert r\\vert , \\vert s \\vert ) and multiplies them by \\cos \\cos of the angle between them. It tells us something about the extent to which the two vectors go in the same direction. If \\theta \\theta is zero, then \\cos \\theta \\cos \\theta is one and r \\cdot s r \\cdot s would just be the size of the two vectors multiplied together. If \\theta \\theta is 90 90 degrees ( i.e. r r and s s are orthogonal), then \\cos 90 \\cos 90 , is 0 0 and r \\cdot s r \\cdot s is 0 0 . More generally, Note Ignore the word \"score\" here, this image was taken from a blog post about machine learning. The blog post is worth checking out though. Full credit to Christian S. Perone for the image. In this way, the dot product captures whether the two vectors are pointing in similar directions (positive) or opposite directions (negative).","title":"Cosine &amp; dot product"},{"location":"linear_algebra/week_2/#projection","text":"The vector projection of a vector s s on (or onto) a nonzero vector r r (also known as the vector component or vector resolution of s s in the direction of r r ) is the orthogonal projection of s s onto a straight line parallel to r r . Tip Understanding projection can be a little tricky. If you want even more practice, check out this Khan Academy series. For the following triangle, Recall the geometric definition of the dot product: r \\cdot s = \\vert r\\vert \\vert s\\vert \\cos \\theta r \\cdot s = \\vert r\\vert \\vert s\\vert \\cos \\theta Notice that \\vert s\\vert \\cos \\theta \\vert s\\vert \\cos \\theta is the length of the adjacent side (adjacent to the angle shown). This term is the projection of the vector s s into (or onto) the vector r r . This is why the dot product is also called the projection product , because it takes the projection of one vector ( s s ) onto another ( r r ) times the magnitude or length of the other ( \\vert r \\vert \\vert r \\vert ). Note Note again that if s s was orthogonal to r r then \\vert s\\vert \\cos \\theta = \\vert s\\vert \\cos 90 = 0 = r \\cdot s \\vert s\\vert \\cos \\theta = \\vert s\\vert \\cos 90 = 0 = r \\cdot s . This provides a convenient way to check for orthogonality. Rearranging, we can compute the scalar projection of s s on r r : r \\cdot s = \\vert r\\vert\\vert s\\vert \\cos \\theta r \\cdot s = \\vert r\\vert\\vert s\\vert \\cos \\theta \\Rightarrow \\frac{ r \\cdot s}{\\vert r\\vert } = \\vert s\\vert \\cos \\theta \\Rightarrow \\frac{ r \\cdot s}{\\vert r\\vert } = \\vert s\\vert \\cos \\theta The scalar projection is a scalar , equal to the length of the orthogonal projection of s s on r r , with a negative sign if the projection has an opposite direction with respect to r r . We can also define the vector projection r \\cdot s = \\vert r\\vert\\vert s\\vert \\cos \\theta r \\cdot s = \\vert r\\vert\\vert s\\vert \\cos \\theta \\Rightarrow \\frac{ r \\cdot s}{\\vert r\\vert } = \\vert s\\vert \\cos \\theta \\Rightarrow \\frac{ r \\cdot s}{\\vert r\\vert } = \\vert s\\vert \\cos \\theta \\Rightarrow \\frac{r}{\\vert r \\vert} \\cdot \\frac{r \\cdot s}{\\vert r\\vert } = \\vert s\\vert \\cos \\theta \\cdot \\frac{r}{\\vert r \\vert} \\Rightarrow \\frac{r}{\\vert r \\vert} \\cdot \\frac{r \\cdot s}{\\vert r\\vert } = \\vert s\\vert \\cos \\theta \\cdot \\frac{r}{\\vert r \\vert} which is the orthogonal projection of s s onto a straight line parallel to r r . Notice that this formula is intuitive, we take the scaler projection of s s onto r r (the length of the orthogonal projection of s s on r r ) and multiply it by a unit vector in the direction of r r , \\frac{r}{\\vert r \\vert} \\frac{r}{\\vert r \\vert} . Conclusions This was really the core video for this week. We found the size of a vector and we defined the dot product . We've then found out some mathematical operations we can do with the dot product (multiplication by a scalar and the dot product). We also proved that mathematical operations with vectors obey the following properties: commutative distributive over vector addition associative with scalar multiplication We then found that the dot product actually captures the angle between two vectors, the extent to which they go in the same direction, and also finds the projection of one vector onto another.","title":"Projection"},{"location":"linear_algebra/week_2/#changing-the-reference-frame","text":"Tip It is best to watch this video first, then return to and read through this section. If you want even more exposure to these ideas, try the first three sections of the Khan Academy course here .","title":"Changing the reference frame"},{"location":"linear_algebra/week_2/#changing-basis","text":"So far we haven't really talked about the coordinate system of our vector space , the coordinates in which all of our vectors exist. In this section we'll look at what we mean by coordinate systems, and walk through a few examples of changing from one coordinate system to another. Remember that a vector (e.g. r r ) is just an object that takes us from the origin to some point in space . This could be some physical space or it could be some data space, like the attributes of a house (bedrooms, price, etc.). We could use a coordinate system defined itself by vectors, such as the vectors \\hat{i} \\hat{i} and \\hat{j} \\hat{j} that we defined before. Lets give them names \\hat{e_1} \\hat{e_1} and \\hat{e_2} \\hat{e_2} instead. We will define them to be of unit length, meaning they're of length 1. Note The little hat ( \\hat i \\hat i ) denotes unit length. So, \\hat{e_1} = \\begin{bmatrix} 1 \\\\\\ 0 \\end{bmatrix}, \\; \\hat{e_2} = \\begin{bmatrix} 0 \\\\\\ 1 \\end{bmatrix} \\hat{e_1} = \\begin{bmatrix} 1 \\\\\\ 0 \\end{bmatrix}, \\; \\hat{e_2} = \\begin{bmatrix} 0 \\\\\\ 1 \\end{bmatrix} Note if we had more dimensions in our space, we could just use more one-hot encoded vectors ( \\hat{e_n} \\hat{e_n} ) of dimension equal to the dimensions in our space. We can then define any other vector in our space in terms of \\hat{e_1} \\hat{e_1} and \\hat{e_2} \\hat{e_2} . For example, r_e = 3\\hat{e_1} + 3 \\hat{e_2} = \\begin{bmatrix} 3 \\\\\\ 4 \\end{bmatrix} r_e = 3\\hat{e_1} + 3 \\hat{e_2} = \\begin{bmatrix} 3 \\\\\\ 4 \\end{bmatrix} Here, the instruction is that r_e r_e is going to be equal to doing a vector sum of 3 \\hat{e_1} 3 \\hat{e_1} and 4 \\hat{e_2} 4 \\hat{e_2} . If you think about it, our choice of \\hat{e_1} \\hat{e_1} and \\hat{e_2} \\hat{e_2} is kind of arbitrary. There's no reason we couldn't have used different vectors to define our coordinate system Note These vectors don't even need to be at 90 degrees to each other or of the same length In any case, I could still have described r r as being some sum of some vectors I used to define the space. We call the vectors we use to define our vector space (e.g. \\hat{e_1} \\hat{e_1} and \\hat{e_2} \\hat{e_2} ) basis vectors . What we realize here, is that our vector r r exists independently of the coordinate system we use. The vector still takes us from the origin to some point in space, even when we change the coordinate system, more specifically, even when we change the basis vectors used to describe our vector space . It turns out, we can actually change the basis of the vector r r (call this r_e r_e ) to a new set of basis vectors, i.e. \\hat{b_1} \\hat{b_1} and \\hat{b_2} \\hat{b_2} , which we will denote r_b r_b . Furthermore, we can do this using the dot product so long as The new basis vectors are orthogonal to each other, i.e. \\hat{b_1} \\cdot \\hat{b_2} = 0 \\hat{b_1} \\cdot \\hat{b_2} = 0 We know the position of \\hat{b_1} \\hat{b_1} and \\hat{b_2} \\hat{b_2} in the space defined by \\hat{e_1} \\hat{e_1} and \\hat{e_2} \\hat{e_2} . Note We can still change basis even when the new basis vectors are not orthogonal to one another, but for this we will need matrices. See later parts of the course. Lets define \\hat{b_1} \\hat{b_1} and \\hat{b_2} \\hat{b_2} in the space defined by \\hat{e_1} \\hat{e_1} and \\hat{e_2} \\hat{e_2} : \\hat{b_1} = \\begin{bmatrix} -2 \\\\\\ 4 \\end{bmatrix}, \\; \\hat{b_2} = \\begin{bmatrix} 2 \\\\\\ 1 \\end{bmatrix} \\hat{b_1} = \\begin{bmatrix} -2 \\\\\\ 4 \\end{bmatrix}, \\; \\hat{b_2} = \\begin{bmatrix} 2 \\\\\\ 1 \\end{bmatrix} In order to determine r_b r_b , i.e. the vector r r defined in terms of the basis vectors \\hat{b_1} \\hat{b_1} and \\hat{b_2} \\hat{b_2} , we need to take sum the vector projection of r_e r_e onto \\hat{b_1} \\hat{b_1} and the vector projection of r_e r_e onto \\hat{b_2} \\hat{b_2} So, lets do it: Vector projection of r_e r_e onto \\hat{b_1} \\hat{b_1} \\hat{b_1}\\frac{r_e \\cdot \\hat{b_1}}{\\vert \\hat{b_1}\\vert ^2} = \\frac{3 \\times 2 + 4 \\times 1}{2^2 + 1^2} = \\frac{10}{5} = 2 \\hat{b_1} = 2 \\begin{bmatrix} 2 \\\\\\ 1\\end{bmatrix} = \\begin{bmatrix} 4 \\\\\\ 2\\end{bmatrix} \\hat{b_1}\\frac{r_e \\cdot \\hat{b_1}}{\\vert \\hat{b_1}\\vert ^2} = \\frac{3 \\times 2 + 4 \\times 1}{2^2 + 1^2} = \\frac{10}{5} = 2 \\hat{b_1} = 2 \\begin{bmatrix} 2 \\\\\\ 1\\end{bmatrix} = \\begin{bmatrix} 4 \\\\\\ 2\\end{bmatrix} Vector projection of r_e r_e onto \\hat{b_2} \\hat{b_2} \\hat{b_2}\\frac{r_e \\cdot \\hat{b_2}}{\\vert \\hat{b_2}\\vert ^2} = \\frac{3 \\times -2 + 4 \\times 4}{-2^2 + 4^2} = \\frac{10}{20} = \\frac{1}{2} \\hat{b_2} = \\frac{1}{2} \\begin{bmatrix} -2 \\\\\\ 4\\end{bmatrix} = \\begin{bmatrix} -1 \\\\\\ 2\\end{bmatrix} \\hat{b_2}\\frac{r_e \\cdot \\hat{b_2}}{\\vert \\hat{b_2}\\vert ^2} = \\frac{3 \\times -2 + 4 \\times 4}{-2^2 + 4^2} = \\frac{10}{20} = \\frac{1}{2} \\hat{b_2} = \\frac{1}{2} \\begin{bmatrix} -2 \\\\\\ 4\\end{bmatrix} = \\begin{bmatrix} -1 \\\\\\ 2\\end{bmatrix} Thus, r_b = \\hat{b_1} \\frac{r_e \\cdot \\hat{b_1}}{\\vert \\hat{b_1}\\vert ^2} + \\hat{b_2} \\frac{r_e \\cdot \\hat{b_2}}{\\vert \\hat{b_2}\\vert ^2} = 2 \\hat{b_1} + \\frac{1}{2} \\hat{b_2} = \\begin{bmatrix}2 \\\\\\ \\frac{1}{2}\\end{bmatrix} r_b = \\hat{b_1} \\frac{r_e \\cdot \\hat{b_1}}{\\vert \\hat{b_1}\\vert ^2} + \\hat{b_2} \\frac{r_e \\cdot \\hat{b_2}}{\\vert \\hat{b_2}\\vert ^2} = 2 \\hat{b_1} + \\frac{1}{2} \\hat{b_2} = \\begin{bmatrix}2 \\\\\\ \\frac{1}{2}\\end{bmatrix} Finally, notice that r_b = 2 \\hat{b_1} + \\frac{1}{2} \\hat{b_2} = 2 \\begin{bmatrix} 2 \\\\\\ 1\\end{bmatrix} + \\frac{1}{2} \\begin{bmatrix} -2 \\\\\\ 4\\end{bmatrix} = \\begin{bmatrix} 3 \\\\\\ 4\\end{bmatrix} = r_e r_b = 2 \\hat{b_1} + \\frac{1}{2} \\hat{b_2} = 2 \\begin{bmatrix} 2 \\\\\\ 1\\end{bmatrix} + \\frac{1}{2} \\begin{bmatrix} -2 \\\\\\ 4\\end{bmatrix} = \\begin{bmatrix} 3 \\\\\\ 4\\end{bmatrix} = r_e","title":"Changing basis"},{"location":"linear_algebra/week_2/#conclusions","text":"We've seen that our vector describing our data isn't tied to the axis that we originally used to describe it . We can redescribe it using some other axis , some other basis vectors . It turns out that choosing basis vectors we use to describe the space of data carefully to help us solve our problem will be a very important thing in linear algebra, and in general. We can move the numbers in the vector we used to describe a data item from one basis to another. We can do that change just by taking the dot or projection product so long as the new basis factors are orthogonal to each other.","title":"Conclusions"},{"location":"linear_algebra/week_2/#basis-vector-space-and-linear-independence","text":"Tip Linear independence is really only brushed on here. To go deeper, check out this Khan Academy series. Previously we've seen that our basis vectors do not have to be the so called standard (or natural) basis . We can actually choose any basis vectors we want, which redefine how we we move about space. Standard Basis The set of unit vectors pointing in the direction of the axes of a Cartesian coordinate system. \\hat{e_x} = \\begin{bmatrix} 1 \\\\\\ 0 \\\\\\ 0 \\end{bmatrix}, \\; \\hat{e_y} = \\begin{bmatrix} 0 \\\\\\ 1 \\\\\\ 0 \\end{bmatrix}, \\; \\hat{e_z} = \\begin{bmatrix} 0 \\\\\\ 0 \\\\\\ 1 \\end{bmatrix} \\hat{e_x} = \\begin{bmatrix} 1 \\\\\\ 0 \\\\\\ 0 \\end{bmatrix}, \\; \\hat{e_y} = \\begin{bmatrix} 0 \\\\\\ 1 \\\\\\ 0 \\end{bmatrix}, \\; \\hat{e_z} = \\begin{bmatrix} 0 \\\\\\ 0 \\\\\\ 1 \\end{bmatrix} Note Also known as the orthonormal basis . Lets formally define what we mean by a basis (vector space), and define linear independence , which is going to let us understand how many dimensions our vector space actually has. Basis The basis is a set of n n vectors that: are not linear combinations of each other (linear independent) span the space that they describe If these two qualities are fulfilled, then the space defined by the basis is n n -dimensional. Linear independence A set of vectors is said to be linearly dependent if one of the vectors in the set can be defined as a linear combination of the others; if no vector in the set can be written in this way, then the vectors are said to be linearly independent . For example, imagine we had some candidate vector {b_3} {b_3} . If we could write {b_3} {b_3} as a linear combination of, say, {b_1} {b_1} and {b_2} {b_2} : {b_3} = c_1 {b_1} + c_2 {b_2} {b_3} = c_1 {b_1} + c_2 {b_2} where c_1 c_1 and c_2 c_2 were constants, then we would say that {b_3} {b_3} is linearly dependent on {b_1} {b_1} and {b_2} {b_2} . To drive the point home, we note that the following are true if {b_3} {b_3} is linearly dependent to the vectors {b_1} {b_1} and {b_2} {b_2} : {b_3} {b_3} does not lie in the plane spanned by {b_1} {b_1} and {b_2} {b_2} {b_3} \\ne c_1 {b_1} + c_2 {b_2} {b_3} \\ne c_1 {b_1} + c_2 {b_2} for any c_1 c_1 , c_2 \\in \\mathbb R c_2 \\in \\mathbb R OR, equivalently, 0 = c_1 {b_1} + c_2 {b_2} + c_3 {b_3} 0 = c_1 {b_1} + c_2 {b_2} + c_3 {b_3} implies that c_1 = c_2 = c_3 = 0 c_1 = c_2 = c_3 = 0 These concepts are central to the definition of dimension . As stated previously, if we have a set of n n basis vectors, then these vectors describe an n n -dimensional space, as we can express any n n -dimensional vector as a linear combination of our n n basis vectors. Now, notice what our basis vectors {b_n} {b_n} don't have to be. they don't have to be unit vectors, by which we mean vectors of length 1 and they don't have to be orthogonal (or normal ) to each other But, as it turns out, everything is going to be much easier if they are. So if at all possible, you want to construct what's called an orthonormal basic vector set , where all vectors of the set are at 90 90 degrees to each other and are all of unit length. Now, let's think about what happens when we map from one basis to another. The axes of the original grid are projected onto the new grid ; and potentially have different values on that new grid, but crucially , the projection keeps the grid being evenly spaced. Therefore, any mapping that we do from one set of basis vectors, (one coordinate system) to another set of basis vectors (another coordinate system), keeps the vector space being a regularly spaced grid , where our original rules of vector addition and multiplication by a scaler still work. Note Basically, it doesn't warp or fold space, which is what the linear bit in linear algebra means geometrically. Things might be stretched or rotated or inverted, but everything remains evenly spaced and linear combinations still work. Now, when the new basis vectors aren't orthogonal, then we won't be able to use the dot product (really, the projection) to map from one basis to another. We'll have to use matrices instead, which we'll meet in the next module. Tip Honestly, this part is tricky. It might be worth it to watch the first three videos of the Essence of Linear Algebra series. For the lazy, jumpy straight to this video .","title":"Basis, vector space, and linear independence"},{"location":"linear_algebra/week_2/#conclusions_1","text":"In this section, we've talked about the dimensionality of a vector space in terms of the number of independent basis factors that it has. We found a test for independence: vectors are independent if one of them is not a linear combination of the others. Finally, we discussed what that means to map vectors from one space to another and how that is going to be useful in data science and machine learning.","title":"Conclusions"},{"location":"linear_algebra/week_2/#summary-of-week-2","text":"We've looked at vectors as being objects that describe where we are in space which could be a physical space, a space of data, or a parameter space of the parameters of a function. It doesn't really matter. It's just some space. Then we've defined vector addition and scaling a vector by a number, making it bigger or reversing its direction. Then we've gone on to find the magnitude or modulus of a vector, and the dot scalar and vector projection product. We've defined the basis of a vector space, its dimension, and the ideas of linear independence and linear combinations. We've used projections to look at one case of changes from one basis to another, for the case where the new basis is orthogonal.","title":"Summary of week 2"},{"location":"linear_algebra/week_3/","text":"Week 3: Matrices as Objects that Operate on Vectors Lets now turn our attention from vectors to matrices . First we will look at how to use matrices as tools to solve linear algebra problems, before introducing them as objects that transform vectors. We will then explain how to solve systems of linear equations using matrices, which will introduce the concept of inverse matrices and determinants. Finally, we'll look at cases of special matrices: when the determinant is zero, and where the matrix isn't invertible. Because many algorithms require us to invert a matrix as one of their steps, this special case is important. Learning Objectives Understand what a matrix is and how it corresponds to a transformation Explain and calculate inverse and determinant of matrices Identify and explain how to find inverses computationally Explore what it means for a matrix to be inertible Matrices Introduction to matrices At the start of the course, we encountered the \"apples and bananas\" problem: how to find the price of things when we only have the total bill. Now we're going to look at matrices, which can be thought of as objects that rotate and stretch vectors, and how they can be used to solve these sorts of problems. Let's go back to that apples and bananas problem. Say we walk into a shop and we buy two apples, and three bananas and that costs us 8 euros: 2a + 3b = 8 2a + 3b = 8 On another day and we buy 10 apples and 1 banana, and that costs me 13 euros: 10a + 1b = 13 10a + 1b = 13 Note Now you might say this is silly. What shop doesn't have sticker prices after all? But actually, businesses with complicated products and service agreements often use price discovery . Now these are just simultaneous equations but , I can write them down with matrices as follows: \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} We can say that the matrix \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} operates on the vector \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} to give the other vector \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} Our question, our problem to solve, is what vector \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} transforms to give us \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} Now, what if we use our matrix \\begin{pmatrix}2 & 3 \\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}2 & 3 \\\\ 10 & 1\\end{pmatrix} to transform our basis vectors \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 ? \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\cdot \\hat e_1 = \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}1 \\\\\\ 0 \\end{pmatrix} = \\begin{pmatrix}2 \\\\\\ 10 \\end{pmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\cdot \\hat e_1 = \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}1 \\\\\\ 0 \\end{pmatrix} = \\begin{pmatrix}2 \\\\\\ 10 \\end{pmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\cdot \\hat e_2 = \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}0 \\\\\\ 1 \\end{pmatrix} = \\begin{pmatrix}3 \\\\\\ 1 \\end{pmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\cdot \\hat e_2 = \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}0 \\\\\\ 1 \\end{pmatrix} = \\begin{pmatrix}3 \\\\\\ 1 \\end{pmatrix} It becomes clear that what this matrix is doing is actually transforming the basis vectors \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 to give us the vectors \\begin{pmatrix}2 \\\\\\ 10 \\end{pmatrix} \\text{ , } \\begin{pmatrix}3 \\\\\\ 1 \\end{pmatrix} \\begin{pmatrix}2 \\\\\\ 10 \\end{pmatrix} \\text{ , } \\begin{pmatrix}3 \\\\\\ 1 \\end{pmatrix} Generally speaking, we can think of the matrix \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} as a function that operates on input vectors in order to give us output vectors. A set of simultaneous equations, like the ones we have here, is asking, in effect, what input vector I need in order to get a transformed product (the output vector ) at position \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} Conclusions Hopefully, it is a little more clear now what we mean now by the term linear algebra. Linear algebra is linear , because it just takes input values, our a a and b b for example, and multiplies them by constants . Everything is linear . Finally, it's algebra simply because it is a notation describing mathematical objects and a system of manipulating those notations. So linear algebra is a mathematical system for manipulating vectors in the spaces described by vectors . Note This is important! We are noticing some kind of deep connection between simultaneous equations, these things called matrices, and the vectors we were talking about last week. It turns that the key to solving simultaneous equation problems is appreciating how vectors are transformed by matrices, which is the heart of linear algebra. Matrices as objects that operate on vectors How matrices transform space Tip Watch this video before reading through this section. So far, we have introduced the idea of a matrix and related it to the problem of solving simultaneous equations . We showed that the columns of a matrix can be thought of as the transformations applied to unit basis vector along each axis. This is a pretty profound idea, so lets flesh it out. We know that we can make any (2D) vector out of a vector sum of the scaled versions of \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 (our basis vectors). This means that the result of any linear transformation is just going to be some sum of the transformed basis vectors, ( \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 here). This is a bit hard to see but what it means is that the grid lines of our space stay parallel and evenly spaced . They might be stretched or sheared, but the origin stays where it is and there isn't any curviness to the space, it doesn't get warped --- a consequence of our scalar addition and multiplication rules for vectors. If we write down the matrix A A and the vector it is transforming as r r , we can represent our apples and bananas problem introduced earlier as: \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} A r = r' A r = r' Where r' r' is our transformed vector. We can generalize further, and add a scalar, n n : A (n r) = n r' A (n r) = n r' We notice that: A (r + s) = A r + A s A (r + s) = A r + A s Putting it together, we can represent any vector in 2D space as: A (n \\hat e_1 + m \\hat e_2) = n A \\hat e_1 + m A \\hat e_2 A (n \\hat e_1 + m \\hat e_2) = n A \\hat e_1 + m A \\hat e_2 \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;= n \\hat e_1' + m \\hat e_2' \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;= n \\hat e_1' + m \\hat e_2' Now let's try an example. Let, A = \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} A = \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} r = \\begin{bmatrix}3 \\\\\\ 2\\end{bmatrix} r = \\begin{bmatrix}3 \\\\\\ 2\\end{bmatrix} Then, A r = r' A r = r' \\Rightarrow \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}3 \\\\\\ 2\\end{bmatrix} = \\begin{bmatrix}12 \\\\\\ 32\\end{bmatrix} \\Rightarrow \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}3 \\\\\\ 2\\end{bmatrix} = \\begin{bmatrix}12 \\\\\\ 32\\end{bmatrix} Which is no different than how we might have multiplied matrices and vectors in school. But, we can think of this another way: A (n \\hat e_1 + m \\hat e_2) = r' A (n \\hat e_1 + m \\hat e_2) = r' \\Rightarrow \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\Biggl (3 \\begin{bmatrix}1 \\\\\\ 0\\end{bmatrix} + 2 \\begin{bmatrix}0 \\\\\\ 1\\end{bmatrix} \\Biggl) = \\Rightarrow \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\Biggl (3 \\begin{bmatrix}1 \\\\\\ 0\\end{bmatrix} + 2 \\begin{bmatrix}0 \\\\\\ 1\\end{bmatrix} \\Biggl) = 3 \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}1 \\\\\\ 0\\end{bmatrix} + 2 \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}0 \\\\\\ 1\\end{bmatrix} = 3 \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}1 \\\\\\ 0\\end{bmatrix} + 2 \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}0 \\\\\\ 1\\end{bmatrix} = = 3 \\begin{bmatrix}2 \\\\\\ 10\\end{bmatrix} + 2 \\begin{bmatrix}3 \\\\\\ 1\\end{bmatrix} = \\begin{bmatrix}12 \\\\\\ 32\\end{bmatrix} = 3 \\begin{bmatrix}2 \\\\\\ 10\\end{bmatrix} + 2 \\begin{bmatrix}3 \\\\\\ 1\\end{bmatrix} = \\begin{bmatrix}12 \\\\\\ 32\\end{bmatrix} The take home idea here is that the matrix A A just tells us where the basis vectors go . That's the transformation it does. Types of matrix transformation Lets illustrate the type of transformations we can perform with matrices with a number of examples. Note Remember, in linear algebra, linear transformations can be represented by matrices! We are only going to scratch the surface here and to continue to build up our intuition of viewing matrices as functions that apply transformations to some input vector . Identity matrix First, let's think about a matrix that doesn't change anything. Such a matrix is just composed of the basis vectors of the space, \\begin{bmatrix} 1 & 0 \\\\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} this is known as the identity matrix . It's the matrix that does nothing and leaves everything preserved, typically denoted I_m I_m where m m is the number of dimensions in our vector space. Scaling When our matrix contains values other than 0 0 in the diagnonal, we get a scaling : \\begin{bmatrix} m & 0 \\\\\\ 0 & n\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} mx \\\\\\ ny\\end{bmatrix} \\begin{bmatrix} m & 0 \\\\\\ 0 & n\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} mx \\\\\\ ny\\end{bmatrix} If m = 3 m = 3 and n=2 n=2 , visually this looks like: This transformation simply scales each dimension of the space by the value at the corresponding diagonal of the matrix. Note Note that when m \\lt 1 m \\lt 1 and/or n \\lt 1 n \\lt 1 , our space is actually compressed along the axes. Reflections When one or more of our diagonal values is negative, we get a reflection : \\begin{bmatrix} -1 & 0 \\\\\\ 0 & k\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} -x \\\\\\ ky\\end{bmatrix} \\begin{bmatrix} -1 & 0 \\\\\\ 0 & k\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} -x \\\\\\ ky\\end{bmatrix} In this case, our coordinate system is flipped across the vertical axis. When k = 2 k = 2 , visually this looks like: In another example, \\begin{bmatrix} -1 & 0 \\\\\\ 0 & -1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} -x \\\\\\ -y\\end{bmatrix} \\begin{bmatrix} -1 & 0 \\\\\\ 0 & -1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} -x \\\\\\ -y\\end{bmatrix} This transformation inverts all axes, and is known as an inversion . We can also produce mirror reflections over a straight line that crosses through the origin: \\begin{bmatrix} 0 & 1 \\\\\\ 1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} y \\\\\\ x\\end{bmatrix} \\begin{bmatrix} 0 & 1 \\\\\\ 1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} y \\\\\\ x\\end{bmatrix} or, \\begin{bmatrix} 0 & -1 \\\\\\ -1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} -y \\\\\\ -x\\end{bmatrix} \\begin{bmatrix} 0 & -1 \\\\\\ -1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} -y \\\\\\ -x\\end{bmatrix} Note Of course, we can also produce mirror transformations over the x x or y y axis as well, by making one of the diagonals 1 1 and the other -1 -1 . Shears Shears are visually similar to slanting . There are two possibilities: A shear parallel to the x x axis looks like: \\begin{bmatrix} 1 & k \\\\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x + ky \\\\\\ y\\end{bmatrix} \\begin{bmatrix} 1 & k \\\\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x + ky \\\\\\ y\\end{bmatrix} If k=1 k=1 , then visually this looks like: Or a shear parallel to the y y axis: \\begin{bmatrix} 1 & 0 \\\\\\ k & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\\\\\ kx + y\\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\\\ k & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\\\\\ kx + y\\end{bmatrix} Rotations Finally, we can rotate the space about the orign. For example, \\begin{bmatrix} 0 & -1 \\\\\\ 1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} y \\\\\\ -x\\end{bmatrix} \\begin{bmatrix} 0 & -1 \\\\\\ 1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} y \\\\\\ -x\\end{bmatrix} would rotate the entire space 90^0 90^0 counterclockwise. More generally, for a rotation by a angle \\theta \\theta clockwise about the orgin: \\begin{bmatrix} \\cos \\theta & \\sin \\theta \\\\\\ - \\sin \\theta & \\cos \\theta \\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\cos \\theta + y \\sin \\theta \\\\\\ - x \\sin \\theta + y \\cos \\theta\\end{bmatrix} \\begin{bmatrix} \\cos \\theta & \\sin \\theta \\\\\\ - \\sin \\theta & \\cos \\theta \\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\cos \\theta + y \\sin \\theta \\\\\\ - x \\sin \\theta + y \\cos \\theta\\end{bmatrix} and for a rotation by a angle \\theta \\theta counterclockwise about the orgin \\begin{bmatrix} \\cos \\theta & - \\sin \\theta \\\\\\ \\sin \\theta & \\cos \\theta \\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\cos \\theta - y \\sin \\theta \\\\\\ x \\sin \\theta + y \\cos \\theta\\end{bmatrix} \\begin{bmatrix} \\cos \\theta & - \\sin \\theta \\\\\\ \\sin \\theta & \\cos \\theta \\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\cos \\theta - y \\sin \\theta \\\\\\ x \\sin \\theta + y \\cos \\theta\\end{bmatrix} Conclusions In this section, we described the major transformations that a matrix can perform on a vector. Next, we will look at how we can combine these transformations (known as composition ) to produce more complex matrix transformations Composition or combination of matrix transformations Tip Watch this video before reading through this section. So what is the point of introducing these different geometric transformations in this class? Well, if you want to do any kind of shape alteration, say of all the pixels in an image, or a face, then you can always make that shape change out of some combination of rotations , shears , stretches , and inverses . Note One example where these geometric transformations may be useful is in facial recognition , where we may preprocess every image by transforming it so that the person(s) face(s) are directly facing the camera. Lets illustrate this composition of matrix transformations with an example. Here, we will first apply a 90^o 90^o rotation clockwise about the x x -axis, and then a shear parallel to the x x -axis. Let the first transformation matrix be A_1 A_1 : \\begin{bmatrix} 0 & 1 \\\\\\ -1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} y \\\\\\ -x\\end{bmatrix} \\begin{bmatrix} 0 & 1 \\\\\\ -1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} y \\\\\\ -x\\end{bmatrix} And our second transformation matrix A_2 A_2 : \\begin{bmatrix} 1 & 1 \\\\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x + y \\\\\\ y\\end{bmatrix} \\begin{bmatrix} 1 & 1 \\\\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x + y \\\\\\ y\\end{bmatrix} Applying A_1 A_1 to our basis vectors \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 gives us \\hat e_1' \\hat e_1' and \\hat e_1' \\hat e_1' : \\hat e_1' = A_1 \\cdot \\hat e_1 = \\begin{bmatrix} 0 \\\\\\ -1\\end{bmatrix} \\hat e_1' = A_1 \\cdot \\hat e_1 = \\begin{bmatrix} 0 \\\\\\ -1\\end{bmatrix} \\hat e_2' = A_1 \\cdot \\hat e_2 = \\begin{bmatrix} 1 \\\\\\ 0\\end{bmatrix} \\hat e_2' = A_1 \\cdot \\hat e_2 = \\begin{bmatrix} 1 \\\\\\ 0\\end{bmatrix} Applying A_2 A_2 to our new transformed vectors \\hat e_1' \\hat e_1' and \\hat e_2' \\hat e_2' gives us \\hat e_1'' \\hat e_1'' and \\hat e_1'' \\hat e_1'' : \\hat e_1'' = A_2 \\cdot \\hat e_1' = \\begin{bmatrix}-1 \\\\\\ -1\\end{bmatrix} \\hat e_1'' = A_2 \\cdot \\hat e_1' = \\begin{bmatrix}-1 \\\\\\ -1\\end{bmatrix} \\hat e_2'' = A_2 \\cdot \\hat e_2' = \\begin{bmatrix} 1 \\\\\\ 0\\end{bmatrix} \\hat e_2'' = A_2 \\cdot \\hat e_2' = \\begin{bmatrix} 1 \\\\\\ 0\\end{bmatrix} Notice that if we stack our final transformed vectors \\hat e_1'' \\hat e_1'' and \\hat e_1'' \\hat e_1'' as columns we get the matrix: \\begin{bmatrix} -1 & 1 \\\\\\ -1 & 0\\end{bmatrix} \\begin{bmatrix} -1 & 1 \\\\\\ -1 & 0\\end{bmatrix} Which is equal to A_2 \\cdot A_1 A_2 \\cdot A_1 . Geometrically, this looks like: Conclusions The take home message here is that the transformation A_2 \\cdot (A_1 \\cdot r) A_2 \\cdot (A_1 \\cdot r) for some transformation matrices A_1 A_1 and A_2 A_2 and some vector r r , is equivalent to the transformation (A_2A_1) \\cdot r (A_2A_1) \\cdot r Note Note that A_2 \\cdot (A_1 \\cdot r) \\ne A_1 \\cdot (A_2 \\cdot r) A_2 \\cdot (A_1 \\cdot r) \\ne A_1 \\cdot (A_2 \\cdot r) , that is, the order in which we apply our transformations matters . As it turns out, the key to solving simultaneous equation problems is appreciating how vectors are transformed by matrices, which is at the the heart of linear algebra. Matrix inverses In this section, we are finally going to present a way to solve the apples and bananas problem with matrices. Along the way, we're going to find out about a thing called the inverse of a matrix and a method for finding it. Tip Watch this video before reading this section. For more practice with matrix inverses, see Khan Academy sections here and here . Gaussian elimination: Solving the apples and bananas problem First, recall our apples and bananas problem in matrix form: \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix} 8 \\\\\\ 3\\end{bmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix} 8 \\\\\\ 3\\end{bmatrix} Where we want to find the price of individual apples ( a a ) and bananas ( b b ). Simplifying, lets say that: A\\cdot r = s A\\cdot r = s Note This, in effect is saying: \" A A operates on vector r r to give us s s \". To solve for r r , we need to move A A to the other side of the equation. But how? . Well, to \"undo\" a division (and isolate x x ), you multiply by the reciprocal: \\frac{1}{2}x = 4 \\frac{1}{2}x = 4 \\Rightarrow x = 8 \\Rightarrow x = 8 Likewise, to undo a multiplication, we divide by the reciprocal: 2x = 4 2x = 4 \\Rightarrow x = 2 \\Rightarrow x = 2 How do we undo the transformation performed by A A ? The answer is to find the matrix A^{-1} A^{-1} (known as the inverse of A A ) such that: A^{-1}A = I A^{-1}A = I where I I is the identity matrix . We call A^{-1} A^{-1} the inverse of A A because is it reverses whatever transformation A A does, giving us back I I . We note that: A^{-1}A\\cdot r = A^{-1} s A^{-1}A\\cdot r = A^{-1} s \\Rightarrow\\ I \\cdot r = A^{-1} s \\Rightarrow\\ I \\cdot r = A^{-1} s \\Rightarrow\\ r = A^{-1} s \\Rightarrow\\ r = A^{-1} s So, if we could find the inverse of A A (i.e. find A^{-1} A^{-1} ), we can solve our problem (i.e. find a a and b b ). We can solve for A^{-1} A^{-1} with a series of row operations or substitutions (known as Gaussian elimination ). Let's look at a slightly more complicated problem to see how this is done: \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 15 \\\\\\ 21 \\\\\\ 13\\end{bmatrix} \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 15 \\\\\\ 21 \\\\\\ 13\\end{bmatrix} We start by subtracting row 1 from rows 2 and 3, which gives us a matrix in row echelon form (technically, reduced row echelon form ): \\begin{pmatrix}1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 15 \\\\\\ 6 \\\\\\ 2\\end{bmatrix} \\begin{pmatrix}1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 15 \\\\\\ 6 \\\\\\ 2\\end{bmatrix} We then perform two steps of back substitution to get the identify matrix: \\begin{pmatrix}1 & 1 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 9 \\\\\\ 4 \\\\\\ 2\\end{bmatrix} \\begin{pmatrix}1 & 1 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 9 \\\\\\ 4 \\\\\\ 2\\end{bmatrix} \\begin{pmatrix}1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 5 \\\\\\ 4 \\\\\\ 2\\end{bmatrix} \\begin{pmatrix}1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 5 \\\\\\ 4 \\\\\\ 2\\end{bmatrix} We can then read the solution right from the matrices a = 5 a = 5 , b = 4 b = 4 , c = 2 c = 2 . Tip If you are still feeling uneasy about using Gaussian elimination to solve a system of linear equations, see here for an example walked through step-by-step example. Full credit to PatrickJMT . Conclusions As it turns out, we don't have to compute the inverse at all to solve a system of linear equations. Although we showed the process of Gaussian elimination for some vectors r r and s s , we can use it in the general case to solve for any linear equation of the form A\\cdot r = s A\\cdot r = s . This actually one of the most computationally efficient ways to solve this problem, and it's going to work every time. From Gaussian elimination to finding the inverse matrix Now, let's think about how we can apply this idea of elimination to find the inverse matrix, which solves the more general problem no matter what vectors I write down on the right hand side. Say I have a 3 \\times \\times 3 matrix A A and its inverse B B ( B = A^{-1}) B = A^{-1}) . By the definition of the inverse , AB = BA = I_n AB = BA = I_n If we use our matrix A A from the last section: \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = I \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = I Note Where b_{ij} b_{ij} is the element at the i^{th} i^{th} row and j^{th} j^{th} column of matrix B B . we notice that the first column of B B is just a vector. It's a vector that describes what the B B matrix, the inverse of A A , does to space. Note Actually, it's the transformation that that vector does to the x-axis. This means that: \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{pmatrix}b_{11} \\\\\\ b_{21} \\\\\\ b_{31}\\end{pmatrix} = \\begin{pmatrix} 1 \\\\\\ 0 \\\\\\ 0\\end{pmatrix} \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{pmatrix}b_{11} \\\\\\ b_{21} \\\\\\ b_{31}\\end{pmatrix} = \\begin{pmatrix} 1 \\\\\\ 0 \\\\\\ 0\\end{pmatrix} Now, we could solve this by the elimination method and back substitution in just the way we did before. Then, we could do it again for the second column of B B , and finally the third. In this way, we would have solved for B B , in other words, we would have found A^{-1} A^{-1} . It turns out, we can actually solve for B B all at once. So lets do that: \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = I \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = I Subtract row 1 from row 2 and 3: \\Rightarrow \\begin{pmatrix}1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & -1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix}1 & 0 & 0 \\\\\\ -1 & 1 & 0 \\\\\\ -1 & 0 & 1\\end{pmatrix} \\Rightarrow \\begin{pmatrix}1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & -1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix}1 & 0 & 0 \\\\\\ -1 & 1 & 0 \\\\\\ -1 & 0 & 1\\end{pmatrix} Multiply row 3 by -1 -1 : \\Rightarrow \\begin{pmatrix}1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix}1 & 0 & 0 \\\\\\ -1 & 1 & 0 \\\\\\ 1 & 0 & -1\\end{pmatrix} \\Rightarrow \\begin{pmatrix}1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix}1 & 0 & 0 \\\\\\ -1 & 1 & 0 \\\\\\ 1 & 0 & -1\\end{pmatrix} Now that row 3 is in row echelon form , we can substitute it back into row 2 and row 1: \\Rightarrow \\begin{pmatrix}1 & 1 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} -2 & 0 & 3 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} \\Rightarrow \\begin{pmatrix}1 & 1 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} -2 & 0 & 3 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} And finally, back substitute row 2 into row 1: \\Rightarrow \\begin{pmatrix}1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} 0 & -1 & 2 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} \\Rightarrow \\begin{pmatrix}1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} 0 & -1 & 2 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} Because any matrix times its identity is that matrix itself: \\begin{pmatrix}1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} 0 & -1 & 2 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} \\begin{pmatrix}1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} 0 & -1 & 2 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} \\Rightarrow \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} 0 & -1 & 2 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} \\Rightarrow \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} 0 & -1 & 2 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} Note You can actually prove to yourself that we got the right answer by checking that A \\cdot B = I A \\cdot B = I . So that's our answer. We've found the identity matrix, B = A^{-1} B = A^{-1} for A A , and we did this by transforming A A into it's identity matrix via elimination and back substitution. Moreover, because B B could be any matrix, we have solved this in the general case. The solution is the same regardless of the number of dimensions, and this leads to a computationally efficient way to invert a matrix. Note There are computationally faster methods of computing the inverse, one such method is known as a decomposition process . In practice, you will simply call the solver of your problem or function, something like inv(A) , and it will pick the best method by inspecting the matrix you give it and return the answer. Conclusions We have figured out how to solve sets of linear equations in the general case, by a procedure we can implement in a computer really easily (known as Gaussian elimination ), and we've generalized this method to the to find the inverse of a matrix, regardless of what is on the right hand side of our system of equations. Special matrices In the final section of this week, we're going to look at a property of a matrix called the determinant . Note The determinant is a value that can be computed from the elements of a square matrix . The determinant of a matrix A A is denoted det(A) det(A) , det A det A , or \\vert A \\vert \\vert A \\vert . Geometrically, it can be viewed as the scaling factor of the linear transformation described by the matrix. We'll also look at what happens when a matrix doesn't have linearly independent basis vectors. Tip Watch this video before reading this section. For more practice with matrix inverses, see Khan Academy sections here and here . The determinant Let's start by looking at the simple matrix: \\begin{pmatrix} a & 0 \\\\\\ 0 & d\\end{pmatrix} \\begin{pmatrix} a & 0 \\\\\\ 0 & d\\end{pmatrix} If we multiply this matrix by our basis vectors \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 , we get \\hat e_1' = \\begin{bmatrix} a \\\\\\ 0 \\end{bmatrix} \\hat e_1' = \\begin{bmatrix} a \\\\\\ 0 \\end{bmatrix} \\hat e_2' = \\begin{bmatrix} 0 \\\\\\ d \\end{bmatrix} \\hat e_2' = \\begin{bmatrix} 0 \\\\\\ d \\end{bmatrix} So, in plain english, we have stretched our x x -axis by a factor of a a and our y y -axis by a factor of d d and, therefore, scaled the area of the grid cells of the vector space by a factor of ad ad . Note This is a property of the fact that any linear transformation keeps grid lines parallel and evenly spaced. We call this number, ad ad the determinant . Now, if I instead have a matrix: \\begin{pmatrix} a & b \\\\\\ 0 & d\\end{pmatrix} \\begin{pmatrix} a & b \\\\\\ 0 & d\\end{pmatrix} then this is still going to stretch \\hat e_1 \\hat e_1 out by a factor of a a , but on the other axis, I am going to move \\hat e_2 \\hat e_2 hat to: \\hat e_2 = \\begin{pmatrix} b \\\\\\ d\\end{pmatrix} \\hat e_2 = \\begin{pmatrix} b \\\\\\ d\\end{pmatrix} What we have done is taken the original grid and stretched it along the x x -axis by a a and, along the y y -axis by d d and sheared it (parallel to the x x -axis) by b b . Note We've still changed the size, the scale of the space (which is what the determinant really is) by a factor of ad ad . Notice that the area defined by our transformed vectors \\hat e_1' \\hat e_1' and \\hat e_2' \\hat e_2' is still just the base times the perpendicular height, ad ad (the determinant). Lets flesh this out in the general case. Say we have the matrix: A = \\begin{pmatrix} a & b \\\\\\ c & d\\end{pmatrix} A = \\begin{pmatrix} a & b \\\\\\ c & d\\end{pmatrix} Multiplying this matrix by our basis vectors yields a parallelogram (stretched by a a and d d and sheared by b b and c c ). We can actual compute the area of this parallelogram as follows: Note We find the area of this parallelogram by finding the area of the whole box the encloses it, and subtracting off combined area of the the little bits around it. The exact method for solving the area is not important (although it is pretty trivial). What is important, is that the determinant of A A can be computed as \\vert A \\vert = ad - bc \\vert A \\vert = ad - bc , and that this computation has a geometric interpretation . Note This is the formula for the determinant of a square 2 \\times 2 2 \\times 2 matrix. See here for the formula for higher dimensional matrices. Now, in school when you looked at matrices, you probably saw that you could find the inverse in the following way. For a matrix: A = \\begin{pmatrix} a & b \\\\\\ c & d\\end{pmatrix} A = \\begin{pmatrix} a & b \\\\\\ c & d\\end{pmatrix} Exchange a a and the d d , and switch the sign on the b b and the c c Multiply A A by this matrix Scale the transformation by \\frac{1}{ad - bc} \\frac{1}{ad - bc} In the general case, this looks like: \\frac{1}{ad - bc} \\begin{pmatrix} a & b \\\\\\ c & d\\end{pmatrix} \\begin{pmatrix} d & -b \\\\\\ -c & a\\end{pmatrix} = \\frac{1}{ad - bc} \\begin{pmatrix} ad - bc & 0 \\\\\\ 0 & ad - bc\\end{pmatrix} = I \\frac{1}{ad - bc} \\begin{pmatrix} a & b \\\\\\ c & d\\end{pmatrix} \\begin{pmatrix} d & -b \\\\\\ -c & a\\end{pmatrix} = \\frac{1}{ad - bc} \\begin{pmatrix} ad - bc & 0 \\\\\\ 0 & ad - bc\\end{pmatrix} = I This demonstrates that \\frac{1}{ad - bc} \\begin{pmatrix} d & -b \\\\\\ -c & a\\end{pmatrix} \\frac{1}{ad - bc} \\begin{pmatrix} d & -b \\\\\\ -c & a\\end{pmatrix} is in fact the inverse of the matrix A A . This helps capture what the determinant really is. It's the amount by which the original matrix scaled vector space. In the above example, dividing by the determinant normalizes the space back to its original size. Note We could spend a lot of time talking about how to solve for the determinant. However, knowing how to do the operations isn't really a useful skill. Many programming libraries (e.g. python ) have linear algebra libraries (e.g. Numpy ) which makes computing the determinant as easy, for example, as calling det(A) . If you really want to know how to compute determinants by hand, then look up a QR decomposition online. A determinant of zero Now, let's think about the matrix A = \\begin{pmatrix} 1 & 2 \\\\\\ 1 & 2\\end{pmatrix} A = \\begin{pmatrix} 1 & 2 \\\\\\ 1 & 2\\end{pmatrix} If we multiply this matrix by our basis vectors \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 , we get \\hat e_1' = \\begin{bmatrix} 1 \\\\\\ 1 \\end{bmatrix} \\hat e_1' = \\begin{bmatrix} 1 \\\\\\ 1 \\end{bmatrix} \\hat e_2' = \\begin{bmatrix} 2 \\\\\\ 2 \\end{bmatrix} \\hat e_2' = \\begin{bmatrix} 2 \\\\\\ 2 \\end{bmatrix} So this matrix, when applied to our vector space, actually collapses it onto a line . All our y y 's are mapped to the vector \\begin{bmatrix}2 \\\\\\ 2 \\end{bmatrix} \\begin{bmatrix}2 \\\\\\ 2 \\end{bmatrix} and our x x 's to \\begin{bmatrix}1 \\\\\\ 1 \\end{bmatrix} \\begin{bmatrix}1 \\\\\\ 1 \\end{bmatrix} Correspondingly, we notice that \\vert A \\vert = 0 \\vert A \\vert = 0 . Therefore, the area enclosed by the new basis vectors is zero . A negative determinant A negative determinant simply means that the transformation has flipped the orientation of our vector space. This is much easier to see than to explain ; check out this video which presents some awesome visualizations of the determinant. What the determinant means numerically To drive home the numerical interpretation of the determinant, lets start with this set of simultaneous equations: \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 2 & 3 & 7\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 17 \\\\\\ 29\\end{pmatrix} \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 2 & 3 & 7\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 17 \\\\\\ 29\\end{pmatrix} You'll notice that both the rows and the columns are linearly dependent . Thinking about the columns of this matrix as the basis vectors of some 3-dimensional space, we note that this transformation collapses our vector space from being 3D to 2D (by collapsing every point in space onto a plane). Let's see what this means numerically by trying to reduce our matrix to row-echelon form: \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 2 & 3 & 7\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 17 \\\\\\ 29\\end{pmatrix} \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 2 & 3 & 7\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 17 \\\\\\ 29\\end{pmatrix} Subtract row 1 from row 2, \\Rightarrow \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 2 & 3 & 7\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 5 \\\\\\ 29\\end{pmatrix} \\Rightarrow \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 2 & 3 & 7\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 5 \\\\\\ 29\\end{pmatrix} Subtract row 1 plus row 2 from row 3, \\Rightarrow \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & 0\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 5 \\\\\\ 0\\end{pmatrix} \\Rightarrow \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & 0\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 5 \\\\\\ 0\\end{pmatrix} while the matrix is now is row-echelon form, we don't have a final entry of the matrix ( 0\\cdot c = 0 0\\cdot c = 0 ). Because we don't have a solution for c c we can't back substitute, and we can't solve our system of equations. Note The reason we can't solve this system is because we don't have enough information. In keeping with our apples and bananas problem, imagine that when we went in to buy apples and bananas and carrots the third time, we ordered just a the sum of first two orders. Therefore, we didn't get any new information and thus don't have enough data to find out the solution for how much apples and bananas and carrots cost. This third order wasn't linearly independent from our first two, in the language of matrices and vectors. So, when the basis vectors describing a matrix aren't linearly independent, then the determinant is zero , and we can't solve the system. The loss of information when we map from n n -dimensional to (n-x) (n-x) -dimensional space (where x \\ge 1 x \\ge 1 ) means we cannot possibly know what the inverse matrix is (as it is impossible to map a lower dimensional space back to the original, higher dimensional space). When a matrix has no inverse, we say that it is singular . Note There are situations where we might want to do a transformation that collapses the number of dimensions in a space, but it means that we cannot possibly reverse the mapping, meaning the matrix has no inverse. This also means we cannot solve a system of linear equations defined by a singular matrix using Gaussian elimination and back substitution. Summary In the last section of this week, we took a look at the determinant, which is how much a given transformation scales our space. In 2-dimensions, this can be thought as the scalar multiple appleid to any area of our space, and in 3-dimensions any volume of our space. We also looked at the special case where the determinant is zero and found that this means that the basis vectors aren't linearly independent, which in turn means that the inverse doesn't exist. To summarize Week 3 , we introduced matrices as objects that transforms space. looked at different archetypes of matrices, like rotations , inverses , stretches , and shears , how to combine matrices by doing successive transformations, known as m atrix multiplication or composition how to solve systems of linear equations by elimination and how to find inverses and finally, we introduced determinants and showed how that relates to the concept of linear independence .","title":"Week 3"},{"location":"linear_algebra/week_3/#week-3-matrices-as-objects-that-operate-on-vectors","text":"Lets now turn our attention from vectors to matrices . First we will look at how to use matrices as tools to solve linear algebra problems, before introducing them as objects that transform vectors. We will then explain how to solve systems of linear equations using matrices, which will introduce the concept of inverse matrices and determinants. Finally, we'll look at cases of special matrices: when the determinant is zero, and where the matrix isn't invertible. Because many algorithms require us to invert a matrix as one of their steps, this special case is important. Learning Objectives Understand what a matrix is and how it corresponds to a transformation Explain and calculate inverse and determinant of matrices Identify and explain how to find inverses computationally Explore what it means for a matrix to be inertible","title":"Week 3: Matrices as Objects that Operate on Vectors"},{"location":"linear_algebra/week_3/#matrices","text":"","title":"Matrices"},{"location":"linear_algebra/week_3/#introduction-to-matrices","text":"At the start of the course, we encountered the \"apples and bananas\" problem: how to find the price of things when we only have the total bill. Now we're going to look at matrices, which can be thought of as objects that rotate and stretch vectors, and how they can be used to solve these sorts of problems. Let's go back to that apples and bananas problem. Say we walk into a shop and we buy two apples, and three bananas and that costs us 8 euros: 2a + 3b = 8 2a + 3b = 8 On another day and we buy 10 apples and 1 banana, and that costs me 13 euros: 10a + 1b = 13 10a + 1b = 13 Note Now you might say this is silly. What shop doesn't have sticker prices after all? But actually, businesses with complicated products and service agreements often use price discovery . Now these are just simultaneous equations but , I can write them down with matrices as follows: \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} We can say that the matrix \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} operates on the vector \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} to give the other vector \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} Our question, our problem to solve, is what vector \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} transforms to give us \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} Now, what if we use our matrix \\begin{pmatrix}2 & 3 \\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}2 & 3 \\\\ 10 & 1\\end{pmatrix} to transform our basis vectors \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 ? \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\cdot \\hat e_1 = \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}1 \\\\\\ 0 \\end{pmatrix} = \\begin{pmatrix}2 \\\\\\ 10 \\end{pmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\cdot \\hat e_1 = \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}1 \\\\\\ 0 \\end{pmatrix} = \\begin{pmatrix}2 \\\\\\ 10 \\end{pmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\cdot \\hat e_2 = \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}0 \\\\\\ 1 \\end{pmatrix} = \\begin{pmatrix}3 \\\\\\ 1 \\end{pmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\cdot \\hat e_2 = \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}0 \\\\\\ 1 \\end{pmatrix} = \\begin{pmatrix}3 \\\\\\ 1 \\end{pmatrix} It becomes clear that what this matrix is doing is actually transforming the basis vectors \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 to give us the vectors \\begin{pmatrix}2 \\\\\\ 10 \\end{pmatrix} \\text{ , } \\begin{pmatrix}3 \\\\\\ 1 \\end{pmatrix} \\begin{pmatrix}2 \\\\\\ 10 \\end{pmatrix} \\text{ , } \\begin{pmatrix}3 \\\\\\ 1 \\end{pmatrix} Generally speaking, we can think of the matrix \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} as a function that operates on input vectors in order to give us output vectors. A set of simultaneous equations, like the ones we have here, is asking, in effect, what input vector I need in order to get a transformed product (the output vector ) at position \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix}","title":"Introduction to matrices"},{"location":"linear_algebra/week_3/#conclusions","text":"Hopefully, it is a little more clear now what we mean now by the term linear algebra. Linear algebra is linear , because it just takes input values, our a a and b b for example, and multiplies them by constants . Everything is linear . Finally, it's algebra simply because it is a notation describing mathematical objects and a system of manipulating those notations. So linear algebra is a mathematical system for manipulating vectors in the spaces described by vectors . Note This is important! We are noticing some kind of deep connection between simultaneous equations, these things called matrices, and the vectors we were talking about last week. It turns that the key to solving simultaneous equation problems is appreciating how vectors are transformed by matrices, which is the heart of linear algebra.","title":"Conclusions"},{"location":"linear_algebra/week_3/#matrices-as-objects-that-operate-on-vectors","text":"","title":"Matrices as objects that operate on vectors"},{"location":"linear_algebra/week_3/#how-matrices-transform-space","text":"Tip Watch this video before reading through this section. So far, we have introduced the idea of a matrix and related it to the problem of solving simultaneous equations . We showed that the columns of a matrix can be thought of as the transformations applied to unit basis vector along each axis. This is a pretty profound idea, so lets flesh it out. We know that we can make any (2D) vector out of a vector sum of the scaled versions of \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 (our basis vectors). This means that the result of any linear transformation is just going to be some sum of the transformed basis vectors, ( \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 here). This is a bit hard to see but what it means is that the grid lines of our space stay parallel and evenly spaced . They might be stretched or sheared, but the origin stays where it is and there isn't any curviness to the space, it doesn't get warped --- a consequence of our scalar addition and multiplication rules for vectors. If we write down the matrix A A and the vector it is transforming as r r , we can represent our apples and bananas problem introduced earlier as: \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} A r = r' A r = r' Where r' r' is our transformed vector. We can generalize further, and add a scalar, n n : A (n r) = n r' A (n r) = n r' We notice that: A (r + s) = A r + A s A (r + s) = A r + A s Putting it together, we can represent any vector in 2D space as: A (n \\hat e_1 + m \\hat e_2) = n A \\hat e_1 + m A \\hat e_2 A (n \\hat e_1 + m \\hat e_2) = n A \\hat e_1 + m A \\hat e_2 \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;= n \\hat e_1' + m \\hat e_2' \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;= n \\hat e_1' + m \\hat e_2' Now let's try an example. Let, A = \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} A = \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} r = \\begin{bmatrix}3 \\\\\\ 2\\end{bmatrix} r = \\begin{bmatrix}3 \\\\\\ 2\\end{bmatrix} Then, A r = r' A r = r' \\Rightarrow \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}3 \\\\\\ 2\\end{bmatrix} = \\begin{bmatrix}12 \\\\\\ 32\\end{bmatrix} \\Rightarrow \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}3 \\\\\\ 2\\end{bmatrix} = \\begin{bmatrix}12 \\\\\\ 32\\end{bmatrix} Which is no different than how we might have multiplied matrices and vectors in school. But, we can think of this another way: A (n \\hat e_1 + m \\hat e_2) = r' A (n \\hat e_1 + m \\hat e_2) = r' \\Rightarrow \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\Biggl (3 \\begin{bmatrix}1 \\\\\\ 0\\end{bmatrix} + 2 \\begin{bmatrix}0 \\\\\\ 1\\end{bmatrix} \\Biggl) = \\Rightarrow \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\Biggl (3 \\begin{bmatrix}1 \\\\\\ 0\\end{bmatrix} + 2 \\begin{bmatrix}0 \\\\\\ 1\\end{bmatrix} \\Biggl) = 3 \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}1 \\\\\\ 0\\end{bmatrix} + 2 \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}0 \\\\\\ 1\\end{bmatrix} = 3 \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}1 \\\\\\ 0\\end{bmatrix} + 2 \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}0 \\\\\\ 1\\end{bmatrix} = = 3 \\begin{bmatrix}2 \\\\\\ 10\\end{bmatrix} + 2 \\begin{bmatrix}3 \\\\\\ 1\\end{bmatrix} = \\begin{bmatrix}12 \\\\\\ 32\\end{bmatrix} = 3 \\begin{bmatrix}2 \\\\\\ 10\\end{bmatrix} + 2 \\begin{bmatrix}3 \\\\\\ 1\\end{bmatrix} = \\begin{bmatrix}12 \\\\\\ 32\\end{bmatrix} The take home idea here is that the matrix A A just tells us where the basis vectors go . That's the transformation it does.","title":"How matrices transform space"},{"location":"linear_algebra/week_3/#types-of-matrix-transformation","text":"Lets illustrate the type of transformations we can perform with matrices with a number of examples. Note Remember, in linear algebra, linear transformations can be represented by matrices! We are only going to scratch the surface here and to continue to build up our intuition of viewing matrices as functions that apply transformations to some input vector . Identity matrix First, let's think about a matrix that doesn't change anything. Such a matrix is just composed of the basis vectors of the space, \\begin{bmatrix} 1 & 0 \\\\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} this is known as the identity matrix . It's the matrix that does nothing and leaves everything preserved, typically denoted I_m I_m where m m is the number of dimensions in our vector space. Scaling When our matrix contains values other than 0 0 in the diagnonal, we get a scaling : \\begin{bmatrix} m & 0 \\\\\\ 0 & n\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} mx \\\\\\ ny\\end{bmatrix} \\begin{bmatrix} m & 0 \\\\\\ 0 & n\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} mx \\\\\\ ny\\end{bmatrix} If m = 3 m = 3 and n=2 n=2 , visually this looks like: This transformation simply scales each dimension of the space by the value at the corresponding diagonal of the matrix. Note Note that when m \\lt 1 m \\lt 1 and/or n \\lt 1 n \\lt 1 , our space is actually compressed along the axes. Reflections When one or more of our diagonal values is negative, we get a reflection : \\begin{bmatrix} -1 & 0 \\\\\\ 0 & k\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} -x \\\\\\ ky\\end{bmatrix} \\begin{bmatrix} -1 & 0 \\\\\\ 0 & k\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} -x \\\\\\ ky\\end{bmatrix} In this case, our coordinate system is flipped across the vertical axis. When k = 2 k = 2 , visually this looks like: In another example, \\begin{bmatrix} -1 & 0 \\\\\\ 0 & -1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} -x \\\\\\ -y\\end{bmatrix} \\begin{bmatrix} -1 & 0 \\\\\\ 0 & -1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} -x \\\\\\ -y\\end{bmatrix} This transformation inverts all axes, and is known as an inversion . We can also produce mirror reflections over a straight line that crosses through the origin: \\begin{bmatrix} 0 & 1 \\\\\\ 1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} y \\\\\\ x\\end{bmatrix} \\begin{bmatrix} 0 & 1 \\\\\\ 1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} y \\\\\\ x\\end{bmatrix} or, \\begin{bmatrix} 0 & -1 \\\\\\ -1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} -y \\\\\\ -x\\end{bmatrix} \\begin{bmatrix} 0 & -1 \\\\\\ -1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} -y \\\\\\ -x\\end{bmatrix} Note Of course, we can also produce mirror transformations over the x x or y y axis as well, by making one of the diagonals 1 1 and the other -1 -1 . Shears Shears are visually similar to slanting . There are two possibilities: A shear parallel to the x x axis looks like: \\begin{bmatrix} 1 & k \\\\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x + ky \\\\\\ y\\end{bmatrix} \\begin{bmatrix} 1 & k \\\\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x + ky \\\\\\ y\\end{bmatrix} If k=1 k=1 , then visually this looks like: Or a shear parallel to the y y axis: \\begin{bmatrix} 1 & 0 \\\\\\ k & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\\\\\ kx + y\\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\\\ k & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\\\\\ kx + y\\end{bmatrix} Rotations Finally, we can rotate the space about the orign. For example, \\begin{bmatrix} 0 & -1 \\\\\\ 1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} y \\\\\\ -x\\end{bmatrix} \\begin{bmatrix} 0 & -1 \\\\\\ 1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} y \\\\\\ -x\\end{bmatrix} would rotate the entire space 90^0 90^0 counterclockwise. More generally, for a rotation by a angle \\theta \\theta clockwise about the orgin: \\begin{bmatrix} \\cos \\theta & \\sin \\theta \\\\\\ - \\sin \\theta & \\cos \\theta \\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\cos \\theta + y \\sin \\theta \\\\\\ - x \\sin \\theta + y \\cos \\theta\\end{bmatrix} \\begin{bmatrix} \\cos \\theta & \\sin \\theta \\\\\\ - \\sin \\theta & \\cos \\theta \\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\cos \\theta + y \\sin \\theta \\\\\\ - x \\sin \\theta + y \\cos \\theta\\end{bmatrix} and for a rotation by a angle \\theta \\theta counterclockwise about the orgin \\begin{bmatrix} \\cos \\theta & - \\sin \\theta \\\\\\ \\sin \\theta & \\cos \\theta \\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\cos \\theta - y \\sin \\theta \\\\\\ x \\sin \\theta + y \\cos \\theta\\end{bmatrix} \\begin{bmatrix} \\cos \\theta & - \\sin \\theta \\\\\\ \\sin \\theta & \\cos \\theta \\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\cos \\theta - y \\sin \\theta \\\\\\ x \\sin \\theta + y \\cos \\theta\\end{bmatrix}","title":"Types of matrix transformation"},{"location":"linear_algebra/week_3/#conclusions_1","text":"In this section, we described the major transformations that a matrix can perform on a vector. Next, we will look at how we can combine these transformations (known as composition ) to produce more complex matrix transformations","title":"Conclusions"},{"location":"linear_algebra/week_3/#composition-or-combination-of-matrix-transformations","text":"Tip Watch this video before reading through this section. So what is the point of introducing these different geometric transformations in this class? Well, if you want to do any kind of shape alteration, say of all the pixels in an image, or a face, then you can always make that shape change out of some combination of rotations , shears , stretches , and inverses . Note One example where these geometric transformations may be useful is in facial recognition , where we may preprocess every image by transforming it so that the person(s) face(s) are directly facing the camera. Lets illustrate this composition of matrix transformations with an example. Here, we will first apply a 90^o 90^o rotation clockwise about the x x -axis, and then a shear parallel to the x x -axis. Let the first transformation matrix be A_1 A_1 : \\begin{bmatrix} 0 & 1 \\\\\\ -1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} y \\\\\\ -x\\end{bmatrix} \\begin{bmatrix} 0 & 1 \\\\\\ -1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} y \\\\\\ -x\\end{bmatrix} And our second transformation matrix A_2 A_2 : \\begin{bmatrix} 1 & 1 \\\\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x + y \\\\\\ y\\end{bmatrix} \\begin{bmatrix} 1 & 1 \\\\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x + y \\\\\\ y\\end{bmatrix} Applying A_1 A_1 to our basis vectors \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 gives us \\hat e_1' \\hat e_1' and \\hat e_1' \\hat e_1' : \\hat e_1' = A_1 \\cdot \\hat e_1 = \\begin{bmatrix} 0 \\\\\\ -1\\end{bmatrix} \\hat e_1' = A_1 \\cdot \\hat e_1 = \\begin{bmatrix} 0 \\\\\\ -1\\end{bmatrix} \\hat e_2' = A_1 \\cdot \\hat e_2 = \\begin{bmatrix} 1 \\\\\\ 0\\end{bmatrix} \\hat e_2' = A_1 \\cdot \\hat e_2 = \\begin{bmatrix} 1 \\\\\\ 0\\end{bmatrix} Applying A_2 A_2 to our new transformed vectors \\hat e_1' \\hat e_1' and \\hat e_2' \\hat e_2' gives us \\hat e_1'' \\hat e_1'' and \\hat e_1'' \\hat e_1'' : \\hat e_1'' = A_2 \\cdot \\hat e_1' = \\begin{bmatrix}-1 \\\\\\ -1\\end{bmatrix} \\hat e_1'' = A_2 \\cdot \\hat e_1' = \\begin{bmatrix}-1 \\\\\\ -1\\end{bmatrix} \\hat e_2'' = A_2 \\cdot \\hat e_2' = \\begin{bmatrix} 1 \\\\\\ 0\\end{bmatrix} \\hat e_2'' = A_2 \\cdot \\hat e_2' = \\begin{bmatrix} 1 \\\\\\ 0\\end{bmatrix} Notice that if we stack our final transformed vectors \\hat e_1'' \\hat e_1'' and \\hat e_1'' \\hat e_1'' as columns we get the matrix: \\begin{bmatrix} -1 & 1 \\\\\\ -1 & 0\\end{bmatrix} \\begin{bmatrix} -1 & 1 \\\\\\ -1 & 0\\end{bmatrix} Which is equal to A_2 \\cdot A_1 A_2 \\cdot A_1 . Geometrically, this looks like: Conclusions The take home message here is that the transformation A_2 \\cdot (A_1 \\cdot r) A_2 \\cdot (A_1 \\cdot r) for some transformation matrices A_1 A_1 and A_2 A_2 and some vector r r , is equivalent to the transformation (A_2A_1) \\cdot r (A_2A_1) \\cdot r Note Note that A_2 \\cdot (A_1 \\cdot r) \\ne A_1 \\cdot (A_2 \\cdot r) A_2 \\cdot (A_1 \\cdot r) \\ne A_1 \\cdot (A_2 \\cdot r) , that is, the order in which we apply our transformations matters . As it turns out, the key to solving simultaneous equation problems is appreciating how vectors are transformed by matrices, which is at the the heart of linear algebra.","title":"Composition or combination of matrix transformations"},{"location":"linear_algebra/week_3/#matrix-inverses","text":"In this section, we are finally going to present a way to solve the apples and bananas problem with matrices. Along the way, we're going to find out about a thing called the inverse of a matrix and a method for finding it. Tip Watch this video before reading this section. For more practice with matrix inverses, see Khan Academy sections here and here .","title":"Matrix inverses"},{"location":"linear_algebra/week_3/#gaussian-elimination-solving-the-apples-and-bananas-problem","text":"First, recall our apples and bananas problem in matrix form: \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix} 8 \\\\\\ 3\\end{bmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix} 8 \\\\\\ 3\\end{bmatrix} Where we want to find the price of individual apples ( a a ) and bananas ( b b ). Simplifying, lets say that: A\\cdot r = s A\\cdot r = s Note This, in effect is saying: \" A A operates on vector r r to give us s s \". To solve for r r , we need to move A A to the other side of the equation. But how? . Well, to \"undo\" a division (and isolate x x ), you multiply by the reciprocal: \\frac{1}{2}x = 4 \\frac{1}{2}x = 4 \\Rightarrow x = 8 \\Rightarrow x = 8 Likewise, to undo a multiplication, we divide by the reciprocal: 2x = 4 2x = 4 \\Rightarrow x = 2 \\Rightarrow x = 2 How do we undo the transformation performed by A A ? The answer is to find the matrix A^{-1} A^{-1} (known as the inverse of A A ) such that: A^{-1}A = I A^{-1}A = I where I I is the identity matrix . We call A^{-1} A^{-1} the inverse of A A because is it reverses whatever transformation A A does, giving us back I I . We note that: A^{-1}A\\cdot r = A^{-1} s A^{-1}A\\cdot r = A^{-1} s \\Rightarrow\\ I \\cdot r = A^{-1} s \\Rightarrow\\ I \\cdot r = A^{-1} s \\Rightarrow\\ r = A^{-1} s \\Rightarrow\\ r = A^{-1} s So, if we could find the inverse of A A (i.e. find A^{-1} A^{-1} ), we can solve our problem (i.e. find a a and b b ). We can solve for A^{-1} A^{-1} with a series of row operations or substitutions (known as Gaussian elimination ). Let's look at a slightly more complicated problem to see how this is done: \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 15 \\\\\\ 21 \\\\\\ 13\\end{bmatrix} \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 15 \\\\\\ 21 \\\\\\ 13\\end{bmatrix} We start by subtracting row 1 from rows 2 and 3, which gives us a matrix in row echelon form (technically, reduced row echelon form ): \\begin{pmatrix}1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 15 \\\\\\ 6 \\\\\\ 2\\end{bmatrix} \\begin{pmatrix}1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 15 \\\\\\ 6 \\\\\\ 2\\end{bmatrix} We then perform two steps of back substitution to get the identify matrix: \\begin{pmatrix}1 & 1 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 9 \\\\\\ 4 \\\\\\ 2\\end{bmatrix} \\begin{pmatrix}1 & 1 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 9 \\\\\\ 4 \\\\\\ 2\\end{bmatrix} \\begin{pmatrix}1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 5 \\\\\\ 4 \\\\\\ 2\\end{bmatrix} \\begin{pmatrix}1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 5 \\\\\\ 4 \\\\\\ 2\\end{bmatrix} We can then read the solution right from the matrices a = 5 a = 5 , b = 4 b = 4 , c = 2 c = 2 . Tip If you are still feeling uneasy about using Gaussian elimination to solve a system of linear equations, see here for an example walked through step-by-step example. Full credit to PatrickJMT . Conclusions As it turns out, we don't have to compute the inverse at all to solve a system of linear equations. Although we showed the process of Gaussian elimination for some vectors r r and s s , we can use it in the general case to solve for any linear equation of the form A\\cdot r = s A\\cdot r = s . This actually one of the most computationally efficient ways to solve this problem, and it's going to work every time.","title":"Gaussian elimination: Solving the apples and bananas problem"},{"location":"linear_algebra/week_3/#from-gaussian-elimination-to-finding-the-inverse-matrix","text":"Now, let's think about how we can apply this idea of elimination to find the inverse matrix, which solves the more general problem no matter what vectors I write down on the right hand side. Say I have a 3 \\times \\times 3 matrix A A and its inverse B B ( B = A^{-1}) B = A^{-1}) . By the definition of the inverse , AB = BA = I_n AB = BA = I_n If we use our matrix A A from the last section: \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = I \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = I Note Where b_{ij} b_{ij} is the element at the i^{th} i^{th} row and j^{th} j^{th} column of matrix B B . we notice that the first column of B B is just a vector. It's a vector that describes what the B B matrix, the inverse of A A , does to space. Note Actually, it's the transformation that that vector does to the x-axis. This means that: \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{pmatrix}b_{11} \\\\\\ b_{21} \\\\\\ b_{31}\\end{pmatrix} = \\begin{pmatrix} 1 \\\\\\ 0 \\\\\\ 0\\end{pmatrix} \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{pmatrix}b_{11} \\\\\\ b_{21} \\\\\\ b_{31}\\end{pmatrix} = \\begin{pmatrix} 1 \\\\\\ 0 \\\\\\ 0\\end{pmatrix} Now, we could solve this by the elimination method and back substitution in just the way we did before. Then, we could do it again for the second column of B B , and finally the third. In this way, we would have solved for B B , in other words, we would have found A^{-1} A^{-1} . It turns out, we can actually solve for B B all at once. So lets do that: \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = I \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = I Subtract row 1 from row 2 and 3: \\Rightarrow \\begin{pmatrix}1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & -1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix}1 & 0 & 0 \\\\\\ -1 & 1 & 0 \\\\\\ -1 & 0 & 1\\end{pmatrix} \\Rightarrow \\begin{pmatrix}1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & -1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix}1 & 0 & 0 \\\\\\ -1 & 1 & 0 \\\\\\ -1 & 0 & 1\\end{pmatrix} Multiply row 3 by -1 -1 : \\Rightarrow \\begin{pmatrix}1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix}1 & 0 & 0 \\\\\\ -1 & 1 & 0 \\\\\\ 1 & 0 & -1\\end{pmatrix} \\Rightarrow \\begin{pmatrix}1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix}1 & 0 & 0 \\\\\\ -1 & 1 & 0 \\\\\\ 1 & 0 & -1\\end{pmatrix} Now that row 3 is in row echelon form , we can substitute it back into row 2 and row 1: \\Rightarrow \\begin{pmatrix}1 & 1 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} -2 & 0 & 3 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} \\Rightarrow \\begin{pmatrix}1 & 1 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} -2 & 0 & 3 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} And finally, back substitute row 2 into row 1: \\Rightarrow \\begin{pmatrix}1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} 0 & -1 & 2 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} \\Rightarrow \\begin{pmatrix}1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} 0 & -1 & 2 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} Because any matrix times its identity is that matrix itself: \\begin{pmatrix}1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} 0 & -1 & 2 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} \\begin{pmatrix}1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} 0 & -1 & 2 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} \\Rightarrow \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} 0 & -1 & 2 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} \\Rightarrow \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} 0 & -1 & 2 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} Note You can actually prove to yourself that we got the right answer by checking that A \\cdot B = I A \\cdot B = I . So that's our answer. We've found the identity matrix, B = A^{-1} B = A^{-1} for A A , and we did this by transforming A A into it's identity matrix via elimination and back substitution. Moreover, because B B could be any matrix, we have solved this in the general case. The solution is the same regardless of the number of dimensions, and this leads to a computationally efficient way to invert a matrix. Note There are computationally faster methods of computing the inverse, one such method is known as a decomposition process . In practice, you will simply call the solver of your problem or function, something like inv(A) , and it will pick the best method by inspecting the matrix you give it and return the answer. Conclusions We have figured out how to solve sets of linear equations in the general case, by a procedure we can implement in a computer really easily (known as Gaussian elimination ), and we've generalized this method to the to find the inverse of a matrix, regardless of what is on the right hand side of our system of equations.","title":"From Gaussian elimination to finding the inverse matrix"},{"location":"linear_algebra/week_3/#special-matrices","text":"In the final section of this week, we're going to look at a property of a matrix called the determinant . Note The determinant is a value that can be computed from the elements of a square matrix . The determinant of a matrix A A is denoted det(A) det(A) , det A det A , or \\vert A \\vert \\vert A \\vert . Geometrically, it can be viewed as the scaling factor of the linear transformation described by the matrix. We'll also look at what happens when a matrix doesn't have linearly independent basis vectors. Tip Watch this video before reading this section. For more practice with matrix inverses, see Khan Academy sections here and here .","title":"Special matrices"},{"location":"linear_algebra/week_3/#the-determinant","text":"Let's start by looking at the simple matrix: \\begin{pmatrix} a & 0 \\\\\\ 0 & d\\end{pmatrix} \\begin{pmatrix} a & 0 \\\\\\ 0 & d\\end{pmatrix} If we multiply this matrix by our basis vectors \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 , we get \\hat e_1' = \\begin{bmatrix} a \\\\\\ 0 \\end{bmatrix} \\hat e_1' = \\begin{bmatrix} a \\\\\\ 0 \\end{bmatrix} \\hat e_2' = \\begin{bmatrix} 0 \\\\\\ d \\end{bmatrix} \\hat e_2' = \\begin{bmatrix} 0 \\\\\\ d \\end{bmatrix} So, in plain english, we have stretched our x x -axis by a factor of a a and our y y -axis by a factor of d d and, therefore, scaled the area of the grid cells of the vector space by a factor of ad ad . Note This is a property of the fact that any linear transformation keeps grid lines parallel and evenly spaced. We call this number, ad ad the determinant . Now, if I instead have a matrix: \\begin{pmatrix} a & b \\\\\\ 0 & d\\end{pmatrix} \\begin{pmatrix} a & b \\\\\\ 0 & d\\end{pmatrix} then this is still going to stretch \\hat e_1 \\hat e_1 out by a factor of a a , but on the other axis, I am going to move \\hat e_2 \\hat e_2 hat to: \\hat e_2 = \\begin{pmatrix} b \\\\\\ d\\end{pmatrix} \\hat e_2 = \\begin{pmatrix} b \\\\\\ d\\end{pmatrix} What we have done is taken the original grid and stretched it along the x x -axis by a a and, along the y y -axis by d d and sheared it (parallel to the x x -axis) by b b . Note We've still changed the size, the scale of the space (which is what the determinant really is) by a factor of ad ad . Notice that the area defined by our transformed vectors \\hat e_1' \\hat e_1' and \\hat e_2' \\hat e_2' is still just the base times the perpendicular height, ad ad (the determinant). Lets flesh this out in the general case. Say we have the matrix: A = \\begin{pmatrix} a & b \\\\\\ c & d\\end{pmatrix} A = \\begin{pmatrix} a & b \\\\\\ c & d\\end{pmatrix} Multiplying this matrix by our basis vectors yields a parallelogram (stretched by a a and d d and sheared by b b and c c ). We can actual compute the area of this parallelogram as follows: Note We find the area of this parallelogram by finding the area of the whole box the encloses it, and subtracting off combined area of the the little bits around it. The exact method for solving the area is not important (although it is pretty trivial). What is important, is that the determinant of A A can be computed as \\vert A \\vert = ad - bc \\vert A \\vert = ad - bc , and that this computation has a geometric interpretation . Note This is the formula for the determinant of a square 2 \\times 2 2 \\times 2 matrix. See here for the formula for higher dimensional matrices. Now, in school when you looked at matrices, you probably saw that you could find the inverse in the following way. For a matrix: A = \\begin{pmatrix} a & b \\\\\\ c & d\\end{pmatrix} A = \\begin{pmatrix} a & b \\\\\\ c & d\\end{pmatrix} Exchange a a and the d d , and switch the sign on the b b and the c c Multiply A A by this matrix Scale the transformation by \\frac{1}{ad - bc} \\frac{1}{ad - bc} In the general case, this looks like: \\frac{1}{ad - bc} \\begin{pmatrix} a & b \\\\\\ c & d\\end{pmatrix} \\begin{pmatrix} d & -b \\\\\\ -c & a\\end{pmatrix} = \\frac{1}{ad - bc} \\begin{pmatrix} ad - bc & 0 \\\\\\ 0 & ad - bc\\end{pmatrix} = I \\frac{1}{ad - bc} \\begin{pmatrix} a & b \\\\\\ c & d\\end{pmatrix} \\begin{pmatrix} d & -b \\\\\\ -c & a\\end{pmatrix} = \\frac{1}{ad - bc} \\begin{pmatrix} ad - bc & 0 \\\\\\ 0 & ad - bc\\end{pmatrix} = I This demonstrates that \\frac{1}{ad - bc} \\begin{pmatrix} d & -b \\\\\\ -c & a\\end{pmatrix} \\frac{1}{ad - bc} \\begin{pmatrix} d & -b \\\\\\ -c & a\\end{pmatrix} is in fact the inverse of the matrix A A . This helps capture what the determinant really is. It's the amount by which the original matrix scaled vector space. In the above example, dividing by the determinant normalizes the space back to its original size. Note We could spend a lot of time talking about how to solve for the determinant. However, knowing how to do the operations isn't really a useful skill. Many programming libraries (e.g. python ) have linear algebra libraries (e.g. Numpy ) which makes computing the determinant as easy, for example, as calling det(A) . If you really want to know how to compute determinants by hand, then look up a QR decomposition online.","title":"The determinant"},{"location":"linear_algebra/week_3/#a-determinant-of-zero","text":"Now, let's think about the matrix A = \\begin{pmatrix} 1 & 2 \\\\\\ 1 & 2\\end{pmatrix} A = \\begin{pmatrix} 1 & 2 \\\\\\ 1 & 2\\end{pmatrix} If we multiply this matrix by our basis vectors \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 , we get \\hat e_1' = \\begin{bmatrix} 1 \\\\\\ 1 \\end{bmatrix} \\hat e_1' = \\begin{bmatrix} 1 \\\\\\ 1 \\end{bmatrix} \\hat e_2' = \\begin{bmatrix} 2 \\\\\\ 2 \\end{bmatrix} \\hat e_2' = \\begin{bmatrix} 2 \\\\\\ 2 \\end{bmatrix} So this matrix, when applied to our vector space, actually collapses it onto a line . All our y y 's are mapped to the vector \\begin{bmatrix}2 \\\\\\ 2 \\end{bmatrix} \\begin{bmatrix}2 \\\\\\ 2 \\end{bmatrix} and our x x 's to \\begin{bmatrix}1 \\\\\\ 1 \\end{bmatrix} \\begin{bmatrix}1 \\\\\\ 1 \\end{bmatrix} Correspondingly, we notice that \\vert A \\vert = 0 \\vert A \\vert = 0 . Therefore, the area enclosed by the new basis vectors is zero .","title":"A determinant of zero"},{"location":"linear_algebra/week_3/#a-negative-determinant","text":"A negative determinant simply means that the transformation has flipped the orientation of our vector space. This is much easier to see than to explain ; check out this video which presents some awesome visualizations of the determinant.","title":"A negative determinant"},{"location":"linear_algebra/week_3/#what-the-determinant-means-numerically","text":"To drive home the numerical interpretation of the determinant, lets start with this set of simultaneous equations: \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 2 & 3 & 7\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 17 \\\\\\ 29\\end{pmatrix} \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 2 & 3 & 7\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 17 \\\\\\ 29\\end{pmatrix} You'll notice that both the rows and the columns are linearly dependent . Thinking about the columns of this matrix as the basis vectors of some 3-dimensional space, we note that this transformation collapses our vector space from being 3D to 2D (by collapsing every point in space onto a plane). Let's see what this means numerically by trying to reduce our matrix to row-echelon form: \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 2 & 3 & 7\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 17 \\\\\\ 29\\end{pmatrix} \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 2 & 3 & 7\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 17 \\\\\\ 29\\end{pmatrix} Subtract row 1 from row 2, \\Rightarrow \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 2 & 3 & 7\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 5 \\\\\\ 29\\end{pmatrix} \\Rightarrow \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 2 & 3 & 7\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 5 \\\\\\ 29\\end{pmatrix} Subtract row 1 plus row 2 from row 3, \\Rightarrow \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & 0\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 5 \\\\\\ 0\\end{pmatrix} \\Rightarrow \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & 0\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 5 \\\\\\ 0\\end{pmatrix} while the matrix is now is row-echelon form, we don't have a final entry of the matrix ( 0\\cdot c = 0 0\\cdot c = 0 ). Because we don't have a solution for c c we can't back substitute, and we can't solve our system of equations. Note The reason we can't solve this system is because we don't have enough information. In keeping with our apples and bananas problem, imagine that when we went in to buy apples and bananas and carrots the third time, we ordered just a the sum of first two orders. Therefore, we didn't get any new information and thus don't have enough data to find out the solution for how much apples and bananas and carrots cost. This third order wasn't linearly independent from our first two, in the language of matrices and vectors. So, when the basis vectors describing a matrix aren't linearly independent, then the determinant is zero , and we can't solve the system. The loss of information when we map from n n -dimensional to (n-x) (n-x) -dimensional space (where x \\ge 1 x \\ge 1 ) means we cannot possibly know what the inverse matrix is (as it is impossible to map a lower dimensional space back to the original, higher dimensional space). When a matrix has no inverse, we say that it is singular . Note There are situations where we might want to do a transformation that collapses the number of dimensions in a space, but it means that we cannot possibly reverse the mapping, meaning the matrix has no inverse. This also means we cannot solve a system of linear equations defined by a singular matrix using Gaussian elimination and back substitution.","title":"What the determinant means numerically"},{"location":"linear_algebra/week_3/#summary","text":"In the last section of this week, we took a look at the determinant, which is how much a given transformation scales our space. In 2-dimensions, this can be thought as the scalar multiple appleid to any area of our space, and in 3-dimensions any volume of our space. We also looked at the special case where the determinant is zero and found that this means that the basis vectors aren't linearly independent, which in turn means that the inverse doesn't exist. To summarize Week 3 , we introduced matrices as objects that transforms space. looked at different archetypes of matrices, like rotations , inverses , stretches , and shears , how to combine matrices by doing successive transformations, known as m atrix multiplication or composition how to solve systems of linear equations by elimination and how to find inverses and finally, we introduced determinants and showed how that relates to the concept of linear independence .","title":"Summary"},{"location":"linear_algebra/week_4/","text":"Week 4: Matrices Make Linear Mappings In Module 4, we continue our discussion of matrices; first we think about how to code up matrix multiplication and matrix operations using the Einstein Summation Convention, which is a widely used notation in more advanced linear algebra courses. Then, we look at how matrices can transform a description of a vector from one basis (set of axes) to another. This will allow us to, for example, manipulate images. We'll also look at how to construct a convenient basis vector set in order to do such transformations. Then, we'll write some code to do these transformations and apply this work computationally. Learning Objectives Identify matrices as operators Relate the transformation matrix to a set of new basis vectors Formulate code for mappings based on these transformation matrices Write code to find an orthonormal basis set computationally Matrices as objects that map one vector onto another Introduction to Einstein summation convention and the symmetry of the dot product There is a different, important way to write matrix transformations that we have not yet discussed. It's called the Einstein's Summation Convention . In this convention, we write down the actual operations on the elements of a matrix, which is useful when you're coding or programming. It also lets us see something neat about the dot product, and it lets us deal with non-square matrices. When we started, we said that multiplying a matrix by a vector or with another matrix is a process of taking every element in each row of the first matrix multiplied with the corresponding element in each column of the other matrix, summing the products and putting them in place. In Einstein's Summation Convention, we represent the matrix product C = AB C = AB : \\begin{pmatrix} a_{11} & a_{12} & . & . & . & a_{1n} \\\\\\ a_{21} & a_{22} & & & & . &\\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ a_{n1} & . & . & . & . & a_{nm}\\end{pmatrix} \\begin{pmatrix} b_{11} & b_{12} & . & . & . & b_{1m} \\\\\\ b_{21} & b_{22} & & & & . &\\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ b_{m1} & . & . & . & . & b_{mp}\\end{pmatrix} = AB \\begin{pmatrix} a_{11} & a_{12} & . & . & . & a_{1n} \\\\\\ a_{21} & a_{22} & & & & . &\\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ a_{n1} & . & . & . & . & a_{nm}\\end{pmatrix} \\begin{pmatrix} b_{11} & b_{12} & . & . & . & b_{1m} \\\\\\ b_{21} & b_{22} & & & & . &\\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ b_{m1} & . & . & . & . & b_{mp}\\end{pmatrix} = AB as, c_{ij} = a_{i1}b_{1j} + ... + a_{im}b_{mj} = \\sum_{k=1}^m a_{ik}b_{kj} c_{ij} = a_{i1}b_{1j} + ... + a_{im}b_{mj} = \\sum_{k=1}^m a_{ik}b_{kj} Note To be clear, its the c_{ij} c_{ij} itself that is written Einstein's Summation Convention, not everything that comes to the right of the = = sign. For i = 1, ..., n i = 1, ..., n and j = 1, ..., p j = 1, ..., p , where A A is a n \\times m n \\times m matrix, B B is a m \\times p m \\times p matrix, and C C is a n \\times p n \\times p matrix. This is useful when we are implementing matrix multiplication in code, because it makes it obvious exactly what operations we need to perform. In this case, it should be obvious that we can run three loops over i i , j j and k k , and then use an accumulator on the k k 's to find the elements of the product matrix AB AB . Note We haven't talked about this yet, but now we can see it clearly. There's no reason, so long as the matrices have the same number of entries in k k , that the matrices we multiply need to be the same shape! Let's revisit the dot product in light of the Einstein Summation Convention. If we've got two vectors, let's call them u u and v v , where u u is a column vector having elements u_i u_i and v v is another column vector having elements v_i v_i . \\begin{pmatrix}u_i \\\\\\ \\vdots \\\\\\ u_{n}\\end{pmatrix} \\cdot \\begin{pmatrix}v_i \\\\\\ \\vdots \\\\\\ v_n \\end{pmatrix} \\begin{pmatrix}u_i \\\\\\ \\vdots \\\\\\ u_{n}\\end{pmatrix} \\cdot \\begin{pmatrix}v_i \\\\\\ \\vdots \\\\\\ v_n \\end{pmatrix} When we dot them together, we are computing the following: u \\cdot v = \\sum^n_{i=1}u_iv_i = u_1v_1 + u_2v_2 + ... + u_nv_n u \\cdot v = \\sum^n_{i=1}u_iv_i = u_1v_1 + u_2v_2 + ... + u_nv_n Notice that, \\begin{pmatrix}u_i \\\\\\ \\vdots \\\\\\ u_{n}\\end{pmatrix} \\cdot \\begin{pmatrix}v_i \\\\\\ \\vdots \\\\\\ v_n \\end{pmatrix} = \\begin{pmatrix}u_i & \\cdots & u_{n}\\end{pmatrix} \\cdot \\begin{pmatrix}v_i \\\\\\ \\vdots \\\\\\ v_n \\end{pmatrix} \\begin{pmatrix}u_i \\\\\\ \\vdots \\\\\\ u_{n}\\end{pmatrix} \\cdot \\begin{pmatrix}v_i \\\\\\ \\vdots \\\\\\ v_n \\end{pmatrix} = \\begin{pmatrix}u_i & \\cdots & u_{n}\\end{pmatrix} \\cdot \\begin{pmatrix}v_i \\\\\\ \\vdots \\\\\\ v_n \\end{pmatrix} and so we notice that there's some equivalence between a matrix transformation (or multiplication ) and the dot product . Lets explore this in more detail. Symmetry of the dot product Say we have the vector \\hat u \\hat u , with components u_1 u_1 and u_2 u_2 . Let's imagine what happens if we dot \\hat u \\hat u with the basis vector \\hat e_1 \\hat e_1 . We know from previous sections that this gives us the length of the projection of \\hat u_1 \\hat u_1 on \\hat e_1 \\hat e_1 multiplied by the norm of \\hat e_1 \\hat e_1 . But what if we do the reverse? What if we dot \\hat e_1 \\hat e_1 with \\hat u \\hat u ? We already know that numerically, the result will be the same, as the dot product is commutative . So geometrically, we can imagine drawing a line of symmetry between the point where the two projections cross: Previously, we stated (without proof, although the numerical proof is trivial) that the dot product is commutative, and now, we have shown geometrically why that is true. Conclusions In this section, we introduced Einstein's Summation Convention , which is a compact and computationally useful (but not very visually intuitive) way to write down matrix operations. This led to a discussion on the similarities between the dot product and matrix multiplication, where we noticed a connection between matrix multiplication , and the dot product , which itself has a geometric understanding as the concept of projection , i.e. projecting one vector onto another. This allows us to think about matrix multiplication with a vector as being the projection of that vector onto the vectors composing the matrix (i.e. the columns of the matrix). Matrices transform into the new basis vector set Tip Watch this video before reading this section. For more practice with changing basis, see this Khan Academy section. Matrices changing basis We have said before that the columns of a transformation matrix are the axes of the new basis vectors after applying the mapping. We're now going to spend a little time looking at how to transform a vector from one set of basis vectors to another . Let's say we have two sets of basis vectors, which define a first coordinate system ( \\text{CS}_1) \\text{CS}_1) and a second coordinate system ( \\text{CS}_2) \\text{CS}_2) . Let the basis vectors of \\text{CS}_2 \\text{CS}_2 , from the perspective of \\text{CS}_1 \\text{CS}_1 be: \\hat b_1 = \\begin{bmatrix}3 \\\\\\ 1\\end{bmatrix}, \\hat b_2 = \\begin{bmatrix}1 \\\\\\ 1\\end{bmatrix} \\hat b_1 = \\begin{bmatrix}3 \\\\\\ 1\\end{bmatrix}, \\hat b_2 = \\begin{bmatrix}1 \\\\\\ 1\\end{bmatrix} Lets package these basis vectors into a matrix \\text{CS}_{21} \\text{CS}_{21} , for convenience \\text{CS}_{21} = \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\text{CS}_{21} = \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} Think of these as the basis vectors of \\text{CS}_2 \\text{CS}_2 as they would appear in \\text{CS}_1 \\text{CS}_1 . If we wanted to change the basis of any vectors in \\text{CS}_2 \\text{CS}_2 to \\text{CS}_1 \\text{CS}_1 , we simply do: \\text{CS}_{21} \\cdot \\text{vector in CS}_2 = \\text{vector in CS}_1 \\text{CS}_{21} \\cdot \\text{vector in CS}_2 = \\text{vector in CS}_1 E.g., for the vector \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} defined in terms of \\text{CS}_2 \\text{CS}_2 \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} = \\begin{bmatrix}5 \\\\\\ 2\\end{bmatrix} \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} = \\begin{bmatrix}5 \\\\\\ 2\\end{bmatrix} That is, a vector \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} described in \\text{CS}_2 \\text{CS}_2 , is described as \\begin{bmatrix}5 \\\\\\ 2\\end{bmatrix} \\begin{bmatrix}5 \\\\\\ 2\\end{bmatrix} in \\text{CS}_1 \\text{CS}_1 . Why does this make sense? Well, you can think of \\text{CS}_{21} \\text{CS}_{21} as the transformation that takes the basis vectors of \\text{CS}_1 \\text{CS}_1 and moves them to the positions of the basis vectors of \\text{CS}_2 \\text{CS}_2 . Applying this transformation to a vector in \\text{CS}_2 \\text{CS}_2 , therefore, gives us the corresponding vector in \\text{CS}_1 \\text{CS}_1 . Now, how do we do the reverse? How do we translate a vector in \\text{CS}_1 \\text{CS}_1 to a vector in \\text{CS}_2 \\text{CS}_2 ? All we need to do to change basis in the reverse case is to multiply a vector in one coordinate system by the inverse of the matrix containing the basis vectors of another: \\text{CS}^{-1}_{21} \\cdot \\text{vector in CS}_1 = \\text{vector in CS}_2 \\text{CS}^{-1}_{21} \\cdot \\text{vector in CS}_1 = \\text{vector in CS}_2 E.g., \\frac{1}{2}\\begin{bmatrix}1 & -1 \\\\\\ -1 & 3\\end{bmatrix}\\begin{bmatrix}5 \\\\\\ 2\\end{bmatrix} = \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} \\frac{1}{2}\\begin{bmatrix}1 & -1 \\\\\\ -1 & 3\\end{bmatrix}\\begin{bmatrix}5 \\\\\\ 2\\end{bmatrix} = \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} Notice that this process gave us the coordinates of the vector \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} in \\text{CS}_2 \\text{CS}_2 , which is equal to the vector that we started with. Generalizing This is a little bit tricky, so lets generalize. Imagine we have two coordinate systems, \"ours\" and \"theirs\". We can translate vectors in \"their\" coordinate system to \"our\" coordinate system by applying a transformation A A , where A A is a matrix whose columns contain the basis vectors of \"their\" coordinate system as they appear in \"our coordinate system\": \\text{(1) } A \\cdot \\text{vector in \"their\" coordinate system} = \\text{vector in \"our\" coordinate system} \\text{(1) } A \\cdot \\text{vector in \"their\" coordinate system} = \\text{vector in \"our\" coordinate system} To do the reverse, i.e. take a vector in \"our\" coordinate system and translate it to \"their\" coordinate system, we simply multiply \"our\" vector by the inverse of A \\text{(2) } \\text{vector in \"their\" coordinate system} = A^{-1} \\cdot \\text{vector in \"our\" coordinate system} \\text{(2) } \\text{vector in \"their\" coordinate system} = A^{-1} \\cdot \\text{vector in \"our\" coordinate system} It should be obvious now why this is the case, we simply moved A A over in the (1) to get (2)! Orthonormal basis set When we discussed vectors and projections, we said that if the new basis vectors were orthogonal then we could use projection to easily change basis. Note see Changing basis for a fleshed out example. Summary Not orthogonal, use matrix multiplication. Orthogonal, use projection product. Doing a transformation in a changed basis Tip Watch the last little bit of this video first. Lets discuss the process of applying a transformation in a changed basis. Say again that the basis vectors of coordinate system \\text{CS}_2 \\text{CS}_2 from the perspective of coordinate system \\text{CS}_1 \\text{CS}_1 are: \\text{CS}_{21} = \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\text{CS}_{21} = \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} Say further that we have a vector in \\text{CS}_2 \\text{CS}_2 that we want to transform: {c_2} = \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} {c_2} = \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} And the tranformation we want to apply is: N = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\\\ 1 & 1\\end{bmatrix} N = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\\\ 1 & 1\\end{bmatrix} Note This rotates the vector space 45^0 45^0 counter-clockwise. How do we apply the transformation N N to a vector defined by the coordinate system \\text{CS}_2 \\text{CS}_2 ? The first thing to do is take the vector {c_2} {c_2} and multiply it by \\text{CS}_{21} \\text{CS}_{21} , that is, change the basis of the vector {c_2} {c_2} from \\text{CS}_2 \\text{CS}_2 to \\text{CS}_1 \\text{CS}_1 : {c_1} = \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} {c_1} = \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} Then, we can apply the transformation: {c_1}' = R \\cdot \\text{CS}_{21} \\cdot {c_2} {c_1}' = R \\cdot \\text{CS}_{21} \\cdot {c_2} {c_1}' = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} {c_1}' = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}2 & 0 \\\\\\ 4 & 2\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}2 & 0 \\\\\\ 4 & 2\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} What if, once we obtained this output vector, we wanted to change its basis back to \\text{CS}_2 \\text{CS}_2 ? Recall from the last section that we multiply the whole thing by the inverse of \\text{CS}_{21}^{-1} \\text{CS}_{21}^{-1} {c_2}' = \\text{CS}_{21}^{-1} \\cdot R \\cdot \\text{CS}_{21} \\cdot {c_2} {c_2}' = \\text{CS}_{21}^{-1} \\cdot R \\cdot \\text{CS}_{21} \\cdot {c_2} {c_2}' = \\frac{1}{2} \\begin{bmatrix}1 & -1 \\\\\\ -1 & 3\\end{bmatrix} \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} {c_2}' = \\frac{1}{2} \\begin{bmatrix}1 & -1 \\\\\\ -1 & 3\\end{bmatrix} \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} = \\frac{1}{2} \\begin{bmatrix}1 & -1 \\\\\\ -1 & 3\\end{bmatrix}\\frac{1}{\\sqrt{2}}\\begin{bmatrix}2 & 0 \\\\\\ 4 & 2\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} = \\frac{1}{2} \\begin{bmatrix}1 & -1 \\\\\\ -1 & 3\\end{bmatrix}\\frac{1}{\\sqrt{2}}\\begin{bmatrix}2 & 0 \\\\\\ 4 & 2\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}-1 & -1 \\\\\\ 5 & 3\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}-1 & -1 \\\\\\ 5 & 3\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} This operation essentially builds on the previous operation to return the transformed output vector, {c_1}' {c_1}' relative to \\text{CS}_2 \\text{CS}_2 , that is it returns {c_2}' {c_2}' , where {c_2}' {c_2}' is where c_2 c_2 ends up after in the basis \\text{CS}_2 \\text{CS}_2 after some transformation M M has been applied. Generalizing Again, this is tricky, so lets generalize. Imagine we have two coordinate systems, \"ours\" and \"theirs\". If we are given: a vector with its basis in \"their\" coordinate system A A , a matrix whose columns are the basis vectors of \"their\" coordinate system as they appear in \"our\" coordinate system M M some transformation, with its basis in \"our\" coordinate system Then, Change the vector in \"their\" basis to \"ours\": A \\cdot \\text{vector in \"their\" coordinate system} A \\cdot \\text{vector in \"their\" coordinate system} Apply the transformation in \"our\" coordinate system: M \\cdot A \\cdot \\text{vector in \"their\" coordinate system} M \\cdot A \\cdot \\text{vector in \"their\" coordinate system} Change the resulting vector back to \"their\" coordinate system: A^{-1} \\cdot M \\cdot A \\cdot \\text{vector in \"their\" coordinate system} A^{-1} \\cdot M \\cdot A \\cdot \\text{vector in \"their\" coordinate system} In sum, the transformation A^{-1}MA A^{-1}MA will take any vector in \"their\" coordinate system, apply some transformation in \"our\" coordinate system, and return the resulting vector in \"their\" coordinate system. Making multiple mappings, deciding if these are reversible Orthogonal matrices It is very useful to compose a transformation matrix whose column and row vectors make up a new basis , with the additional constraint of making all of these component vectors orthogonal. Such a square matrix of orthonormal columns and rows is known as an orthogonal matrix . In this section we are going to look at how to construct such a matrix, and why it's useful. (Aside) Transpose First, we need to define a new matrix operation called the transpose . The transpose of a matrix is an operator which flips a matrix over its diagonal , that is it switches the row and column indices of the matrix by producing another matrix denoted as A^T A^T . It is achieved by any one of the following equivalent actions: reflect A A over its main diagonal (which runs from top-left to bottom-right) to obtain A^T A^T , write the rows of A A as the columns of A^T A^T , write the columns of A A as the rows of A^T A^T . Formally, the i -th row, j -th column element of A^T A^T is the j -th row, i -th column element of A A : [A^T]_{ij} = [A]_{ji} [A^T]_{ij} = [A]_{ji} Now let's imagine I have a n \\times n n \\times n matrix A A , with a series of column vectors which are going to be the basis vectors of the some new transformed vector space: A = \\begin{pmatrix} \\begin{pmatrix}\\vdots \\\\\\ \\hat a_1 \\\\\\ \\vdots\\end{pmatrix} & \\begin{pmatrix} \\vdots \\\\\\ \\hat a_2\\\\\\ \\vdots \\end{pmatrix} & \\dots \\begin{pmatrix} \\vdots \\\\\\ \\hat a_n\\\\\\ \\vdots \\end{pmatrix} \\end{pmatrix} A = \\begin{pmatrix} \\begin{pmatrix}\\vdots \\\\\\ \\hat a_1 \\\\\\ \\vdots\\end{pmatrix} & \\begin{pmatrix} \\vdots \\\\\\ \\hat a_2\\\\\\ \\vdots \\end{pmatrix} & \\dots \\begin{pmatrix} \\vdots \\\\\\ \\hat a_n\\\\\\ \\vdots \\end{pmatrix} \\end{pmatrix} Lets place two more constaints on this matrix A A : First, the column vectors a_i a_i have unit length Second, the column vectors a_i a_i are orthogonal to each other. Note That is, \\hat a_i \\cdot \\hat a_j = 0 \\hat a_i \\cdot \\hat a_j = 0 for \\forall i \\ne j \\forall i \\ne j and \\hat a_i \\cdot \\hat a_i = 1 \\hat a_i \\cdot \\hat a_i = 1 . Lets think about what happens when we multiply A A by its transpose, A^T A^T : A^TA = \\begin{pmatrix} \\begin{pmatrix}\\dots & \\hat a_1 & \\dots\\end{pmatrix} \\\\\\ \\begin{pmatrix} \\dots & \\hat a_2 & \\dots \\end{pmatrix} \\\\\\ \\vdots \\\\\\ \\begin{pmatrix} \\dots & \\hat a_n & \\dots \\end{pmatrix} \\end{pmatrix} \\begin{pmatrix} \\begin{pmatrix}\\vdots \\\\\\ \\hat a_1 \\\\\\ \\vdots\\end{pmatrix} & \\begin{pmatrix} \\vdots \\\\\\ \\hat a_2\\\\\\ \\vdots \\end{pmatrix} & \\dots \\begin{pmatrix} \\vdots \\\\\\ \\hat a_n\\\\\\ \\vdots \\end{pmatrix} \\end{pmatrix} = I_n A^TA = \\begin{pmatrix} \\begin{pmatrix}\\dots & \\hat a_1 & \\dots\\end{pmatrix} \\\\\\ \\begin{pmatrix} \\dots & \\hat a_2 & \\dots \\end{pmatrix} \\\\\\ \\vdots \\\\\\ \\begin{pmatrix} \\dots & \\hat a_n & \\dots \\end{pmatrix} \\end{pmatrix} \\begin{pmatrix} \\begin{pmatrix}\\vdots \\\\\\ \\hat a_1 \\\\\\ \\vdots\\end{pmatrix} & \\begin{pmatrix} \\vdots \\\\\\ \\hat a_2\\\\\\ \\vdots \\end{pmatrix} & \\dots \\begin{pmatrix} \\vdots \\\\\\ \\hat a_n\\\\\\ \\vdots \\end{pmatrix} \\end{pmatrix} = I_n So what we notice is that in the case where A A is composed of vectors that are normal to each other and have unit length , (i.e. when they're orthonormal ), then A^TA = I A^TA = I . Stated another way, A^T A^T in this situation is actually the inverse of A A ! This special case is known as an orthogonal matrix . Another thing to note is that because all the basis vectors are of unit length, it must scale space by a factor of one . Stated another way, the determinant of an orthogonal matrix must be either plus or minus one. \\vert A \\vert = \\pm 1 \\vert A \\vert = \\pm 1 Where the minus one arises if the new basis vector set flip space around (from right-handed to left-handed or vice versa). Notice that if A^T A^T , the inverse of A A , then by the definition of the inverse: A^TA = AA^T = I_n A^TA = AA^T = I_n So, we could pre- or post- multiply and still get the identity. This means that the rows of the orthogonal matrix are also orthonormal to each other! So, the transpose matrix of an orthogonal basis set, is itself another orthogonal basis set. Now, remember that in the last module on vectors, we said that transforming a vector onto a new coordinate system was as easy as taking the projection or dot product of that vector onto each of the new bases vectors, as long as they were orthogonal to each other . So, if we have a vector r r and we want to project r r into a new set of axes, let's call them \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 , as long as these vectors are orthogonal to each other, then we can project into the new vector space just by taking the dot product of r r with \\hat e_2 \\hat e_2 , and the dot product of r r with \\hat e_1 \\hat e_1 , and then we'd have its components in the new set of axis. Note See Changing basis for a walked-through example. Conclusions In this section, we introduced the most convenient basis vector set of all, the orthonormal bases vector set . Wherever possible we want to use an orthonormal basis vector set represented as a orthogonal matrix A A . This set of vectors has the following properties and consequences that make it easy to work with: the transpose of such a matrix will be its inverse , which makes the inverse incredibly easy to compute, the transformation will be reversible because space doesn't get collapsed by any dimensions, the projections (i.e. the result of computing the projection of a vector onto the matrix A A ) are just the dot products. One final note. If we arrange the bases vectors in the correct order, then the determinant will be one . \\vert A \\vert = 1 \\vert A \\vert = 1 An easy way to check if they aren't in the right order, is to check if the determinant is minus one. This means we've transformed our space from right to left handed orientation. All we have to do to remedy this is to exchange a pair of vectors in A A such that \\vert A \\vert = 1 \\vert A \\vert = 1 . Recognizing mapping matrices and applying these to data The Gram\u2013Schmidt process In the last section, we motivated the idea that life is much easier if we can construct an orthonormal basis vector set, but we haven't talked about how to do it. In this section, we will explore just that. We'll start from the assumption that we already have some linearly independent vectors that span the space we're interested in. Say we have some such vectors V = \\{v_1, v_2, ..., v_n\\} V = \\{v_1, v_2, ..., v_n\\} , Note If you want to check linear independence, you can write down your vectors as the the columns in a matrix and check that the determinant of that matrix isn't zero. but they aren't orthogonal to each other or of unit length. Our life would probably be easier if we could construct some orthonormal basis. As it turns out, there's a process for doing just that which is called the Gram-Schmidt process . Let's take the first vector in the set to be v_1 v_1 . In this first step, we're just going to normalize v_1 v_1 to get our eventual first basis vector e_1 e_1 e_1 = \\frac{v_1}{\\vert v_1 \\vert} e_1 = \\frac{v_1}{\\vert v_1 \\vert} Now, we can think of v_2 v_2 as being composed of two things: a component that is in the direction of e_1 e_1 and a component that's perpendicular to e_1 e_1 . We can find the component that's in the direction of e_1 e_1 by taking the vector projection v_2 v_2 onto e_1 e_1 : v_2 = (v_2 \\cdot e_1) \\frac{e_1}{\\vert e_1 \\vert} v_2 = (v_2 \\cdot e_1) \\frac{e_1}{\\vert e_1 \\vert} Note |e_1| |e_1| is 1 so we could actually omit it. If we subtract this vector projection from v_2 v_2 we get u_2 u_2 , a vector which is orthogonal to e_1 e_1 : u_2 = v_2 - (v_2 \\cdot e_1) e_1 u_2 = v_2 - (v_2 \\cdot e_1) e_1 Finally, dividing u_2 u_2 by its length gives us e_2 e_2 , the unit vector orthogonal to e_1 e_1 : e_2 = \\frac{u_2}{\\vert u_2 \\vert} e_2 = \\frac{u_2}{\\vert u_2 \\vert} We could continue this process for all vectors in our set V V . The general formula (in pseudocode) is: # For all vectors in our set V for i in | V | : # For all vectors in our set V that come before v_i for j in i : # Subtract the component of v_i in the direction of the previous vectors v_j v_i = v_i - v_i . dot ( v_j ) * v_j # If |v_i| is not zero, normalize it to unit length. Otherwise it is linearly dependent on a # previous vector, so set it equal to the zero vector. if | v_i | !!! note 0 : v_i = v_i / | v_i | else : v_i = zero_vector Conclusions So that's how we construct an orthonormal basis set, which makes our lives much easier for all the reasons we discussed here . Reflecting in a plane This is a rather involved example, and is probably best if you just watch it yourself here . If I can find the time, i'll make notes for the video!","title":"Week 4"},{"location":"linear_algebra/week_4/#week-4-matrices-make-linear-mappings","text":"In Module 4, we continue our discussion of matrices; first we think about how to code up matrix multiplication and matrix operations using the Einstein Summation Convention, which is a widely used notation in more advanced linear algebra courses. Then, we look at how matrices can transform a description of a vector from one basis (set of axes) to another. This will allow us to, for example, manipulate images. We'll also look at how to construct a convenient basis vector set in order to do such transformations. Then, we'll write some code to do these transformations and apply this work computationally. Learning Objectives Identify matrices as operators Relate the transformation matrix to a set of new basis vectors Formulate code for mappings based on these transformation matrices Write code to find an orthonormal basis set computationally","title":"Week 4: Matrices Make Linear Mappings"},{"location":"linear_algebra/week_4/#matrices-as-objects-that-map-one-vector-onto-another","text":"","title":"Matrices as objects that map one vector onto another"},{"location":"linear_algebra/week_4/#introduction-to-einstein-summation-convention-and-the-symmetry-of-the-dot-product","text":"There is a different, important way to write matrix transformations that we have not yet discussed. It's called the Einstein's Summation Convention . In this convention, we write down the actual operations on the elements of a matrix, which is useful when you're coding or programming. It also lets us see something neat about the dot product, and it lets us deal with non-square matrices. When we started, we said that multiplying a matrix by a vector or with another matrix is a process of taking every element in each row of the first matrix multiplied with the corresponding element in each column of the other matrix, summing the products and putting them in place. In Einstein's Summation Convention, we represent the matrix product C = AB C = AB : \\begin{pmatrix} a_{11} & a_{12} & . & . & . & a_{1n} \\\\\\ a_{21} & a_{22} & & & & . &\\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ a_{n1} & . & . & . & . & a_{nm}\\end{pmatrix} \\begin{pmatrix} b_{11} & b_{12} & . & . & . & b_{1m} \\\\\\ b_{21} & b_{22} & & & & . &\\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ b_{m1} & . & . & . & . & b_{mp}\\end{pmatrix} = AB \\begin{pmatrix} a_{11} & a_{12} & . & . & . & a_{1n} \\\\\\ a_{21} & a_{22} & & & & . &\\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ a_{n1} & . & . & . & . & a_{nm}\\end{pmatrix} \\begin{pmatrix} b_{11} & b_{12} & . & . & . & b_{1m} \\\\\\ b_{21} & b_{22} & & & & . &\\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ b_{m1} & . & . & . & . & b_{mp}\\end{pmatrix} = AB as, c_{ij} = a_{i1}b_{1j} + ... + a_{im}b_{mj} = \\sum_{k=1}^m a_{ik}b_{kj} c_{ij} = a_{i1}b_{1j} + ... + a_{im}b_{mj} = \\sum_{k=1}^m a_{ik}b_{kj} Note To be clear, its the c_{ij} c_{ij} itself that is written Einstein's Summation Convention, not everything that comes to the right of the = = sign. For i = 1, ..., n i = 1, ..., n and j = 1, ..., p j = 1, ..., p , where A A is a n \\times m n \\times m matrix, B B is a m \\times p m \\times p matrix, and C C is a n \\times p n \\times p matrix. This is useful when we are implementing matrix multiplication in code, because it makes it obvious exactly what operations we need to perform. In this case, it should be obvious that we can run three loops over i i , j j and k k , and then use an accumulator on the k k 's to find the elements of the product matrix AB AB . Note We haven't talked about this yet, but now we can see it clearly. There's no reason, so long as the matrices have the same number of entries in k k , that the matrices we multiply need to be the same shape! Let's revisit the dot product in light of the Einstein Summation Convention. If we've got two vectors, let's call them u u and v v , where u u is a column vector having elements u_i u_i and v v is another column vector having elements v_i v_i . \\begin{pmatrix}u_i \\\\\\ \\vdots \\\\\\ u_{n}\\end{pmatrix} \\cdot \\begin{pmatrix}v_i \\\\\\ \\vdots \\\\\\ v_n \\end{pmatrix} \\begin{pmatrix}u_i \\\\\\ \\vdots \\\\\\ u_{n}\\end{pmatrix} \\cdot \\begin{pmatrix}v_i \\\\\\ \\vdots \\\\\\ v_n \\end{pmatrix} When we dot them together, we are computing the following: u \\cdot v = \\sum^n_{i=1}u_iv_i = u_1v_1 + u_2v_2 + ... + u_nv_n u \\cdot v = \\sum^n_{i=1}u_iv_i = u_1v_1 + u_2v_2 + ... + u_nv_n Notice that, \\begin{pmatrix}u_i \\\\\\ \\vdots \\\\\\ u_{n}\\end{pmatrix} \\cdot \\begin{pmatrix}v_i \\\\\\ \\vdots \\\\\\ v_n \\end{pmatrix} = \\begin{pmatrix}u_i & \\cdots & u_{n}\\end{pmatrix} \\cdot \\begin{pmatrix}v_i \\\\\\ \\vdots \\\\\\ v_n \\end{pmatrix} \\begin{pmatrix}u_i \\\\\\ \\vdots \\\\\\ u_{n}\\end{pmatrix} \\cdot \\begin{pmatrix}v_i \\\\\\ \\vdots \\\\\\ v_n \\end{pmatrix} = \\begin{pmatrix}u_i & \\cdots & u_{n}\\end{pmatrix} \\cdot \\begin{pmatrix}v_i \\\\\\ \\vdots \\\\\\ v_n \\end{pmatrix} and so we notice that there's some equivalence between a matrix transformation (or multiplication ) and the dot product . Lets explore this in more detail. Symmetry of the dot product Say we have the vector \\hat u \\hat u , with components u_1 u_1 and u_2 u_2 . Let's imagine what happens if we dot \\hat u \\hat u with the basis vector \\hat e_1 \\hat e_1 . We know from previous sections that this gives us the length of the projection of \\hat u_1 \\hat u_1 on \\hat e_1 \\hat e_1 multiplied by the norm of \\hat e_1 \\hat e_1 . But what if we do the reverse? What if we dot \\hat e_1 \\hat e_1 with \\hat u \\hat u ? We already know that numerically, the result will be the same, as the dot product is commutative . So geometrically, we can imagine drawing a line of symmetry between the point where the two projections cross: Previously, we stated (without proof, although the numerical proof is trivial) that the dot product is commutative, and now, we have shown geometrically why that is true. Conclusions In this section, we introduced Einstein's Summation Convention , which is a compact and computationally useful (but not very visually intuitive) way to write down matrix operations. This led to a discussion on the similarities between the dot product and matrix multiplication, where we noticed a connection between matrix multiplication , and the dot product , which itself has a geometric understanding as the concept of projection , i.e. projecting one vector onto another. This allows us to think about matrix multiplication with a vector as being the projection of that vector onto the vectors composing the matrix (i.e. the columns of the matrix).","title":"Introduction to Einstein summation convention and the symmetry of the dot product"},{"location":"linear_algebra/week_4/#matrices-transform-into-the-new-basis-vector-set","text":"Tip Watch this video before reading this section. For more practice with changing basis, see this Khan Academy section.","title":"Matrices transform into the new basis vector set"},{"location":"linear_algebra/week_4/#matrices-changing-basis","text":"We have said before that the columns of a transformation matrix are the axes of the new basis vectors after applying the mapping. We're now going to spend a little time looking at how to transform a vector from one set of basis vectors to another . Let's say we have two sets of basis vectors, which define a first coordinate system ( \\text{CS}_1) \\text{CS}_1) and a second coordinate system ( \\text{CS}_2) \\text{CS}_2) . Let the basis vectors of \\text{CS}_2 \\text{CS}_2 , from the perspective of \\text{CS}_1 \\text{CS}_1 be: \\hat b_1 = \\begin{bmatrix}3 \\\\\\ 1\\end{bmatrix}, \\hat b_2 = \\begin{bmatrix}1 \\\\\\ 1\\end{bmatrix} \\hat b_1 = \\begin{bmatrix}3 \\\\\\ 1\\end{bmatrix}, \\hat b_2 = \\begin{bmatrix}1 \\\\\\ 1\\end{bmatrix} Lets package these basis vectors into a matrix \\text{CS}_{21} \\text{CS}_{21} , for convenience \\text{CS}_{21} = \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\text{CS}_{21} = \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} Think of these as the basis vectors of \\text{CS}_2 \\text{CS}_2 as they would appear in \\text{CS}_1 \\text{CS}_1 . If we wanted to change the basis of any vectors in \\text{CS}_2 \\text{CS}_2 to \\text{CS}_1 \\text{CS}_1 , we simply do: \\text{CS}_{21} \\cdot \\text{vector in CS}_2 = \\text{vector in CS}_1 \\text{CS}_{21} \\cdot \\text{vector in CS}_2 = \\text{vector in CS}_1 E.g., for the vector \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} defined in terms of \\text{CS}_2 \\text{CS}_2 \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} = \\begin{bmatrix}5 \\\\\\ 2\\end{bmatrix} \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} = \\begin{bmatrix}5 \\\\\\ 2\\end{bmatrix} That is, a vector \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} described in \\text{CS}_2 \\text{CS}_2 , is described as \\begin{bmatrix}5 \\\\\\ 2\\end{bmatrix} \\begin{bmatrix}5 \\\\\\ 2\\end{bmatrix} in \\text{CS}_1 \\text{CS}_1 . Why does this make sense? Well, you can think of \\text{CS}_{21} \\text{CS}_{21} as the transformation that takes the basis vectors of \\text{CS}_1 \\text{CS}_1 and moves them to the positions of the basis vectors of \\text{CS}_2 \\text{CS}_2 . Applying this transformation to a vector in \\text{CS}_2 \\text{CS}_2 , therefore, gives us the corresponding vector in \\text{CS}_1 \\text{CS}_1 . Now, how do we do the reverse? How do we translate a vector in \\text{CS}_1 \\text{CS}_1 to a vector in \\text{CS}_2 \\text{CS}_2 ? All we need to do to change basis in the reverse case is to multiply a vector in one coordinate system by the inverse of the matrix containing the basis vectors of another: \\text{CS}^{-1}_{21} \\cdot \\text{vector in CS}_1 = \\text{vector in CS}_2 \\text{CS}^{-1}_{21} \\cdot \\text{vector in CS}_1 = \\text{vector in CS}_2 E.g., \\frac{1}{2}\\begin{bmatrix}1 & -1 \\\\\\ -1 & 3\\end{bmatrix}\\begin{bmatrix}5 \\\\\\ 2\\end{bmatrix} = \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} \\frac{1}{2}\\begin{bmatrix}1 & -1 \\\\\\ -1 & 3\\end{bmatrix}\\begin{bmatrix}5 \\\\\\ 2\\end{bmatrix} = \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} Notice that this process gave us the coordinates of the vector \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} in \\text{CS}_2 \\text{CS}_2 , which is equal to the vector that we started with.","title":"Matrices changing basis"},{"location":"linear_algebra/week_4/#generalizing","text":"This is a little bit tricky, so lets generalize. Imagine we have two coordinate systems, \"ours\" and \"theirs\". We can translate vectors in \"their\" coordinate system to \"our\" coordinate system by applying a transformation A A , where A A is a matrix whose columns contain the basis vectors of \"their\" coordinate system as they appear in \"our coordinate system\": \\text{(1) } A \\cdot \\text{vector in \"their\" coordinate system} = \\text{vector in \"our\" coordinate system} \\text{(1) } A \\cdot \\text{vector in \"their\" coordinate system} = \\text{vector in \"our\" coordinate system} To do the reverse, i.e. take a vector in \"our\" coordinate system and translate it to \"their\" coordinate system, we simply multiply \"our\" vector by the inverse of A \\text{(2) } \\text{vector in \"their\" coordinate system} = A^{-1} \\cdot \\text{vector in \"our\" coordinate system} \\text{(2) } \\text{vector in \"their\" coordinate system} = A^{-1} \\cdot \\text{vector in \"our\" coordinate system} It should be obvious now why this is the case, we simply moved A A over in the (1) to get (2)! Orthonormal basis set When we discussed vectors and projections, we said that if the new basis vectors were orthogonal then we could use projection to easily change basis. Note see Changing basis for a fleshed out example. Summary Not orthogonal, use matrix multiplication. Orthogonal, use projection product.","title":"Generalizing"},{"location":"linear_algebra/week_4/#doing-a-transformation-in-a-changed-basis","text":"Tip Watch the last little bit of this video first. Lets discuss the process of applying a transformation in a changed basis. Say again that the basis vectors of coordinate system \\text{CS}_2 \\text{CS}_2 from the perspective of coordinate system \\text{CS}_1 \\text{CS}_1 are: \\text{CS}_{21} = \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\text{CS}_{21} = \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} Say further that we have a vector in \\text{CS}_2 \\text{CS}_2 that we want to transform: {c_2} = \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} {c_2} = \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} And the tranformation we want to apply is: N = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\\\ 1 & 1\\end{bmatrix} N = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\\\ 1 & 1\\end{bmatrix} Note This rotates the vector space 45^0 45^0 counter-clockwise. How do we apply the transformation N N to a vector defined by the coordinate system \\text{CS}_2 \\text{CS}_2 ? The first thing to do is take the vector {c_2} {c_2} and multiply it by \\text{CS}_{21} \\text{CS}_{21} , that is, change the basis of the vector {c_2} {c_2} from \\text{CS}_2 \\text{CS}_2 to \\text{CS}_1 \\text{CS}_1 : {c_1} = \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} {c_1} = \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} Then, we can apply the transformation: {c_1}' = R \\cdot \\text{CS}_{21} \\cdot {c_2} {c_1}' = R \\cdot \\text{CS}_{21} \\cdot {c_2} {c_1}' = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} {c_1}' = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}2 & 0 \\\\\\ 4 & 2\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}2 & 0 \\\\\\ 4 & 2\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} What if, once we obtained this output vector, we wanted to change its basis back to \\text{CS}_2 \\text{CS}_2 ? Recall from the last section that we multiply the whole thing by the inverse of \\text{CS}_{21}^{-1} \\text{CS}_{21}^{-1} {c_2}' = \\text{CS}_{21}^{-1} \\cdot R \\cdot \\text{CS}_{21} \\cdot {c_2} {c_2}' = \\text{CS}_{21}^{-1} \\cdot R \\cdot \\text{CS}_{21} \\cdot {c_2} {c_2}' = \\frac{1}{2} \\begin{bmatrix}1 & -1 \\\\\\ -1 & 3\\end{bmatrix} \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} {c_2}' = \\frac{1}{2} \\begin{bmatrix}1 & -1 \\\\\\ -1 & 3\\end{bmatrix} \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} = \\frac{1}{2} \\begin{bmatrix}1 & -1 \\\\\\ -1 & 3\\end{bmatrix}\\frac{1}{\\sqrt{2}}\\begin{bmatrix}2 & 0 \\\\\\ 4 & 2\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} = \\frac{1}{2} \\begin{bmatrix}1 & -1 \\\\\\ -1 & 3\\end{bmatrix}\\frac{1}{\\sqrt{2}}\\begin{bmatrix}2 & 0 \\\\\\ 4 & 2\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}-1 & -1 \\\\\\ 5 & 3\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}-1 & -1 \\\\\\ 5 & 3\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} This operation essentially builds on the previous operation to return the transformed output vector, {c_1}' {c_1}' relative to \\text{CS}_2 \\text{CS}_2 , that is it returns {c_2}' {c_2}' , where {c_2}' {c_2}' is where c_2 c_2 ends up after in the basis \\text{CS}_2 \\text{CS}_2 after some transformation M M has been applied.","title":"Doing a transformation in a changed basis"},{"location":"linear_algebra/week_4/#generalizing_1","text":"Again, this is tricky, so lets generalize. Imagine we have two coordinate systems, \"ours\" and \"theirs\". If we are given: a vector with its basis in \"their\" coordinate system A A , a matrix whose columns are the basis vectors of \"their\" coordinate system as they appear in \"our\" coordinate system M M some transformation, with its basis in \"our\" coordinate system Then, Change the vector in \"their\" basis to \"ours\": A \\cdot \\text{vector in \"their\" coordinate system} A \\cdot \\text{vector in \"their\" coordinate system} Apply the transformation in \"our\" coordinate system: M \\cdot A \\cdot \\text{vector in \"their\" coordinate system} M \\cdot A \\cdot \\text{vector in \"their\" coordinate system} Change the resulting vector back to \"their\" coordinate system: A^{-1} \\cdot M \\cdot A \\cdot \\text{vector in \"their\" coordinate system} A^{-1} \\cdot M \\cdot A \\cdot \\text{vector in \"their\" coordinate system} In sum, the transformation A^{-1}MA A^{-1}MA will take any vector in \"their\" coordinate system, apply some transformation in \"our\" coordinate system, and return the resulting vector in \"their\" coordinate system.","title":"Generalizing"},{"location":"linear_algebra/week_4/#making-multiple-mappings-deciding-if-these-are-reversible","text":"","title":"Making multiple mappings, deciding if these are reversible"},{"location":"linear_algebra/week_4/#orthogonal-matrices","text":"It is very useful to compose a transformation matrix whose column and row vectors make up a new basis , with the additional constraint of making all of these component vectors orthogonal. Such a square matrix of orthonormal columns and rows is known as an orthogonal matrix . In this section we are going to look at how to construct such a matrix, and why it's useful. (Aside) Transpose First, we need to define a new matrix operation called the transpose . The transpose of a matrix is an operator which flips a matrix over its diagonal , that is it switches the row and column indices of the matrix by producing another matrix denoted as A^T A^T . It is achieved by any one of the following equivalent actions: reflect A A over its main diagonal (which runs from top-left to bottom-right) to obtain A^T A^T , write the rows of A A as the columns of A^T A^T , write the columns of A A as the rows of A^T A^T . Formally, the i -th row, j -th column element of A^T A^T is the j -th row, i -th column element of A A : [A^T]_{ij} = [A]_{ji} [A^T]_{ij} = [A]_{ji} Now let's imagine I have a n \\times n n \\times n matrix A A , with a series of column vectors which are going to be the basis vectors of the some new transformed vector space: A = \\begin{pmatrix} \\begin{pmatrix}\\vdots \\\\\\ \\hat a_1 \\\\\\ \\vdots\\end{pmatrix} & \\begin{pmatrix} \\vdots \\\\\\ \\hat a_2\\\\\\ \\vdots \\end{pmatrix} & \\dots \\begin{pmatrix} \\vdots \\\\\\ \\hat a_n\\\\\\ \\vdots \\end{pmatrix} \\end{pmatrix} A = \\begin{pmatrix} \\begin{pmatrix}\\vdots \\\\\\ \\hat a_1 \\\\\\ \\vdots\\end{pmatrix} & \\begin{pmatrix} \\vdots \\\\\\ \\hat a_2\\\\\\ \\vdots \\end{pmatrix} & \\dots \\begin{pmatrix} \\vdots \\\\\\ \\hat a_n\\\\\\ \\vdots \\end{pmatrix} \\end{pmatrix} Lets place two more constaints on this matrix A A : First, the column vectors a_i a_i have unit length Second, the column vectors a_i a_i are orthogonal to each other. Note That is, \\hat a_i \\cdot \\hat a_j = 0 \\hat a_i \\cdot \\hat a_j = 0 for \\forall i \\ne j \\forall i \\ne j and \\hat a_i \\cdot \\hat a_i = 1 \\hat a_i \\cdot \\hat a_i = 1 . Lets think about what happens when we multiply A A by its transpose, A^T A^T : A^TA = \\begin{pmatrix} \\begin{pmatrix}\\dots & \\hat a_1 & \\dots\\end{pmatrix} \\\\\\ \\begin{pmatrix} \\dots & \\hat a_2 & \\dots \\end{pmatrix} \\\\\\ \\vdots \\\\\\ \\begin{pmatrix} \\dots & \\hat a_n & \\dots \\end{pmatrix} \\end{pmatrix} \\begin{pmatrix} \\begin{pmatrix}\\vdots \\\\\\ \\hat a_1 \\\\\\ \\vdots\\end{pmatrix} & \\begin{pmatrix} \\vdots \\\\\\ \\hat a_2\\\\\\ \\vdots \\end{pmatrix} & \\dots \\begin{pmatrix} \\vdots \\\\\\ \\hat a_n\\\\\\ \\vdots \\end{pmatrix} \\end{pmatrix} = I_n A^TA = \\begin{pmatrix} \\begin{pmatrix}\\dots & \\hat a_1 & \\dots\\end{pmatrix} \\\\\\ \\begin{pmatrix} \\dots & \\hat a_2 & \\dots \\end{pmatrix} \\\\\\ \\vdots \\\\\\ \\begin{pmatrix} \\dots & \\hat a_n & \\dots \\end{pmatrix} \\end{pmatrix} \\begin{pmatrix} \\begin{pmatrix}\\vdots \\\\\\ \\hat a_1 \\\\\\ \\vdots\\end{pmatrix} & \\begin{pmatrix} \\vdots \\\\\\ \\hat a_2\\\\\\ \\vdots \\end{pmatrix} & \\dots \\begin{pmatrix} \\vdots \\\\\\ \\hat a_n\\\\\\ \\vdots \\end{pmatrix} \\end{pmatrix} = I_n So what we notice is that in the case where A A is composed of vectors that are normal to each other and have unit length , (i.e. when they're orthonormal ), then A^TA = I A^TA = I . Stated another way, A^T A^T in this situation is actually the inverse of A A ! This special case is known as an orthogonal matrix . Another thing to note is that because all the basis vectors are of unit length, it must scale space by a factor of one . Stated another way, the determinant of an orthogonal matrix must be either plus or minus one. \\vert A \\vert = \\pm 1 \\vert A \\vert = \\pm 1 Where the minus one arises if the new basis vector set flip space around (from right-handed to left-handed or vice versa). Notice that if A^T A^T , the inverse of A A , then by the definition of the inverse: A^TA = AA^T = I_n A^TA = AA^T = I_n So, we could pre- or post- multiply and still get the identity. This means that the rows of the orthogonal matrix are also orthonormal to each other! So, the transpose matrix of an orthogonal basis set, is itself another orthogonal basis set. Now, remember that in the last module on vectors, we said that transforming a vector onto a new coordinate system was as easy as taking the projection or dot product of that vector onto each of the new bases vectors, as long as they were orthogonal to each other . So, if we have a vector r r and we want to project r r into a new set of axes, let's call them \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 , as long as these vectors are orthogonal to each other, then we can project into the new vector space just by taking the dot product of r r with \\hat e_2 \\hat e_2 , and the dot product of r r with \\hat e_1 \\hat e_1 , and then we'd have its components in the new set of axis. Note See Changing basis for a walked-through example. Conclusions In this section, we introduced the most convenient basis vector set of all, the orthonormal bases vector set . Wherever possible we want to use an orthonormal basis vector set represented as a orthogonal matrix A A . This set of vectors has the following properties and consequences that make it easy to work with: the transpose of such a matrix will be its inverse , which makes the inverse incredibly easy to compute, the transformation will be reversible because space doesn't get collapsed by any dimensions, the projections (i.e. the result of computing the projection of a vector onto the matrix A A ) are just the dot products. One final note. If we arrange the bases vectors in the correct order, then the determinant will be one . \\vert A \\vert = 1 \\vert A \\vert = 1 An easy way to check if they aren't in the right order, is to check if the determinant is minus one. This means we've transformed our space from right to left handed orientation. All we have to do to remedy this is to exchange a pair of vectors in A A such that \\vert A \\vert = 1 \\vert A \\vert = 1 .","title":"Orthogonal matrices"},{"location":"linear_algebra/week_4/#recognizing-mapping-matrices-and-applying-these-to-data","text":"","title":"Recognizing mapping matrices and applying these to data"},{"location":"linear_algebra/week_4/#the-gramschmidt-process","text":"In the last section, we motivated the idea that life is much easier if we can construct an orthonormal basis vector set, but we haven't talked about how to do it. In this section, we will explore just that. We'll start from the assumption that we already have some linearly independent vectors that span the space we're interested in. Say we have some such vectors V = \\{v_1, v_2, ..., v_n\\} V = \\{v_1, v_2, ..., v_n\\} , Note If you want to check linear independence, you can write down your vectors as the the columns in a matrix and check that the determinant of that matrix isn't zero. but they aren't orthogonal to each other or of unit length. Our life would probably be easier if we could construct some orthonormal basis. As it turns out, there's a process for doing just that which is called the Gram-Schmidt process . Let's take the first vector in the set to be v_1 v_1 . In this first step, we're just going to normalize v_1 v_1 to get our eventual first basis vector e_1 e_1 e_1 = \\frac{v_1}{\\vert v_1 \\vert} e_1 = \\frac{v_1}{\\vert v_1 \\vert} Now, we can think of v_2 v_2 as being composed of two things: a component that is in the direction of e_1 e_1 and a component that's perpendicular to e_1 e_1 . We can find the component that's in the direction of e_1 e_1 by taking the vector projection v_2 v_2 onto e_1 e_1 : v_2 = (v_2 \\cdot e_1) \\frac{e_1}{\\vert e_1 \\vert} v_2 = (v_2 \\cdot e_1) \\frac{e_1}{\\vert e_1 \\vert} Note |e_1| |e_1| is 1 so we could actually omit it. If we subtract this vector projection from v_2 v_2 we get u_2 u_2 , a vector which is orthogonal to e_1 e_1 : u_2 = v_2 - (v_2 \\cdot e_1) e_1 u_2 = v_2 - (v_2 \\cdot e_1) e_1 Finally, dividing u_2 u_2 by its length gives us e_2 e_2 , the unit vector orthogonal to e_1 e_1 : e_2 = \\frac{u_2}{\\vert u_2 \\vert} e_2 = \\frac{u_2}{\\vert u_2 \\vert} We could continue this process for all vectors in our set V V . The general formula (in pseudocode) is: # For all vectors in our set V for i in | V | : # For all vectors in our set V that come before v_i for j in i : # Subtract the component of v_i in the direction of the previous vectors v_j v_i = v_i - v_i . dot ( v_j ) * v_j # If |v_i| is not zero, normalize it to unit length. Otherwise it is linearly dependent on a # previous vector, so set it equal to the zero vector. if | v_i | !!! note 0 : v_i = v_i / | v_i | else : v_i = zero_vector Conclusions So that's how we construct an orthonormal basis set, which makes our lives much easier for all the reasons we discussed here .","title":"The Gram\u2013Schmidt process"},{"location":"linear_algebra/week_4/#reflecting-in-a-plane","text":"This is a rather involved example, and is probably best if you just watch it yourself here . If I can find the time, i'll make notes for the video!","title":"Reflecting in a plane"},{"location":"linear_algebra/week_5/","text":"Week 5: Eigenvalues and Eigenvectors Eigenvectors are particular vectors that are unrotated by a transformation matrix (i.e., they remain on their own span ) and eigenvalues are the amount by which the eigenvectors are scaled. These special 'eigen-things' are very useful in linear algebra and will let us examine Google's famous PageRank algorithm for presenting web search results. Then we'll apply this in code, which will wrap up the course. Tip It is best to watch this video first, then return to and read through this section. If you want even more exposure to these ideas, try the first three sections of the Khan Academy course here . Learning Objectives Identify geometrically what an eigenvector/value is Apply mathematical formulation in simple cases Build an intuition of larger dimensional eigensystems Write code to solve a large dimensional eigen problem What are eigen-things? What are eigenvalues and eigenvectors? The word, \"eigen\" is perhaps most usefully translated from German as meaning characteristic . So when we talk about an eigenproblem , we're talking about finding the characteristic properties of something . But characteristic of what? This module, like the previous weeks, will try and explain this concept of \"eigen-ness\" primarily through a geometric interpretation, which allows us to discuss images rather than immediately getting tangled up in the math. Note This topic is often considered by students to be quite tricky. But once you know how to sketch these problems, the rest is just algebra. As you've seen from previous weeks, it's possible to express the concept of linear transformations using matrices . These operations can include scalings , rotations , and shears . Often, when applying these transformations, we are thinking about what they might do to a specific vector . However, it can also be useful to think about what it might look like when they are applied to every vector in this space. This is most easily visualized by drawing a square centered at the origin, and then observing how the square is distorted when you apply the transformation. For example, if we apply a scaling of 2 in the vertical direction, the square would become a rectangle. Whereas, if we applied a horizontal shear to this space, it would become a trapezoid: Now, here's the key concept. Notice that, after the transformation is applied, some vectors end up lying on the same line that they started on whereas, others do not. To highlight this, lets draw three specific vectors onto our initial square. Now, consider our vertical scaling again, and think about what will happen to these three vectors. As you can see, the horizontal green vector is unchanged, i.e., it is pointing in the same direction and having the same length. The vertical pink vector is also still pointing in the same direction as before but its length is doubled. Lastly, the diagonal orange vector used to be exactly 45 degrees to the axis, but it's angle has now increased as has its length. Besides the horizontal and vertical vectors, any other vectors' direction would have been changed by this vertical scaling. So in some sense, the horizontal and vertical vectors are special , they are characteristic of this particular transformation. These are our eigenvectors , and the value they are scaled by is know as an eigenvalue . Note From a conceptual perspective, that's about it for 2D eigen-problems, we simply take a transformation and we look for the vectors who are still laying on the same span as before, and then we measure how much their length has changed. This is basically what eigenvectors and their corresponding eigenvalues are. Let's look at two more classic examples to make sure that we can generalize what we've learned. First, let look look at pure shear , where pure means that we aren't performing any scaling or rotation in addition, so the area is unchanged: Notice that it's only the green horizontal line that is still laying along its original span, and all the other vectors will be shifted Finally, let's look at rotation . Clearly, this thing has got no eigenvectors at all, as all of the vectors have been rotated off their original span: Conclusions In this lecture, we've already covered almost all of what you need to know about eigenvectors and eigenvalues. Although we've only been working in two dimensions so far, the concept is exactly the same in three or more dimensions. In the rest of the module, we'll have a look at some special cases, as well as discussing how to describe what we've observed in more mathematical terms. Getting into the detail of eigenproblems Special eigen-cases As we saw previously, eigenvectors are those which lie along the same span both before and after applying a linear transform to a space. Eigenvalues are simply the amount that each of those vectors has been stretched in the process. In this section, we're going to look at three special cases to make sure the intuition we've built so far is robust, and then we're going to try and extend this concept into three dimensions. The first example we're going to consider is that of a uniform scaling, which is where we scale by the same amount in each direction: As you will hopefully have spotted, not only are all three of the vectors that we've highlighted eigenvectors, but in fact, for a uniform scaling, any vector would be an eigenvector. In this second example, we're going to look at rotation. In the previous section, we applied a small rotation, and we found that it had no eigenvectors. However, there is one case of non-zero pure rotation which does have at least some eigenvectors, and that is 180 180 degrees: As you can see, the three eigenvectors are still laying on the same spans as before, but pointing in the opposite direction. This means that once again, all vectors for this transform are eigenvectors, and they all have eigenvalues of -1 -1 , which means that although the eigenvectors haven't changed length, they are all now pointing in the opposite direction. In this third case, we're going to look at a combination of a horizontal shear and a vertical scaling , and it's slightly less obvious than some of the previous examples. Just like the pure shear case we saw previously, the green horizontal vector is an eigenvector and its eigenvalue is still 1 1 . However, despite the fact that neither of the other two vectors shown are eigen, this transformation does have two eigenvectors: Let's apply the inverse transform and watch our parallelogram go back to its original square. But this time, with our other eigenvector visible. Hopefully, you're at least convinced that it is indeed an eigenvector as it stays on its own span: This shows us that while the concept of eigenvectors is fairly straightforward, eigenvectors aren't always easy to spot. This problem is even tougher in three or more dimensions, and many of the uses of eigen theory in machine learning frame the system as being composed of hundreds of dimensions or more. So, clearly, we're going to need a more robust mathematical description of this concept to allow us to proceed. Before we do, let's take a quick look at one example in 3D. Clearly, scaling and shear are all going to operate much the same way in 3D as they do in 2D. However, rotation does take on a neat new meaning. As you can see from the image, although both the pink and green vectors have changed direction, the orange vector has not moved. This means that the orange vector is an eigenvector, but it also tells us, as a physical interpretation, that if we find the eigenvector of a 3D rotation, it means we've also found the axis of rotation . In this video, we've covered a range of special cases, which I hope have prompted the questions in your mind about how we're going to go about writing a formal definition of an eigen-problem. Calculating eigenvectors At this point, we should now have a reasonable feeling for what an eigen-problem looks like, at least geometrically. In this section, we're going to formalize this concept into an algebraic expression, which will allow us to calculate eigenvalues and eigenvectors whenever they exist. Consider a transformation A A . An eigenvector of this transformation is any vector that can be written as a scaled version of itself after the transformation is applied, i.e., Ax = \\lambda x Ax = \\lambda x This expression captures the idea that applying the transformation A A to an eigenvector x x is the same as scaling that eigenvector x x by some number, \\lambda \\lambda (the eigenvalue). In order to solve for the eigenvectors of the transformation A A , we need to find values of x x that make the two sides equal. To help us find the solutions to this expression, we can rewrite it by putting all the terms on one side and then factorizing (A - \\lambda I) x = 0 (A - \\lambda I) x = 0 Note If you're wondering where the I I term came from, it's just an n \\times n n \\times n identity matrix. We didn't need this in the first expression we wrote, as multiplying vectors by scalars is defined. However, subtracting scalars from matrices is not defined , so the I I just tidies up the math, without changing the meaning. Now that we have this expression, we can see that for the left-hand side to equal 0 0 , either the contents of the brackets must be 0 0 or the vector x x must be 0 0 . As it turns out, we're not interested in the case where the vector x x is 0 0 , i.e., when it has no length or direction, as this represents a trivial solution . Instead, we are interested in the case where the term in brackets is 0 0 . Referring back to the material in the previous parts of the course, we can test if a matrix operation will result in a 0 0 output by calculating its determinant det (A - \\lambda I) = 0 det (A - \\lambda I) = 0 Calculating the determinants manually is a lot of work for high dimensional matrices. So let's try applying this to an arbitrary 2 \\times 2 2 \\times 2 transformation. Let A = \\begin{pmatrix}a & b \\\\ c & d\\end{pmatrix} A = \\begin{pmatrix}a & b \\\\ c & d\\end{pmatrix} substituting this into our eigen-finding expression gives the following: det \\Biggl( \\begin{pmatrix}a & b \\\\\\ c & d\\end{pmatrix}- \\begin{pmatrix}\\lambda & 0 \\\\\\ 0 & \\lambda\\end{pmatrix} \\Biggl ) = 0 det \\Biggl( \\begin{pmatrix}a & b \\\\\\ c & d\\end{pmatrix}- \\begin{pmatrix}\\lambda & 0 \\\\\\ 0 & \\lambda\\end{pmatrix} \\Biggl ) = 0 Evaluating this determinant, we get what is referred to as the characteristic polynomial , which looks like this \\lambda^2 - (a + d) \\lambda + ad - bc = 0 \\lambda^2 - (a + d) \\lambda + ad - bc = 0 Our eigenvalues are simply the solutions of this equation. Once we solve for them, we can then plug them back into the original expression to calculate our eigenvectors. Click the dropdown below for a fully-worked out solution to computing eigenvalues and eigenvectors. Example Let's take the case of a vertical scaling by a factor of 2 2 , which is represented by the transformation matrix A = \\begin{pmatrix} 1 & 0 \\\\\\ 0 & 2\\end{pmatrix} A = \\begin{pmatrix} 1 & 0 \\\\\\ 0 & 2\\end{pmatrix} We start with our equation for finding eigenvalues Ax = \\lambda x Ax = \\lambda x Rearranging, we get (A-\\lambda I)x = 0 (A-\\lambda I)x = 0 Solving (A-\\lambda I) = 0 (A-\\lambda I) = 0 is equivalent to asking when the determinant of the matrix is 0 0 det((A- \\lambda I)) = 0 det((A- \\lambda I)) = 0 Subbing in our matrix A A and solving the resulting characteristic polynomial det \\begin{pmatrix} 1 - \\lambda & 0 \\\\\\ 0 & 2- \\lambda \\end{pmatrix} = (1 - \\lambda)(2-\\lambda) = 0 det \\begin{pmatrix} 1 - \\lambda & 0 \\\\\\ 0 & 2- \\lambda \\end{pmatrix} = (1 - \\lambda)(2-\\lambda) = 0 This means that our equation must have solutions at \\lambda = 1 \\lambda = 1 and \\lambda = 2 \\lambda = 2 . Thinking back to our original eigen-finding formula, (A - \\lambda I)x = 0 (A - \\lambda I)x = 0 , we can now sub these two solutions back in. Thinking about the case where \\lambda = 1 \\lambda = 1 , @\\lambda = 1: \\begin{pmatrix} 1 - 1 & 0 \\\\\\ 0 & 2 - 1 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} 0 \\\\\\ x_2\\end{pmatrix} = 0 @\\lambda = 1: \\begin{pmatrix} 1 - 1 & 0 \\\\\\ 0 & 2 - 1 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} 0 \\\\\\ x_2\\end{pmatrix} = 0 Now, thinking about the case where \\lambda = 2 \\lambda = 2 , @\\lambda = 2: \\begin{pmatrix} 1 - 2 & 0 \\\\\\ 0 & 2 - 2 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} -1 & 0 \\\\\\ 0 & 0 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} -x_1 \\\\\\ 0 \\end{pmatrix} = 0 @\\lambda = 2: \\begin{pmatrix} 1 - 2 & 0 \\\\\\ 0 & 2 - 2 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} -1 & 0 \\\\\\ 0 & 0 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} -x_1 \\\\\\ 0 \\end{pmatrix} = 0 So what do these two expressions tell us? Well, in the case where our eigenvalue \\lambda = 1 \\lambda = 1 , we've got an eigenvector where the x_2 x_2 term must be zero. But we don't really know anything about the x_1 x_1 term. Well, this is because any vector that points along the horizontal axis could be an eigenvector of this system . We say that by writing @\\lambda = 1: \\begin{pmatrix} t \\\\\\ 0 \\end{pmatrix} @\\lambda = 1: \\begin{pmatrix} t \\\\\\ 0 \\end{pmatrix} using an arbitrary parameter t t . Similarly for the \\lambda = 2 \\lambda = 2 case, we can say that our eigenvector must equal @\\lambda = 2: \\begin{pmatrix} 0 \\\\\\ t \\end{pmatrix} @\\lambda = 2: \\begin{pmatrix} 0 \\\\\\ t \\end{pmatrix} because as long as it doesn't move at all in the horizontal direction, any vector that's purely vertical would be an eigenvector of this system, as they would lie along the same span. So now we have two eigenvalues, and their two corresponding eigenvectors. Conclusions Despite all the fun that we've just been having, the truth is that you will almost certainly never have to perform this calculation by hand. Note Indeed, with libraries like numpy this is as easy as import numpy as np A = np . array ([[ 1 , 0 ], [ 0 , 2 ]] ) eigenvalues , eigenvectors = numpy . linalg . eig ( A ) Furthermore, we saw that our approach required finding the roots of a polynomial of order n n , i.e., the dimension of your matrix, which means that the problem will very quickly stop being possible by analytical methods alone. When a computer finds the eigensolutions of a 100 dimensional problem it's forced to employ iterative numerical methods. Therefore, developing a strong conceptual understanding of eigen problems will be much more useful than being really good at calculating them by hand. In this sections, we translated our geometrical understanding of eigenvectors into a robust mathematical expression, and validated it on a few test cases. But I hope that I've also convinced you that working through lots of eigen-problems, as is often done in engineering undergraduate degrees, is not a good investment of your time if you already understand the underlying concepts. This is what computers are for. Next video, we'll be referring back to the concept of basis change to see what magic happens when you use eigenvectors as your basis. See you then. When changing to the eigenbasis is really useful Changing to the eigenbasis Now that we know what eigenvectors are and how to calculate them, we can combine this idea with the concept of changing basis (which was covered earlier in the course). What emerges from this synthesis is a particularly powerful tool for performing efficient matrix operations called diagonalisation . Sometimes, we need to apply the same matrix multiplication many times. For example, imagine a transformation matrix, T T , that represents the change in location of a particle after a single time step. We can write that: our initial position, described by vector v_0 v_0 , multiplied by the transformation T T gives us our new location, v_1 v_1 . To work out where our particle will be after two time steps, we can find v_2 v_2 by simply multiplying v_1 v_1 by T T , which is the same thing as multiplying v_0 v_0 by T T two times. So v_2 = T^2 v_0 v_2 = T^2 v_0 . Now imagine that we expect the same linear transformation to occur every time step for n n time steps. We can write this transformation as v_n = T^n v_0 v_n = T^n v_0 You've already seen how much work it takes to apply a single 3D matrix multiplication. If we were to imagine that T T tells us what happens in one second, but we'd like to know where our particle is in two weeks from now, then n n is going to be around 1.2 million, i.e., we'd need to multiply T T by itself more than a million times , which may take quite a while. If all the terms in the matrix are zero except for those along the leading diagonal, we refer to it as a diagonal matrix . When raising matrices to powers, diagonal matrices make things a lot easier. All you need to do is put each of the terms on the diagonal to the power of n n and you've got the answer. So in this case, T^n = \\begin{pmatrix} a^n & 0 & 0 \\\\\\ 0 & b^n & 0 \\\\\\ 0 & 0 & c^n\\end{pmatrix} T^n = \\begin{pmatrix} a^n & 0 & 0 \\\\\\ 0 & b^n & 0 \\\\\\ 0 & 0 & c^n\\end{pmatrix} Thats simple enough, but what if T T is not a diagonal matrix? Well, as you may have guessed, the answer comes from eigen-analysis. Essentially, what we're going to do is change to a basis where our transformation T T becomes diagonal, which is what we call an eigen-basis . We can then easily apply our power of n n to the diagonalized form, and finally transform the resulting matrix back again, giving us T^n T^n , but avoiding much of the work. Tip If this is confusing, watch the last 4 minutes of this video. As we saw in the section on changing basis, each column of our transform matrix simply represents the new location of the transformed unit vectors. So, to build our eigen-basis conversion matrix, we just plug in each of our eigenvectors as columns: C = \\begin{pmatrix}x_1 & x_2 & x_3 \\\\\\ \\vdots & \\vdots & \\vdots\\end{pmatrix} C = \\begin{pmatrix}x_1 & x_2 & x_3 \\\\\\ \\vdots & \\vdots & \\vdots\\end{pmatrix} Note The basic idea here is that the transformation T T just becomes a uniform scaling (represented by a diagonal matrix) in a basis composed strictly of eigenvectors of T T . Applying this transform, we find ourselves in a world where multiplying by T T is effectively just a pure scaling, which is another way of saying that it can now be represented by a diagonal matrix. Crucially, this diagonal matrix, D D , contains the corresponding eigenvalues of the matrix T T . So, D = \\begin{pmatrix} \\lambda_1 & 0 & 0 \\\\\\ 0 & \\lambda_2 & 0 \\\\\\ 0 & 0 & \\lambda_3\\end{pmatrix} D = \\begin{pmatrix} \\lambda_1 & 0 & 0 \\\\\\ 0 & \\lambda_2 & 0 \\\\\\ 0 & 0 & \\lambda_3\\end{pmatrix} We're so close now to unleashing the power of eigen. The final link that we need to see is the following. Bringing together everything we've just said, it should now be clear that applying the transformation T T is just the same as converting to our eigenbasis, applying the diagonalized matrix, and then converting back again. So T = CDC^{-1} T = CDC^{-1} which suggests that T^2 = CDC^{-1}CDC^{-1} = CDDC^{-1} = CD^2C^{-1} T^2 = CDC^{-1}CDC^{-1} = CDDC^{-1} = CD^2C^{-1} Note C^{-1}C = I C^{-1}C = I , so we omit it. We can then generalize this to any power of T T we'd like T^n = CD^nC^{-1} T^n = CD^nC^{-1} We now have a method which lets us apply a transformation matrix as many times as we'd like without paying a large computational cost. Conclusions This result brings together many of the ideas that we've encountered so far in this course. Check out this video, where we'll work through a short example just to ensure that this approach lines up with our expectations when applied to a simple case. Making the PageRank algorithm PageRank The final topic of this module on eigenproblems, as well as the final topic of this course as a whole, will focus on an algorithm called PageRank . Info This algorithm was famously published by and named after Google founder Larry Page and colleagues in 1998. And was used by Google to help them decide which order to display their websites when they returned from search. The central assumption underpinning PageRank is that the importance of a website is related to its links to and from other websites. This bubble diagram represents a model mini Internet, where each bubble is a webpage and each arrow from A, B, C, and D represents a link on that webpage which takes you to one of the others. We're trying to build an expression that tells us, based on this network structure, which of these webpages is most relevant to the person who made the search. As such, we're going to use the concept of Procrastinating Pat who is an imaginary person who goes on the Internet and just randomly click links to avoid doing their work. By mapping all the possible links, we can build a model to estimate the amount of time we would expect Pat to spend on each webpage. We can describe the links present on page A as a vector, where each row is either a one or a zero based on whether there is a link to the corresponding page. And then normalize the vector by the total number of the links, such that they can be used to describe a probability for that page. For example, the vector of links from page A will be \\begin{bmatrix} 0 & 1 & 1 & 1\\end{bmatrix} \\begin{bmatrix} 0 & 1 & 1 & 1\\end{bmatrix} because vector A has links to sites B, to C, and to D, but it doesn't have a link to itself. Also, because there are three links in this page in total, we would normalize by a factor of a third. So the total click probability sums to one. We can write, L_A = \\begin{bmatrix}0 & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3}\\end{bmatrix} L_A = \\begin{bmatrix}0 & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3}\\end{bmatrix} Following the same logic, the link vectors in the next two sites are shown here: L_B = \\begin{bmatrix} \\frac{1}{2} & 0 & 0 & \\frac{1}{2}\\end{bmatrix}, \\; L_C = \\begin{bmatrix}0 & 0 & 0 & 1\\end{bmatrix} L_B = \\begin{bmatrix} \\frac{1}{2} & 0 & 0 & \\frac{1}{2}\\end{bmatrix}, \\; L_C = \\begin{bmatrix}0 & 0 & 0 & 1\\end{bmatrix} and finally, for page D, we can write L_D = \\begin{bmatrix} 0 & \\frac{1}{2} & \\frac{1}{2} & 0 \\end{bmatrix} L_D = \\begin{bmatrix} 0 & \\frac{1}{2} & \\frac{1}{2} & 0 \\end{bmatrix} We can now build our link matrix L L by using each of our linked vectors as a column, which you can see will form a square matrix L = \\begin{bmatrix} 0 & \\frac{1}{2} & 0 & 0 \\\\\\ \\frac{1}{3} & 0 & 0 & \\frac{1}{2} \\\\\\ \\frac{1}{3} & 0 & 0 & \\frac{1}{2} \\\\\\ \\frac{1}{3} & \\frac{1}{2} & 1 & 0 \\end{bmatrix} L = \\begin{bmatrix} 0 & \\frac{1}{2} & 0 & 0 \\\\\\ \\frac{1}{3} & 0 & 0 & \\frac{1}{2} \\\\\\ \\frac{1}{3} & 0 & 0 & \\frac{1}{2} \\\\\\ \\frac{1}{3} & \\frac{1}{2} & 1 & 0 \\end{bmatrix} What we're trying to represent here with our matrix L L is the probability of ending up on each of the pages. For example, the only way to get to A is by being at B. So you then need to know the probability of being at B, which you could've got to from either A or D. As you can see, this problem is self-referential, as the ranks on all the pages depend on all the others. Although we built our matrix from columns of outward links, we can see that the rows of L L describe inward links normalized with respect to their page of origin. We can now write an expression which summarises the approach. We're going to use the vector r r to store the rank of all webpages. To calculate the rank of page A, you need to know three things about all other pages on the Internet: What's your rank? Do you link to page A? And how many outgoing links do you have in total? The following expression combines these three pieces of information for webpage A only r_A = \\sum_{j=1}^n L_{A, j}r_j r_A = \\sum_{j=1}^n L_{A, j}r_j This expression states that the rank of A is the sum of the ranks of all the pages which link to it , weighted by their specific link probability taken from matrix L L . It would be nice, however, if we could modify this expression to solve for all pages simultaneously. We can rewrite the above expression applied to all webpages as a simple matrix multiplication r = Lr r = Lr Tip If this is confusing, think of it as saying, in english: the rank of some page i i is equal to probability that it is linked to from j j times the rank of j j for all pages i, j i, j . Clearly, we start off not knowing r r , so we simply assume that all the ranks are equal and normalize them by the total number of webpages in our analysis, which in this case is 4 r = \\begin{bmatrix}\\frac{1}{4} \\\\ \\frac{1}{4} \\\\ \\frac{1}{4} \\\\ \\frac{1}{4}\\end{bmatrix} r = \\begin{bmatrix}\\frac{1}{4} \\\\ \\frac{1}{4} \\\\ \\frac{1}{4} \\\\ \\frac{1}{4}\\end{bmatrix} Then, each time we multiply r r by our matrix L L , we get an updated value for r r r^{i+1} = Lr^i r^{i+1} = Lr^i Applying this expression repeatedly means that we are solving this problem iteratively. Each time we do this, we update the values in r r until, eventually, r r stops changing, i.e. r = Lr r = Lr . Quite beautifully, this implies that r r is now an eigenvector of matrix L L , with an eigenvalue of 1! Note At this point, you might well be thinking, if we want to multiply r r by L L many times, perhaps we should apply the diagonalization method that we saw in the last video. But don't forget, this would require us to already know all of the eigenvectors, which is what we're trying to find in the first place. Now that we have an equation, and hopefully some idea of where it came from, we can ask our computer to iteratively apply it until it converges to find our rank vector: Although it takes about ten iterations for the numbers to settle down, the order is already established after the first iteration. However, this is just an artifact of our system being so tiny. Finally, we can read off our result, which says that as Procrastinating Pat randomly clicks around our network, we'd expect them to spend about 40% of their time on page D, but only about 12% of their time on page A and 24% on each of pages B and C As it turns out, although there are many approaches for efficiently calculating eigenvectors that have been developed over the years, repeatedly multiplying a randomly selected initial guest vector by a matrix, which is called the power method , is still very effective for the PageRank problem for two reasons. Firstly, although the power method will clearly only give you one eigenvector, when we know that there will be n n for an n n webpage system, it turns out that because of the way we've structured our link matrix, the vector it gives you will always be the one that you're looking for, with an eigenvalue of 1. Secondly, although this is not true for the full webpage mini Internet, when looking at the real Internet you can imagine that almost every entry in the link matrix will be zero, i.e,, most pages don't connect to most other pages. This is referred to as a sparse matrix . And algorithms exist such that multiplications can be performed very efficiently. Damping factor One key aspect of the PageRank algorithm that we haven't discussed so far is the damping factor , d d . This adds an additional term to our iterative formula. So r^{i + 1} r^{i + 1} is now going to equal r^{i + 1}= d Lr^i + \\frac{1 - d}{n} r^{i + 1}= d Lr^i + \\frac{1 - d}{n} where d d is something between 0 and 1. You can think of it as 1 minus the probability with which procrastinating Pat suddenly, randomly types in a web address, rather than clicking on a link on his current page. The effect that this has on the actual calculation is about finding a compromise between speed and stability of the iterative convergence process. There are over one billion websites on the Internet today, compared with just a few million when the PageRank algorithm was first published in 1998, and so the methods for search and ranking have had to evolve to maximize efficiency, although the core concept has remained unchanged for many years. Conclusions This brings us to the end of our introduction to the PageRank algorithm. There are, of course, many details which we didn't cover in this video. But I hope this has allowed you to come away with some insight and understanding into how the PageRank works, and hopefully the confidence to apply this to some larger networks yourself. Summary This brings us to the end of the fifth module and also, to the end of this course on linear algebra for machine learning. We've covered a lot of ground in the past five modules, but I hope that we've managed to balance, the speed with the level of detail to ensure that you've stayed with us throughout. There is a tension at the heart of mathematics teaching in the computer age. Classical teaching approaches focused around working through lots of examples by hand without much emphasis on building intuition. However, computers now do nearly all of the calculation work for us, and it's not typical for the methods appropriate to hand calculation to be the same as those employed by a computer. This can mean that, despite doing lots of work, students can come away from a classical education missing both the detailed view of the computational methods, but also the high level view of what each method is really doing. The concepts that you've been exposed to over the last five modules cover the core of linear algebra. That you will need as you progress your study of machine learning. And we hope that at the very least, when you get stuck in the future, you'll know the appropriate language. So that you can quickly look up some help when you need it. Which, after all, is the most important skill of a professional coder.","title":"Week 5"},{"location":"linear_algebra/week_5/#week-5-eigenvalues-and-eigenvectors","text":"Eigenvectors are particular vectors that are unrotated by a transformation matrix (i.e., they remain on their own span ) and eigenvalues are the amount by which the eigenvectors are scaled. These special 'eigen-things' are very useful in linear algebra and will let us examine Google's famous PageRank algorithm for presenting web search results. Then we'll apply this in code, which will wrap up the course. Tip It is best to watch this video first, then return to and read through this section. If you want even more exposure to these ideas, try the first three sections of the Khan Academy course here .","title":"Week 5: Eigenvalues and Eigenvectors"},{"location":"linear_algebra/week_5/#learning-objectives","text":"Identify geometrically what an eigenvector/value is Apply mathematical formulation in simple cases Build an intuition of larger dimensional eigensystems Write code to solve a large dimensional eigen problem","title":"Learning Objectives"},{"location":"linear_algebra/week_5/#what-are-eigen-things","text":"","title":"What are eigen-things?"},{"location":"linear_algebra/week_5/#what-are-eigenvalues-and-eigenvectors","text":"The word, \"eigen\" is perhaps most usefully translated from German as meaning characteristic . So when we talk about an eigenproblem , we're talking about finding the characteristic properties of something . But characteristic of what? This module, like the previous weeks, will try and explain this concept of \"eigen-ness\" primarily through a geometric interpretation, which allows us to discuss images rather than immediately getting tangled up in the math. Note This topic is often considered by students to be quite tricky. But once you know how to sketch these problems, the rest is just algebra. As you've seen from previous weeks, it's possible to express the concept of linear transformations using matrices . These operations can include scalings , rotations , and shears . Often, when applying these transformations, we are thinking about what they might do to a specific vector . However, it can also be useful to think about what it might look like when they are applied to every vector in this space. This is most easily visualized by drawing a square centered at the origin, and then observing how the square is distorted when you apply the transformation. For example, if we apply a scaling of 2 in the vertical direction, the square would become a rectangle. Whereas, if we applied a horizontal shear to this space, it would become a trapezoid: Now, here's the key concept. Notice that, after the transformation is applied, some vectors end up lying on the same line that they started on whereas, others do not. To highlight this, lets draw three specific vectors onto our initial square. Now, consider our vertical scaling again, and think about what will happen to these three vectors. As you can see, the horizontal green vector is unchanged, i.e., it is pointing in the same direction and having the same length. The vertical pink vector is also still pointing in the same direction as before but its length is doubled. Lastly, the diagonal orange vector used to be exactly 45 degrees to the axis, but it's angle has now increased as has its length. Besides the horizontal and vertical vectors, any other vectors' direction would have been changed by this vertical scaling. So in some sense, the horizontal and vertical vectors are special , they are characteristic of this particular transformation. These are our eigenvectors , and the value they are scaled by is know as an eigenvalue . Note From a conceptual perspective, that's about it for 2D eigen-problems, we simply take a transformation and we look for the vectors who are still laying on the same span as before, and then we measure how much their length has changed. This is basically what eigenvectors and their corresponding eigenvalues are. Let's look at two more classic examples to make sure that we can generalize what we've learned. First, let look look at pure shear , where pure means that we aren't performing any scaling or rotation in addition, so the area is unchanged: Notice that it's only the green horizontal line that is still laying along its original span, and all the other vectors will be shifted Finally, let's look at rotation . Clearly, this thing has got no eigenvectors at all, as all of the vectors have been rotated off their original span: Conclusions In this lecture, we've already covered almost all of what you need to know about eigenvectors and eigenvalues. Although we've only been working in two dimensions so far, the concept is exactly the same in three or more dimensions. In the rest of the module, we'll have a look at some special cases, as well as discussing how to describe what we've observed in more mathematical terms.","title":"What are eigenvalues and eigenvectors?"},{"location":"linear_algebra/week_5/#getting-into-the-detail-of-eigenproblems","text":"","title":"Getting into the detail of eigenproblems"},{"location":"linear_algebra/week_5/#special-eigen-cases","text":"As we saw previously, eigenvectors are those which lie along the same span both before and after applying a linear transform to a space. Eigenvalues are simply the amount that each of those vectors has been stretched in the process. In this section, we're going to look at three special cases to make sure the intuition we've built so far is robust, and then we're going to try and extend this concept into three dimensions. The first example we're going to consider is that of a uniform scaling, which is where we scale by the same amount in each direction: As you will hopefully have spotted, not only are all three of the vectors that we've highlighted eigenvectors, but in fact, for a uniform scaling, any vector would be an eigenvector. In this second example, we're going to look at rotation. In the previous section, we applied a small rotation, and we found that it had no eigenvectors. However, there is one case of non-zero pure rotation which does have at least some eigenvectors, and that is 180 180 degrees: As you can see, the three eigenvectors are still laying on the same spans as before, but pointing in the opposite direction. This means that once again, all vectors for this transform are eigenvectors, and they all have eigenvalues of -1 -1 , which means that although the eigenvectors haven't changed length, they are all now pointing in the opposite direction. In this third case, we're going to look at a combination of a horizontal shear and a vertical scaling , and it's slightly less obvious than some of the previous examples. Just like the pure shear case we saw previously, the green horizontal vector is an eigenvector and its eigenvalue is still 1 1 . However, despite the fact that neither of the other two vectors shown are eigen, this transformation does have two eigenvectors: Let's apply the inverse transform and watch our parallelogram go back to its original square. But this time, with our other eigenvector visible. Hopefully, you're at least convinced that it is indeed an eigenvector as it stays on its own span: This shows us that while the concept of eigenvectors is fairly straightforward, eigenvectors aren't always easy to spot. This problem is even tougher in three or more dimensions, and many of the uses of eigen theory in machine learning frame the system as being composed of hundreds of dimensions or more. So, clearly, we're going to need a more robust mathematical description of this concept to allow us to proceed. Before we do, let's take a quick look at one example in 3D. Clearly, scaling and shear are all going to operate much the same way in 3D as they do in 2D. However, rotation does take on a neat new meaning. As you can see from the image, although both the pink and green vectors have changed direction, the orange vector has not moved. This means that the orange vector is an eigenvector, but it also tells us, as a physical interpretation, that if we find the eigenvector of a 3D rotation, it means we've also found the axis of rotation . In this video, we've covered a range of special cases, which I hope have prompted the questions in your mind about how we're going to go about writing a formal definition of an eigen-problem.","title":"Special eigen-cases"},{"location":"linear_algebra/week_5/#calculating-eigenvectors","text":"At this point, we should now have a reasonable feeling for what an eigen-problem looks like, at least geometrically. In this section, we're going to formalize this concept into an algebraic expression, which will allow us to calculate eigenvalues and eigenvectors whenever they exist. Consider a transformation A A . An eigenvector of this transformation is any vector that can be written as a scaled version of itself after the transformation is applied, i.e., Ax = \\lambda x Ax = \\lambda x This expression captures the idea that applying the transformation A A to an eigenvector x x is the same as scaling that eigenvector x x by some number, \\lambda \\lambda (the eigenvalue). In order to solve for the eigenvectors of the transformation A A , we need to find values of x x that make the two sides equal. To help us find the solutions to this expression, we can rewrite it by putting all the terms on one side and then factorizing (A - \\lambda I) x = 0 (A - \\lambda I) x = 0 Note If you're wondering where the I I term came from, it's just an n \\times n n \\times n identity matrix. We didn't need this in the first expression we wrote, as multiplying vectors by scalars is defined. However, subtracting scalars from matrices is not defined , so the I I just tidies up the math, without changing the meaning. Now that we have this expression, we can see that for the left-hand side to equal 0 0 , either the contents of the brackets must be 0 0 or the vector x x must be 0 0 . As it turns out, we're not interested in the case where the vector x x is 0 0 , i.e., when it has no length or direction, as this represents a trivial solution . Instead, we are interested in the case where the term in brackets is 0 0 . Referring back to the material in the previous parts of the course, we can test if a matrix operation will result in a 0 0 output by calculating its determinant det (A - \\lambda I) = 0 det (A - \\lambda I) = 0 Calculating the determinants manually is a lot of work for high dimensional matrices. So let's try applying this to an arbitrary 2 \\times 2 2 \\times 2 transformation. Let A = \\begin{pmatrix}a & b \\\\ c & d\\end{pmatrix} A = \\begin{pmatrix}a & b \\\\ c & d\\end{pmatrix} substituting this into our eigen-finding expression gives the following: det \\Biggl( \\begin{pmatrix}a & b \\\\\\ c & d\\end{pmatrix}- \\begin{pmatrix}\\lambda & 0 \\\\\\ 0 & \\lambda\\end{pmatrix} \\Biggl ) = 0 det \\Biggl( \\begin{pmatrix}a & b \\\\\\ c & d\\end{pmatrix}- \\begin{pmatrix}\\lambda & 0 \\\\\\ 0 & \\lambda\\end{pmatrix} \\Biggl ) = 0 Evaluating this determinant, we get what is referred to as the characteristic polynomial , which looks like this \\lambda^2 - (a + d) \\lambda + ad - bc = 0 \\lambda^2 - (a + d) \\lambda + ad - bc = 0 Our eigenvalues are simply the solutions of this equation. Once we solve for them, we can then plug them back into the original expression to calculate our eigenvectors. Click the dropdown below for a fully-worked out solution to computing eigenvalues and eigenvectors. Example Let's take the case of a vertical scaling by a factor of 2 2 , which is represented by the transformation matrix A = \\begin{pmatrix} 1 & 0 \\\\\\ 0 & 2\\end{pmatrix} A = \\begin{pmatrix} 1 & 0 \\\\\\ 0 & 2\\end{pmatrix} We start with our equation for finding eigenvalues Ax = \\lambda x Ax = \\lambda x Rearranging, we get (A-\\lambda I)x = 0 (A-\\lambda I)x = 0 Solving (A-\\lambda I) = 0 (A-\\lambda I) = 0 is equivalent to asking when the determinant of the matrix is 0 0 det((A- \\lambda I)) = 0 det((A- \\lambda I)) = 0 Subbing in our matrix A A and solving the resulting characteristic polynomial det \\begin{pmatrix} 1 - \\lambda & 0 \\\\\\ 0 & 2- \\lambda \\end{pmatrix} = (1 - \\lambda)(2-\\lambda) = 0 det \\begin{pmatrix} 1 - \\lambda & 0 \\\\\\ 0 & 2- \\lambda \\end{pmatrix} = (1 - \\lambda)(2-\\lambda) = 0 This means that our equation must have solutions at \\lambda = 1 \\lambda = 1 and \\lambda = 2 \\lambda = 2 . Thinking back to our original eigen-finding formula, (A - \\lambda I)x = 0 (A - \\lambda I)x = 0 , we can now sub these two solutions back in. Thinking about the case where \\lambda = 1 \\lambda = 1 , @\\lambda = 1: \\begin{pmatrix} 1 - 1 & 0 \\\\\\ 0 & 2 - 1 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} 0 \\\\\\ x_2\\end{pmatrix} = 0 @\\lambda = 1: \\begin{pmatrix} 1 - 1 & 0 \\\\\\ 0 & 2 - 1 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} 0 \\\\\\ x_2\\end{pmatrix} = 0 Now, thinking about the case where \\lambda = 2 \\lambda = 2 , @\\lambda = 2: \\begin{pmatrix} 1 - 2 & 0 \\\\\\ 0 & 2 - 2 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} -1 & 0 \\\\\\ 0 & 0 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} -x_1 \\\\\\ 0 \\end{pmatrix} = 0 @\\lambda = 2: \\begin{pmatrix} 1 - 2 & 0 \\\\\\ 0 & 2 - 2 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} -1 & 0 \\\\\\ 0 & 0 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} -x_1 \\\\\\ 0 \\end{pmatrix} = 0 So what do these two expressions tell us? Well, in the case where our eigenvalue \\lambda = 1 \\lambda = 1 , we've got an eigenvector where the x_2 x_2 term must be zero. But we don't really know anything about the x_1 x_1 term. Well, this is because any vector that points along the horizontal axis could be an eigenvector of this system . We say that by writing @\\lambda = 1: \\begin{pmatrix} t \\\\\\ 0 \\end{pmatrix} @\\lambda = 1: \\begin{pmatrix} t \\\\\\ 0 \\end{pmatrix} using an arbitrary parameter t t . Similarly for the \\lambda = 2 \\lambda = 2 case, we can say that our eigenvector must equal @\\lambda = 2: \\begin{pmatrix} 0 \\\\\\ t \\end{pmatrix} @\\lambda = 2: \\begin{pmatrix} 0 \\\\\\ t \\end{pmatrix} because as long as it doesn't move at all in the horizontal direction, any vector that's purely vertical would be an eigenvector of this system, as they would lie along the same span. So now we have two eigenvalues, and their two corresponding eigenvectors.","title":"Calculating eigenvectors"},{"location":"linear_algebra/week_5/#conclusions","text":"Despite all the fun that we've just been having, the truth is that you will almost certainly never have to perform this calculation by hand. Note Indeed, with libraries like numpy this is as easy as import numpy as np A = np . array ([[ 1 , 0 ], [ 0 , 2 ]] ) eigenvalues , eigenvectors = numpy . linalg . eig ( A ) Furthermore, we saw that our approach required finding the roots of a polynomial of order n n , i.e., the dimension of your matrix, which means that the problem will very quickly stop being possible by analytical methods alone. When a computer finds the eigensolutions of a 100 dimensional problem it's forced to employ iterative numerical methods. Therefore, developing a strong conceptual understanding of eigen problems will be much more useful than being really good at calculating them by hand. In this sections, we translated our geometrical understanding of eigenvectors into a robust mathematical expression, and validated it on a few test cases. But I hope that I've also convinced you that working through lots of eigen-problems, as is often done in engineering undergraduate degrees, is not a good investment of your time if you already understand the underlying concepts. This is what computers are for. Next video, we'll be referring back to the concept of basis change to see what magic happens when you use eigenvectors as your basis. See you then.","title":"Conclusions"},{"location":"linear_algebra/week_5/#when-changing-to-the-eigenbasis-is-really-useful","text":"","title":"When changing to the eigenbasis is really useful"},{"location":"linear_algebra/week_5/#changing-to-the-eigenbasis","text":"Now that we know what eigenvectors are and how to calculate them, we can combine this idea with the concept of changing basis (which was covered earlier in the course). What emerges from this synthesis is a particularly powerful tool for performing efficient matrix operations called diagonalisation . Sometimes, we need to apply the same matrix multiplication many times. For example, imagine a transformation matrix, T T , that represents the change in location of a particle after a single time step. We can write that: our initial position, described by vector v_0 v_0 , multiplied by the transformation T T gives us our new location, v_1 v_1 . To work out where our particle will be after two time steps, we can find v_2 v_2 by simply multiplying v_1 v_1 by T T , which is the same thing as multiplying v_0 v_0 by T T two times. So v_2 = T^2 v_0 v_2 = T^2 v_0 . Now imagine that we expect the same linear transformation to occur every time step for n n time steps. We can write this transformation as v_n = T^n v_0 v_n = T^n v_0 You've already seen how much work it takes to apply a single 3D matrix multiplication. If we were to imagine that T T tells us what happens in one second, but we'd like to know where our particle is in two weeks from now, then n n is going to be around 1.2 million, i.e., we'd need to multiply T T by itself more than a million times , which may take quite a while. If all the terms in the matrix are zero except for those along the leading diagonal, we refer to it as a diagonal matrix . When raising matrices to powers, diagonal matrices make things a lot easier. All you need to do is put each of the terms on the diagonal to the power of n n and you've got the answer. So in this case, T^n = \\begin{pmatrix} a^n & 0 & 0 \\\\\\ 0 & b^n & 0 \\\\\\ 0 & 0 & c^n\\end{pmatrix} T^n = \\begin{pmatrix} a^n & 0 & 0 \\\\\\ 0 & b^n & 0 \\\\\\ 0 & 0 & c^n\\end{pmatrix} Thats simple enough, but what if T T is not a diagonal matrix? Well, as you may have guessed, the answer comes from eigen-analysis. Essentially, what we're going to do is change to a basis where our transformation T T becomes diagonal, which is what we call an eigen-basis . We can then easily apply our power of n n to the diagonalized form, and finally transform the resulting matrix back again, giving us T^n T^n , but avoiding much of the work. Tip If this is confusing, watch the last 4 minutes of this video. As we saw in the section on changing basis, each column of our transform matrix simply represents the new location of the transformed unit vectors. So, to build our eigen-basis conversion matrix, we just plug in each of our eigenvectors as columns: C = \\begin{pmatrix}x_1 & x_2 & x_3 \\\\\\ \\vdots & \\vdots & \\vdots\\end{pmatrix} C = \\begin{pmatrix}x_1 & x_2 & x_3 \\\\\\ \\vdots & \\vdots & \\vdots\\end{pmatrix} Note The basic idea here is that the transformation T T just becomes a uniform scaling (represented by a diagonal matrix) in a basis composed strictly of eigenvectors of T T . Applying this transform, we find ourselves in a world where multiplying by T T is effectively just a pure scaling, which is another way of saying that it can now be represented by a diagonal matrix. Crucially, this diagonal matrix, D D , contains the corresponding eigenvalues of the matrix T T . So, D = \\begin{pmatrix} \\lambda_1 & 0 & 0 \\\\\\ 0 & \\lambda_2 & 0 \\\\\\ 0 & 0 & \\lambda_3\\end{pmatrix} D = \\begin{pmatrix} \\lambda_1 & 0 & 0 \\\\\\ 0 & \\lambda_2 & 0 \\\\\\ 0 & 0 & \\lambda_3\\end{pmatrix} We're so close now to unleashing the power of eigen. The final link that we need to see is the following. Bringing together everything we've just said, it should now be clear that applying the transformation T T is just the same as converting to our eigenbasis, applying the diagonalized matrix, and then converting back again. So T = CDC^{-1} T = CDC^{-1} which suggests that T^2 = CDC^{-1}CDC^{-1} = CDDC^{-1} = CD^2C^{-1} T^2 = CDC^{-1}CDC^{-1} = CDDC^{-1} = CD^2C^{-1} Note C^{-1}C = I C^{-1}C = I , so we omit it. We can then generalize this to any power of T T we'd like T^n = CD^nC^{-1} T^n = CD^nC^{-1} We now have a method which lets us apply a transformation matrix as many times as we'd like without paying a large computational cost. Conclusions This result brings together many of the ideas that we've encountered so far in this course. Check out this video, where we'll work through a short example just to ensure that this approach lines up with our expectations when applied to a simple case.","title":"Changing to the eigenbasis"},{"location":"linear_algebra/week_5/#making-the-pagerank-algorithm","text":"","title":"Making the PageRank algorithm"},{"location":"linear_algebra/week_5/#pagerank","text":"The final topic of this module on eigenproblems, as well as the final topic of this course as a whole, will focus on an algorithm called PageRank . Info This algorithm was famously published by and named after Google founder Larry Page and colleagues in 1998. And was used by Google to help them decide which order to display their websites when they returned from search. The central assumption underpinning PageRank is that the importance of a website is related to its links to and from other websites. This bubble diagram represents a model mini Internet, where each bubble is a webpage and each arrow from A, B, C, and D represents a link on that webpage which takes you to one of the others. We're trying to build an expression that tells us, based on this network structure, which of these webpages is most relevant to the person who made the search. As such, we're going to use the concept of Procrastinating Pat who is an imaginary person who goes on the Internet and just randomly click links to avoid doing their work. By mapping all the possible links, we can build a model to estimate the amount of time we would expect Pat to spend on each webpage. We can describe the links present on page A as a vector, where each row is either a one or a zero based on whether there is a link to the corresponding page. And then normalize the vector by the total number of the links, such that they can be used to describe a probability for that page. For example, the vector of links from page A will be \\begin{bmatrix} 0 & 1 & 1 & 1\\end{bmatrix} \\begin{bmatrix} 0 & 1 & 1 & 1\\end{bmatrix} because vector A has links to sites B, to C, and to D, but it doesn't have a link to itself. Also, because there are three links in this page in total, we would normalize by a factor of a third. So the total click probability sums to one. We can write, L_A = \\begin{bmatrix}0 & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3}\\end{bmatrix} L_A = \\begin{bmatrix}0 & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3}\\end{bmatrix} Following the same logic, the link vectors in the next two sites are shown here: L_B = \\begin{bmatrix} \\frac{1}{2} & 0 & 0 & \\frac{1}{2}\\end{bmatrix}, \\; L_C = \\begin{bmatrix}0 & 0 & 0 & 1\\end{bmatrix} L_B = \\begin{bmatrix} \\frac{1}{2} & 0 & 0 & \\frac{1}{2}\\end{bmatrix}, \\; L_C = \\begin{bmatrix}0 & 0 & 0 & 1\\end{bmatrix} and finally, for page D, we can write L_D = \\begin{bmatrix} 0 & \\frac{1}{2} & \\frac{1}{2} & 0 \\end{bmatrix} L_D = \\begin{bmatrix} 0 & \\frac{1}{2} & \\frac{1}{2} & 0 \\end{bmatrix} We can now build our link matrix L L by using each of our linked vectors as a column, which you can see will form a square matrix L = \\begin{bmatrix} 0 & \\frac{1}{2} & 0 & 0 \\\\\\ \\frac{1}{3} & 0 & 0 & \\frac{1}{2} \\\\\\ \\frac{1}{3} & 0 & 0 & \\frac{1}{2} \\\\\\ \\frac{1}{3} & \\frac{1}{2} & 1 & 0 \\end{bmatrix} L = \\begin{bmatrix} 0 & \\frac{1}{2} & 0 & 0 \\\\\\ \\frac{1}{3} & 0 & 0 & \\frac{1}{2} \\\\\\ \\frac{1}{3} & 0 & 0 & \\frac{1}{2} \\\\\\ \\frac{1}{3} & \\frac{1}{2} & 1 & 0 \\end{bmatrix} What we're trying to represent here with our matrix L L is the probability of ending up on each of the pages. For example, the only way to get to A is by being at B. So you then need to know the probability of being at B, which you could've got to from either A or D. As you can see, this problem is self-referential, as the ranks on all the pages depend on all the others. Although we built our matrix from columns of outward links, we can see that the rows of L L describe inward links normalized with respect to their page of origin. We can now write an expression which summarises the approach. We're going to use the vector r r to store the rank of all webpages. To calculate the rank of page A, you need to know three things about all other pages on the Internet: What's your rank? Do you link to page A? And how many outgoing links do you have in total? The following expression combines these three pieces of information for webpage A only r_A = \\sum_{j=1}^n L_{A, j}r_j r_A = \\sum_{j=1}^n L_{A, j}r_j This expression states that the rank of A is the sum of the ranks of all the pages which link to it , weighted by their specific link probability taken from matrix L L . It would be nice, however, if we could modify this expression to solve for all pages simultaneously. We can rewrite the above expression applied to all webpages as a simple matrix multiplication r = Lr r = Lr Tip If this is confusing, think of it as saying, in english: the rank of some page i i is equal to probability that it is linked to from j j times the rank of j j for all pages i, j i, j . Clearly, we start off not knowing r r , so we simply assume that all the ranks are equal and normalize them by the total number of webpages in our analysis, which in this case is 4 r = \\begin{bmatrix}\\frac{1}{4} \\\\ \\frac{1}{4} \\\\ \\frac{1}{4} \\\\ \\frac{1}{4}\\end{bmatrix} r = \\begin{bmatrix}\\frac{1}{4} \\\\ \\frac{1}{4} \\\\ \\frac{1}{4} \\\\ \\frac{1}{4}\\end{bmatrix} Then, each time we multiply r r by our matrix L L , we get an updated value for r r r^{i+1} = Lr^i r^{i+1} = Lr^i Applying this expression repeatedly means that we are solving this problem iteratively. Each time we do this, we update the values in r r until, eventually, r r stops changing, i.e. r = Lr r = Lr . Quite beautifully, this implies that r r is now an eigenvector of matrix L L , with an eigenvalue of 1! Note At this point, you might well be thinking, if we want to multiply r r by L L many times, perhaps we should apply the diagonalization method that we saw in the last video. But don't forget, this would require us to already know all of the eigenvectors, which is what we're trying to find in the first place. Now that we have an equation, and hopefully some idea of where it came from, we can ask our computer to iteratively apply it until it converges to find our rank vector: Although it takes about ten iterations for the numbers to settle down, the order is already established after the first iteration. However, this is just an artifact of our system being so tiny. Finally, we can read off our result, which says that as Procrastinating Pat randomly clicks around our network, we'd expect them to spend about 40% of their time on page D, but only about 12% of their time on page A and 24% on each of pages B and C As it turns out, although there are many approaches for efficiently calculating eigenvectors that have been developed over the years, repeatedly multiplying a randomly selected initial guest vector by a matrix, which is called the power method , is still very effective for the PageRank problem for two reasons. Firstly, although the power method will clearly only give you one eigenvector, when we know that there will be n n for an n n webpage system, it turns out that because of the way we've structured our link matrix, the vector it gives you will always be the one that you're looking for, with an eigenvalue of 1. Secondly, although this is not true for the full webpage mini Internet, when looking at the real Internet you can imagine that almost every entry in the link matrix will be zero, i.e,, most pages don't connect to most other pages. This is referred to as a sparse matrix . And algorithms exist such that multiplications can be performed very efficiently.","title":"PageRank"},{"location":"linear_algebra/week_5/#damping-factor","text":"One key aspect of the PageRank algorithm that we haven't discussed so far is the damping factor , d d . This adds an additional term to our iterative formula. So r^{i + 1} r^{i + 1} is now going to equal r^{i + 1}= d Lr^i + \\frac{1 - d}{n} r^{i + 1}= d Lr^i + \\frac{1 - d}{n} where d d is something between 0 and 1. You can think of it as 1 minus the probability with which procrastinating Pat suddenly, randomly types in a web address, rather than clicking on a link on his current page. The effect that this has on the actual calculation is about finding a compromise between speed and stability of the iterative convergence process. There are over one billion websites on the Internet today, compared with just a few million when the PageRank algorithm was first published in 1998, and so the methods for search and ranking have had to evolve to maximize efficiency, although the core concept has remained unchanged for many years. Conclusions This brings us to the end of our introduction to the PageRank algorithm. There are, of course, many details which we didn't cover in this video. But I hope this has allowed you to come away with some insight and understanding into how the PageRank works, and hopefully the confidence to apply this to some larger networks yourself.","title":"Damping factor"},{"location":"linear_algebra/week_5/#summary","text":"This brings us to the end of the fifth module and also, to the end of this course on linear algebra for machine learning. We've covered a lot of ground in the past five modules, but I hope that we've managed to balance, the speed with the level of detail to ensure that you've stayed with us throughout. There is a tension at the heart of mathematics teaching in the computer age. Classical teaching approaches focused around working through lots of examples by hand without much emphasis on building intuition. However, computers now do nearly all of the calculation work for us, and it's not typical for the methods appropriate to hand calculation to be the same as those employed by a computer. This can mean that, despite doing lots of work, students can come away from a classical education missing both the detailed view of the computational methods, but also the high level view of what each method is really doing. The concepts that you've been exposed to over the last five modules cover the core of linear algebra. That you will need as you progress your study of machine learning. And we hope that at the very least, when you get stuck in the future, you'll know the appropriate language. So that you can quickly look up some help when you need it. Which, after all, is the most important skill of a professional coder.","title":"Summary"},{"location":"multivariate_calculus/course_resources/","text":"Course resources There are lots of useful web resources on multivariate calculus . Typically they go a bit slower or have a different emphasis or way of explaining things, but it can be handy to see how someone else explains something. Khan Academy is a great resource right up to 1st or 2nd year undergraduate material. For this course, there's a handy group of videos here . Grant Sanderson has a great series of videos developing mathematical intuition on YouTube, which you can reach through his site here .","title":"Course Resources"},{"location":"multivariate_calculus/course_resources/#course-resources","text":"There are lots of useful web resources on multivariate calculus . Typically they go a bit slower or have a different emphasis or way of explaining things, but it can be handy to see how someone else explains something. Khan Academy is a great resource right up to 1st or 2nd year undergraduate material. For this course, there's a handy group of videos here . Grant Sanderson has a great series of videos developing mathematical intuition on YouTube, which you can reach through his site here .","title":"Course resources"},{"location":"multivariate_calculus/week_1/","text":"Week 1: What is calculus? Understanding calculus is central to understanding machine learning! You can think of calculus as a set of tools for analyzing the relationship between functions and their inputs . We start this module from the basics, by recalling what a function is and where we might encounter one. Following this, we talk about how the slope of a graph at a given point describes the rate of change of the output of the graphed function with respect to an input at that point. Using this visual intuition, we derive a robust mathematical definition of a derivative, which we then use to differentiate some interesting functions. Finally, by studying a few examples, we develop four handy time saving rules that enable us to speed up differentiation for many common scenarios. Learning Objectives Recall the definition of differentiation Apply differentiation to simple functions Describe the utility of time saving rules Apply sum , product and chain rule Back to basics: functions Welcome to this course Welcome to module one of six. We start right from the basics, but build you up fairly quickly to some interesting applications in modules five and six. This week, we'll be focusing on the fundamental theory of calculus , as well as some handy rules to speed things up. Where possible, we're going to try and represent the concepts graphically so that you can see the derivations rather than just reading them. This sometimes means that we'll be skimming over a few details, but we'll also provide you with links to the more rigorous descriptions so that you can check them out if you're interested. Functions Tip Try watching this video at first for quickly building up your intuition for what calculus is all about. Before diving into calculus, we should first talk briefly about what functions are and where we use them. Essentially, a function is a relationship between some inputs and an output. For example, if I had a function for modeling the distribution of temperature in a room, I might input the x x , y y , and z z coordinates of a specific location I'm interested in as well as the time, t t . The function would return to me the temperature at that specific point in space at that moment in time. Note Like so many areas of math, even if the idea is quite straightforward, often, the notation can make things unnecessarily confusing. We'll be confronted by this again later in the course and although it's sometimes fairly arbitrary historical reasons that decide this, like different people inventing different parts of math at different times, it can also be because a particular notation style is more convenient for the specific application it was developed for. However, and here is where a lot of the problems seem to arise, in order to use and play with the interesting applications of math, it requires you to have done quite a large amount of often quite boring groundwork. Mathematical language is like learning any other language in that respect. You can't enjoy French poetry until you've learned a lot of French vocabulary and grammar including all its quirks and irregularities! Quite understandably, some people find this off-putting. This is made worse by the fact that most people have not even realized that there is math poetry waiting for them at the end of this algebra tunnel. Machine learning is a whole genre of this poetry, so stick with us for the rest of the specialization and you'll be ready to appreciate it and even write some of your own. We often see expressions such as f(x) = x^2 + 3 f(x) = x^2 + 3 It is rather absurd that you should somehow just know that f(x) f(x) means \" f f is a function of x x \" and not \" f f multiplied by x x \". Sometimes, this gets genuinely unclear when other bracketed terms appear in your expression. For example, in the expression f(x) = g(x) + h(x - a) f(x) = g(x) + h(x - a) you can assume that g g , h h , and a a are all not variables. Otherwise, we would have to write f(x, g, h, a) f(x, g, h, a) but you could only know for sure what was going on here if it was explained to you with more context. For example, is g g a function being applied to x x ? What about h h and a a over here? Learning to read between the lines and fill in the missing context is a skill you build up over time. Conclusions Calculus is simply the study of how these functions change with respect to their input variables and it allows you to investigate and manipulate them. But ultimately, it's just a set of tools. By the end of this course, you'll be using them yourself to model some real world data. Warning This video was confusing and likely unnecessary for anyone taking this course, as some high-school math is presumed. Might delete this section. Gradients and derivatives Rise Over Run Previously we said that calculus is just a set of tools for describing the relationship between a function and the change in its variables. In this lecture, we're going to explore what this means and how it might be useful. Let's start by having a look at a classic example, a speed vs. time graph for a moving car The most obvious thing this graph tells us is that the car's speed is not constant, as a constant speed would be represented with a flat horizontal line. Furthermore, starting from zero speed at zero time, this car's speed is initially increasing with time, which is another way of saying that it is accelerating . Towards the end of the time period, the car's speed is shown to be rapidly decreasing, meaning that it is decelerating . As we've already said, a horizontal line implies a constant speed and an acceleration of 0 0 . A steep positive slope represents increasing speed and an acceleration \\gt 1 \\gt 1 . Note In fact, acceleration can be defined as the local gradient of a speed-time graph. We refer to the gradient at a single point as the local gradient . We can illustrate this concept by drawing a tangent line, which is a straight line that touches the curve at a particular point In our example, after the initial acceleration, the car's speed reaches a peak and then begins to decelerate again. By recording the slope of these tangent lines at every point, we could plot an entirely new graph which would show us acceleration versus time rather than speed versus time. Note Before you look at such a plot, think about what this acceleration-time graph would look like for a car traveling at constant speed. For constant speed, we get a flat horizontal line on our speed-time graph. It's gradient is, therefore, zero and so the acceleration-time graph would also just be a horizontal line! Initially, the gradient is positive and fairly constant before it drops to zero at the peak. It then becomes negative for a period before returning to zero. Overlaying the acceleration-time graph on our axes, we get Note Don't forget the vertical axis for the blue line is speed and will have units of distance per time, whereas the vertical axis for the orange line is acceleration and will have units of distance per time squared. Because they have different units, we can scale either of these two lines vertically in this block, and the meaning would still be identical. However, these have been scaled just to make the most use of this plot area available. You can see the points at which the acceleration function is zero, i.e. where it crosses the horizontal axis, coincide with where the speed-time graph is flat and has zero gradient. Although we will be discussing the formal definition of a derivative in a later section, what we've just done by eye is the essence of calculus . We took a continuous function and described its slope at every point by constructing a new function, which is its derivative. We can, of course, plot the derivative of the acceleration function following the same procedure, which would give us the rate of change of acceleration (which we can also think of as being the second derivative of the speed) referred to jerk . Note Think of the jerky motion of a car as it stops and starts. Now, you may have never heard of this concept before, but hopefully, just by telling you that it's the derivative of the acceleration curve, this should be all you need to know to approximately sketch the jerk. Also very interesting is the idea of taking our baseline speed function and trying to imagine what function this would have been the gradient of, as in applying the inverse procedure to the one that we have just discussed. We can refer to this as the anti-derivative , which is closely related to something called the integral . For the example we are discussing here, it would represent the distance of the car from its starting position. This should make more sense when you consider that the change in distance with respect to time, i.e. the slope of the distance-time graph, is how much distance you are covering per unit time, or speed! Conclusions This analysis of slopes is all we are going to discuss in the section. Even though we haven't yet laid out the formal definition of calculus, you are already in a very strong position to start thinking about differentiation more formally. Definition of a derivative Tip Watch this video first before reading through this section. Now that we've explored the relationship between functions and their gradient, we should be ready to lay out a more formal definition of a derivative. All we're going to do is translate the understanding about gradients that we saw in the previous section into some mathematical notation that we can write down. We talked in the last section about horizontal lines having a gradient of zero, whilst upwards and downward sloping lines having positive or negative gradients respectively. We can write down a definition of this concept by taking the example of a linear function, which has the same gradient everywhere. If we start by picking any two points, we can say that the gradient of this line is equal to the amount that the function increases in this interval , divided by the length of the interval . This description is often just condensed to the expression \" rise over run \", where rise is the increase in the vertical direction, and run is the distance along the horizontal axis \\text{gradient} = \\frac{\\text{rise}}{\\text{run}} \\text{gradient} = \\frac{\\text{rise}}{\\text{run}} Fairly straightforward so far; but how does it relate to the more complicated functions we saw previously, where the gradient is different at every point? Let's pick a single point on our speed-time graph from earlier, which we'll call x x . The value of our function at this point is clearly just f(x) f(x) . We now need to pick a second point to draw our rise over run triangle. We can call the horizontal distance between our two points \\Delta x \\Delta x , where \\Delta \\Delta is being used to express a small change in something . By definition, our second point must be at position x + \\Delta x x + \\Delta x . We can also write down the vertical position of our second point as a function f f evaluated at our new location x + \\Delta x x + \\Delta x , i.e. f(x + \\Delta x) f(x + \\Delta x) . We can now build an expression for the approximate gradient of the function f f at our point x x , based on the rise over run gradient between point x x and \\Delta x \\Delta x The last step in this process is simply to notice that for nice smooth continuous functions like the one we're showing here, as \\Delta x \\Delta x gets smaller, the line connecting two points becomes a better and better approximation of the actual gradient at our point x x ! We can express this concept formally by using the limit notation scheme \\lim_{\\Delta x \\rightarrow 0} \\lim_{\\Delta x \\rightarrow 0} Our expression will give us a function for our gradient at any point we choose, which we write as f'(x) f'(x) or \\frac{d_f}{d_x} \\frac{d_f}{d_x} , depending on which notation scheme you prefer. This is a slightly strange concept as we're not allowing \\Delta x = 0 \\Delta x = 0 , because dividing by zero is not defined, but instead investigating what happens when x x is extremely close to zero . This, in a nutshell, is the process of differentiation . Note We could fill up several videos with more robust interpretations of this infinitely small but non-zero \\Delta x \\Delta x ; but for now, don't worry about it too much. We know more than enough to continue on our journey. Let's now put our new derivative expression into practice and see if it works. First, we should try this out on a linear function, once again, as we know that the answer is just going to be a constant. Given the function f(x) = 3x + 2 f(x) = 3x + 2 what is its derivative? f'(x) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{3(x + \\Delta x) + 2 - (3x + 2)}{\\Delta x}\\Biggl) f'(x) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{3(x + \\Delta x) + 2 - (3x + 2)}{\\Delta x}\\Biggl) = \\lim_{\\Delta x \\rightarrow 0}\\Biggl(\\frac{3x + 3 \\Delta x + 2 - 3x - 2}{\\Delta x}\\Biggl) = \\lim_{\\Delta x \\rightarrow 0}\\Biggl(\\frac{3x + 3 \\Delta x + 2 - 3x - 2}{\\Delta x}\\Biggl) = \\lim_{\\Delta x \\rightarrow 0}\\Biggl(\\frac{3 \\Delta x}{\\Delta x}\\Biggl) = \\lim_{\\Delta x \\rightarrow 0}\\Biggl(\\frac{3 \\Delta x}{\\Delta x}\\Biggl) = 3 = 3 Therefore, the gradient of our linear function is a constant, just as we expected. Note We actually differentiated two things at once. A 3x 3x term, and a + 2 + 2 term. We could have differentiated them separately, and then added them together, and still got the same result. This interchangeability of the approach is called the Sum Rule , and it's pretty handy. Let's now try a slightly more complicated example f(x) = 5x^2 f(x) = 5x^2 Again, lets take this thing and put it into the differentiation expression we derived earlier. So, f'(x) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{5(x + \\Delta x)^2 - 5x^2}{\\Delta x} \\Biggl) f'(x) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{5(x + \\Delta x)^2 - 5x^2}{\\Delta x} \\Biggl) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{ 5x^2 + 10 x \\Delta x + 5 \\Delta x^2 - 5x^2}{\\Delta x} \\Biggl) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{ 5x^2 + 10 x \\Delta x + 5 \\Delta x^2 - 5x^2}{\\Delta x} \\Biggl) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(10x + 5 \\Delta x \\Biggl) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(10x + 5 \\Delta x \\Biggl) = 10 x = 10 x We can generalize the lesson from this example to a rule for handling functions with powers of x x . For example, if we take the function f(x)= ax^b f(x)= ax^b and substitute it into our differentiation expression, we will find that the derivative is always f'(x) = abx^{b-1} f'(x) = abx^{b-1} The original power gets multiplied by the front and then the new power is just one less than it was before. This is known as the power rule , and you can put this into your calculus toolbox along with sum rule , which we saw earlier. Conclusions You've now seen two examples in which we apply the limit of rise over run method to differentiate two simple functions. As you can probably imagine, if we wanted to differentiate a long complicated expression, this process is going to become quite tedious. What we are going to see in later videos are more rules (like the sum rule and the power rule) which will help us speed up the process. However, before we do that, we're going to look at some special case functions, which differentiate in an interesting way. Differentiation examples & special cases In this section, we're going to run through three special cases : functions which give us interesting results when differentiated. Special case 1: discontinuities The first example we're going to work through is the function f(x) = \\frac{1}{x} f(x) = \\frac{1}{x} , which you can see plotted here Take a minute to notice that the gradient of this function is negative everywhere except the point x = 0 x = 0 , where it is undefined. On the negative side, the function drops down, presumably towards negative infinity, but then it somehow reemerges from above on the positive side. This sudden break in our otherwise smooth function is what we refer to as a discontinuity . We've mentioned already that dividing by zero is undefined, which means that this function simply doesn't have a value at the point x=0 x=0 . But, what about the gradient at this point? Well, let's sub our function into the differentiation expression to investigate f(x) = \\frac{1}{x} f(x) = \\frac{1}{x} \\Rightarrow f'(x) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{\\frac{1}{x + \\Delta x} - \\frac{1}{x}}{\\Delta x} \\Biggl) \\Rightarrow f'(x) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{\\frac{1}{x + \\Delta x} - \\frac{1}{x}}{\\Delta x} \\Biggl) \\Rightarrow f'(x) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{\\frac{-\\Delta x}{x(x + \\Delta x)}}{\\Delta x} \\Biggl) \\Rightarrow f'(x) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{\\frac{-\\Delta x}{x(x + \\Delta x)}}{\\Delta x} \\Biggl) \\Rightarrow f'(x) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{-1}{x^2 + x \\Delta x} \\Biggl) \\Rightarrow f'(x) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{-1}{x^2 + x \\Delta x} \\Biggl) \\Rightarrow f'(x) = \\frac{-1}{x^2} \\Rightarrow f'(x) = \\frac{-1}{x^2} As we realized just by looking, this derivative function is negative everywhere, and like our base function, the derivative is also undefined at x = 0 x = 0 ! Special case 2: exponentials For our second function, lets start by describing a few special properties of the function. The first property is that f(x) f(x) is always equal to the value of its own gradient f'(x) f'(x) . As it turns out, f(x) = 0 f(x) = 0 has this property, but it is not the function we are talking about. This means that our mystery function must always either be positive or negative, as if it ever tried to cross the horizontal axis, then both the function and the gradient would be zero, and we would be back to f(x) = 0 f(x) = 0 . Another property of our mystery function is that it is always increasing or always decreasing, i.e. it can never return to the same value again. Plenty of functions could fit these criteria, and focusing on the positive case, they all look something like this However, besides the zero function, there is only one function that will satisfy all our demands. This is the exponential function, f(x) = e^x f(x) = e^x , where e e is Euler's number, named after the 18th century Mathematician. Note The number e e , which is approximately 2.718 2.718 , is very important for the study of Calculus. But more than that, e e like \\pi \\pi , turns up all over mathematics and seems to be written all over the fabric of the universe. Differentiating e^x e^x gives us e^x e^x so, clearly, we can just keep differentiating this thing as many times as we'd like and nothing is going to change. This self similarity is going to come in very handy. Special case 3: trigonometric functions The last special case function that we're going to talk about are the trigonometric functions, Sine and Cosine . You may recall that for a right angled triangle, r\\sin(x) r\\sin(x) gives you the length of the opposite side to the angle Let's take a look at the function \\sin(x) \\sin(x) and see if we can work out what shape its derivative would be by eye. So, \\sin(x) \\sin(x) , starts with a positive gradient which gently decreases until at zero at the top of the bump, and then it starts being negative again until it gets to the bottom of the next bump, and so forth and so on As it turns out, the derivative of \\sin(x) \\sin(x) is actually just \\cos(x) \\cos(x) ! Now, what happens when we differentiate \\cos(x) \\cos(x) ? As it turns out, we get -\\sin(x) -\\sin(x) . Differentiating a third time gives us -\\cos(x) -\\cos(x) . Amazingly, differentiating a fourth time brings us all the way back to our original function, \\sin x \\sin x and then the pattern of repeats. This self similarity may remind you of the exponential function we discussed above, and that is because these trigonometric functions are actually just exponentials in disguise, albeit quite a convincing disguise. Note Indeed, \\sin(x) = \\frac{e^{ix} - e^{ix}}{2i} \\sin(x) = \\frac{e^{ix} - e^{ix}}{2i} . Conclusions Many of the details of the functions that we talked about in this section were skimmed over rather quickly. For the benefit of this particular course, all you need to understand is that differentiation is fundamentally quite a simple concept. Even when you might not be able to battle through all the algebra, you're ultimately still just looking for the rise over run gradient at each point. This pragmatic approach to calculus is going to come up again when we start talking about calculating gradients with computers. Time saving rules Tip Before going through this section, consider watching this and this video. Product rule Now that we've defined the differentiation operation in terms of an exact mathematical formula, it's become clear that even for relatively simple functions, calculating derivatives can be quite tedious. However, there exists convenient rules that allow us to avoid working through the limit of the rise over run operation whenever possible. So far, we've met the sum rule and the power rule . In this section, we will cover a convenient shortcut for differentiating the product of two functions, known as the product rule . Note It is of course possible to derive the product rule purely algebraically but it doesn't really help you to develop much insight into what's going on. Instead we will take a geometric approach to deriving the product rule. Imagine a rectangle where the length of one side is the function f(x) f(x) and the other side is the function g(x) g(x) . The product of these two functions, therefore, give us the rectangle's area, which we can call A(x) A(x) Now, consider that if we differentiate f(x)g(x) f(x)g(x) , what we're really looking for is the change in area of our rectangle as we vary x x . Let's see what happens to the area when we increase x x by some small amount, \\Delta x \\Delta x Note For the case we've shown, we've picked a particular friendly pair of functions, where they both happen to increase with x x . However, this won't necessarily always be the case. It does however, make drawing things a lot easier and the conclusions would ultimately be the same. We can now divide up our rectangle into four regions, one of which was our original area, A(x) A(x) . As the total edge length along the top, is now f(x + \\Delta x) f(x + \\Delta x) , this means that the width of the new region must be the difference between the original width and the new width. Of course, the same logic applies to the height We can now write an expression for just the extra area, which we will call \\Delta A \\Delta A . This is the sum of the area of the three new rectangles Although we are trying to avoid a drawn out conversation about limits, you should notice that as \\Delta x \\Delta x goes to 0 0 , although all of the new rectangles will shrink, it's the smallest rectangle that is going to shrink the fastest ! This is the intuition that justifies how we can ultimately ignore the small rectangle and leave its contribution to the area out of our differential expression altogether Now that we've got our expression for approximating \\Delta x \\Delta x , we return to our original question. What is the derivative of A A with respect to x x ? We want the limit of \\Delta A \\Delta A divided by x x , i.e. rise over run, which means we also need to divide the right hand side by delta x x \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{\\Delta A(x)}{\\Delta x} \\Biggl) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl( \\frac{f(x)(g(x + \\Delta x) - g(x)) + g(x)(f(x + \\Delta x) - f(x))}{\\Delta x} \\Biggl) \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{\\Delta A(x)}{\\Delta x} \\Biggl) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl( \\frac{f(x)(g(x + \\Delta x) - g(x)) + g(x)(f(x + \\Delta x) - f(x))}{\\Delta x} \\Biggl) We are so close at this point, all we need to do is slightly rearrange this equation. Firstly, by splitting it into two fractions, and then secondly, by moving f(x) f(x) and g(x) g(x) out of the numerators \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{\\Delta A(x)}{\\Delta x} \\Biggl) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl( f(x) \\frac{(g(x + \\Delta x) - g(x))}{\\Delta x} + g(x) \\frac{(f(x + \\Delta x) - f(x))}{\\Delta x} \\Biggl) \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{\\Delta A(x)}{\\Delta x} \\Biggl) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl( f(x) \\frac{(g(x + \\Delta x) - g(x))}{\\Delta x} + g(x) \\frac{(f(x + \\Delta x) - f(x))}{\\Delta x} \\Biggl) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(f(x)g'(x) + g(x)f'(x)\\Biggl) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(f(x)g'(x) + g(x)f'(x)\\Biggl) What I hope you can see now is that first part contains the definition of the derivative of g(x) g(x) , and the second part contains the derivative of f(x) f(x) . We're now ready to write down our final expression for the derivative of A A with respect to x x , which has now just been reduced to this A'(x) = f(x)g'(x) + g(x)f'(x) A'(x) = f(x)g'(x) + g(x)f'(x) Conclusions So, we can now add the product rule to the list of tools in our calculus toolbox. If we want to differentiate the product of two functions, f(x) f(x) and g(x) g(x) , we simply find the sum of f(x)g'(x) f(x)g'(x) and g(x)f'(x) g(x)f'(x) . This is going to come in really handy in later videos as we start to deal with some more complicated functions. See you then. Chain rule So far, we have learned about the sum rule , the power rule , and the product rule . In this section, we introduce the chain rule . Note After this, our toolbox will then be sufficiently well-stocked that we'll be ready to start tackling some heftier, more interesting problems. Sometimes, we use functions as the inputs of other functions. As you can probably imagine, describing this can get a little bit confusing. What we're going to do is give each of our functions meanings, so that we can hopefully keep track of what's going on. Consider the nested function h(p(m)) h(p(m)) We have the function h h , which is describing how happy I am, as a function of p p , how many pizzas I've eaten that day. Then, the pizzas I get to eat per day is itself a function of m m , which is how much money I make. We're still ultimately relating money to happiness, but via the concept of pizza. Note This nested function scenario comes up a whole lot in science and engineering, as you relate chains of concepts together. First I'm going to give you the function relating happiness and pizza, which has the following polynomial form: h(p) = - \\frac{1}{3}p^2 + p + \\frac{1}{5} h(p) = - \\frac{1}{3}p^2 + p + \\frac{1}{5} which is easily understandable from a plot What we can see is that, although without any pizza it's still possible to be happy, in principle, my peak happiness is with about one and a half pizzas. Any more pizza than this and I become less happy, and then, beyond about three pizzas, my happiness becomes rapidly negative. Next is our function relating pizza and money p(m) = e^m - 1 p(m) = e^m - 1 which is also fairly straightforward to understand by looking at a plot. If you've got no money, you can't buy any pizza. But the more money you have, your pizza purchasing power increases exponentially. As at first you can take advantage of bulk-buy discounts, but as you start getting really rich, you can buy your own pizza oven, and eventually even build your own pizza factory. What we'd like to know is, by considering how much money I have now, how much effort should I put into making more, if my aim is to be happy? To work this out, we're going to need to know what the rate of change of happiness is, with respect to money, which is of course just \\frac{dh}{dm} \\frac{dh}{dm} . For now, this relatively simple example, we could just directly substitute our pizza-money function into our happiness-pizza function, and then differentiate the resulting function directily, h(p(m)) = - \\frac{1}{3}(e^m - 1)^2 + (e^m - 1) + \\frac{1}{5} h(p(m)) = - \\frac{1}{3}(e^m - 1)^2 + (e^m - 1) + \\frac{1}{5} h'(p(m)) = \\frac{1}{3}e^m(5 - 2e^m) h'(p(m)) = \\frac{1}{3}e^m(5 - 2e^m) However, the chain rule provides us with a more elegant approach, which importantly will still work even for complicated functions, where direct substitution like this may not be an option. Consider the derivatives \\frac{dh}{dp} \\frac{dh}{dp} and \\frac{dp}{dm} \\frac{dp}{dm} . You'll notice that in this particular notation convention, where the derivatives are represented by quotients, the product of these two quantities looks like it would give you the desired function \\frac{dh}{dm} \\frac{dh}{dm} , i.e., \\frac{dh}{dp} \\times \\frac{dp}{dm} = \\frac{dh}{dm} \\frac{dh}{dp} \\times \\frac{dp}{dm} = \\frac{dh}{dm} And in actual fact, this is a perfectly sensible way to think about what's going on. This approach is called the chain rule, because, in a sense, we are making a chain of derivative relationships. Now, this is certainly not what you might describe as a formal derivation, but it is already good enough to enable you to make use of the chain rule effectively. So, let's now apply this rule to our function. Firstly, let's differentiate our two functions, h(p) = - \\frac{1}{3}p^2 + p + \\frac{1}{5} h(p) = - \\frac{1}{3}p^2 + p + \\frac{1}{5} \\Rightarrow \\frac{dh}{dp} = 1 - \\frac{2}{3}p \\Rightarrow \\frac{dh}{dp} = 1 - \\frac{2}{3}p p(m) = e^m - 1 p(m) = e^m - 1 \\Rightarrow \\frac{dp}{dm} = e^m \\Rightarrow \\frac{dp}{dm} = e^m And then multiplying these together is simple enough. \\frac{dh}{dp} \\times \\frac{dp}{dm} = \\Biggl(1 - \\frac{2}{3}p\\Biggl)e^m \\frac{dh}{dp} \\times \\frac{dp}{dm} = \\Biggl(1 - \\frac{2}{3}p\\Biggl)e^m However, we just need to remember that if we don't want pizzas, p p , to appear in our final expression, then we need to sub in our expression for p p in terms of m m . And then finally, by simply rearranging the terms, we recover the expression that we saw at the start of the video. \\frac{dh}{dm} = \\Biggl(1 - \\frac{2}{3}(e^m-1)\\Biggl)e^m \\frac{dh}{dm} = \\Biggl(1 - \\frac{2}{3}(e^m-1)\\Biggl)e^m = \\frac{1}{3}e^m(5 - 2e^m) = \\frac{1}{3}e^m(5 - 2e^m) Now, don't forget, that although for this simple example the chain rule didn't save us a huge amount of time, compared to substituting in before the differentiation, don't let that fool you. What's magic about the chain rule is that for some real-world applications, we may not have a nice analytical expression for our function. But we may still have the derivatives. So, being able to simply combine them with the chain rule becomes very powerful, indeed. Before we finish, let's have a quick look at our money-happiness function and its derivative on a graph. As we can see, if you're broke, and it's really worthwhile making some money, but the benefit of getting more, especially once you have enough pizza, decreases dramatically and quickly becomes negative. Conclusisons We now have a fourth and final tool for the module, and in the next section we'll be putting them all to use in an example. Assessment Taming a beast In this section, we're going to work through a nasty looking function that will require us to use all four of the time-saving rules that we've learned so far. To make matters worse, this function isn't going to describe anything familiar. So, we'll just be flying blind and have to trust the maths. This will hopefully give you the confidence to dive into questions in the following exercise. Consider the rather nasty function f(x) = \\frac{\\sin(2x^5 + 3x)}{e^{7x}} f(x) = \\frac{\\sin(2x^5 + 3x)}{e^{7x}} The essence of the sum, product and the chain rules are all about breaking your function down into manageable pieces. So, the first thing to spot is that although it is currently expressed as a fraction, we can rewrite f(x) f(x) as a product by moving the denominator up and raising it to the power of minus one. Note There is actually another rule specifically for dealing with fractions directly called the quotient rule, but it requires memorizing an extra expression. So we're not going to cover it in this course as you can always use the approach that we've used here and rewrite your friction as a product. Next, we'll split f(x) f(x) up into the two parts of the product and work out how to differentiate each parts separately, ready to apply the product rule later on. Let's start with the first part, which we can call g(x) g(x) . We've got the trigonometric functions sin sin applied to a polynomial 2x^5 + 3x 2x^5 + 3x . This is a classic target for the chain rule. So, all we need to do is take our function and split up into the two parts in order to apply the chain rule. g(u) = sin(u) g(u) = sin(u) u(x) = 2x^5 + 3x u(x) = 2x^5 + 3x Now, we've got these two separate functions, and we're going to want to differentiate each of them in order to apply the chain rule. So we say, g'(u) = cos(u) g'(u) = cos(u) u'(x) = 10x^4 + 3 u'(x) = 10x^4 + 3 Now we've got these two expressions, and I'm going to use a mix of notation for convenience. And I'm going to say, \\frac{dg}{du} \\times frac{du}{dx} = cos(u)(10x^4 + 3) \\frac{dg}{du} \\times frac{du}{dx} = cos(u)(10x^4 + 3) \\Rightarrow \\frac{dg}{dx} = cos(2x^5 + 3x)(10x^4 + 3) \\Rightarrow \\frac{dg}{dx} = cos(2x^5 + 3x)(10x^4 + 3) And that's it. We now have an expression for the derivative g(x) g(x) and already we've made use of the chain rule, the sum rule, and the power rule. Now, for the second half of our original expression, which we will call h(x) h(x) , once again, we can simply apply the chain rule after splitting our function up. So we can say that h(v) = e^v h(v) = e^v v(x) = -7x v(x) = -7x Now, once again, we've got our two functions. We just want to find the derivatives. So, h'(v) = e^v h'(v) = e^v v'(x) = -7 v'(x) = -7 So, combining these two back together and using different notation, we can say that \\frac{dh}{dv} \\times \\frac{dv}{du} = -7e^{-7x} \\frac{dh}{dv} \\times \\frac{dv}{du} = -7e^{-7x} As we now have expressions for the derivatives of both parts of our product, we can just apply the product rule to generate the final answer, and that's it. \\frac{df}{dx} = \\frac{dg}{dx}h + g\\frac{dh}{dx} \\frac{df}{dx} = \\frac{dg}{dx}h + g\\frac{dh}{dx} We could rearrange and factorize this in various fancy ways or even express it in terms of the original function. However, there is a saying amongst coders that I think can be applied quite generally, which is that premature optimization is the root of all evil, which in this case means, don't spend time tidying things up and rearranging them until that you're sure that you've finished making a mess. Conclusions I hope that you've managed to follow along with this example, and will now have the confidence to apply our four rules to other problems yourself. In calculus, it's often the case that some initially scary looking functions, like the ones we worked with just now, turn out to be easy to tame if you have the right tools. Meanwhile, other seemingly simple functions occasionally turn out to be beasts, but this can also be fun. So, happy hunting.","title":"Week 1"},{"location":"multivariate_calculus/week_1/#week-1-what-is-calculus","text":"Understanding calculus is central to understanding machine learning! You can think of calculus as a set of tools for analyzing the relationship between functions and their inputs . We start this module from the basics, by recalling what a function is and where we might encounter one. Following this, we talk about how the slope of a graph at a given point describes the rate of change of the output of the graphed function with respect to an input at that point. Using this visual intuition, we derive a robust mathematical definition of a derivative, which we then use to differentiate some interesting functions. Finally, by studying a few examples, we develop four handy time saving rules that enable us to speed up differentiation for many common scenarios. Learning Objectives Recall the definition of differentiation Apply differentiation to simple functions Describe the utility of time saving rules Apply sum , product and chain rule","title":"Week 1: What is calculus?"},{"location":"multivariate_calculus/week_1/#back-to-basics-functions","text":"","title":"Back to basics: functions"},{"location":"multivariate_calculus/week_1/#welcome-to-this-course","text":"Welcome to module one of six. We start right from the basics, but build you up fairly quickly to some interesting applications in modules five and six. This week, we'll be focusing on the fundamental theory of calculus , as well as some handy rules to speed things up. Where possible, we're going to try and represent the concepts graphically so that you can see the derivations rather than just reading them. This sometimes means that we'll be skimming over a few details, but we'll also provide you with links to the more rigorous descriptions so that you can check them out if you're interested.","title":"Welcome to this course"},{"location":"multivariate_calculus/week_1/#functions","text":"Tip Try watching this video at first for quickly building up your intuition for what calculus is all about. Before diving into calculus, we should first talk briefly about what functions are and where we use them. Essentially, a function is a relationship between some inputs and an output. For example, if I had a function for modeling the distribution of temperature in a room, I might input the x x , y y , and z z coordinates of a specific location I'm interested in as well as the time, t t . The function would return to me the temperature at that specific point in space at that moment in time. Note Like so many areas of math, even if the idea is quite straightforward, often, the notation can make things unnecessarily confusing. We'll be confronted by this again later in the course and although it's sometimes fairly arbitrary historical reasons that decide this, like different people inventing different parts of math at different times, it can also be because a particular notation style is more convenient for the specific application it was developed for. However, and here is where a lot of the problems seem to arise, in order to use and play with the interesting applications of math, it requires you to have done quite a large amount of often quite boring groundwork. Mathematical language is like learning any other language in that respect. You can't enjoy French poetry until you've learned a lot of French vocabulary and grammar including all its quirks and irregularities! Quite understandably, some people find this off-putting. This is made worse by the fact that most people have not even realized that there is math poetry waiting for them at the end of this algebra tunnel. Machine learning is a whole genre of this poetry, so stick with us for the rest of the specialization and you'll be ready to appreciate it and even write some of your own. We often see expressions such as f(x) = x^2 + 3 f(x) = x^2 + 3 It is rather absurd that you should somehow just know that f(x) f(x) means \" f f is a function of x x \" and not \" f f multiplied by x x \". Sometimes, this gets genuinely unclear when other bracketed terms appear in your expression. For example, in the expression f(x) = g(x) + h(x - a) f(x) = g(x) + h(x - a) you can assume that g g , h h , and a a are all not variables. Otherwise, we would have to write f(x, g, h, a) f(x, g, h, a) but you could only know for sure what was going on here if it was explained to you with more context. For example, is g g a function being applied to x x ? What about h h and a a over here? Learning to read between the lines and fill in the missing context is a skill you build up over time.","title":"Functions"},{"location":"multivariate_calculus/week_1/#conclusions","text":"Calculus is simply the study of how these functions change with respect to their input variables and it allows you to investigate and manipulate them. But ultimately, it's just a set of tools. By the end of this course, you'll be using them yourself to model some real world data. Warning This video was confusing and likely unnecessary for anyone taking this course, as some high-school math is presumed. Might delete this section.","title":"Conclusions"},{"location":"multivariate_calculus/week_1/#gradients-and-derivatives","text":"","title":"Gradients and derivatives"},{"location":"multivariate_calculus/week_1/#rise-over-run","text":"Previously we said that calculus is just a set of tools for describing the relationship between a function and the change in its variables. In this lecture, we're going to explore what this means and how it might be useful. Let's start by having a look at a classic example, a speed vs. time graph for a moving car The most obvious thing this graph tells us is that the car's speed is not constant, as a constant speed would be represented with a flat horizontal line. Furthermore, starting from zero speed at zero time, this car's speed is initially increasing with time, which is another way of saying that it is accelerating . Towards the end of the time period, the car's speed is shown to be rapidly decreasing, meaning that it is decelerating . As we've already said, a horizontal line implies a constant speed and an acceleration of 0 0 . A steep positive slope represents increasing speed and an acceleration \\gt 1 \\gt 1 . Note In fact, acceleration can be defined as the local gradient of a speed-time graph. We refer to the gradient at a single point as the local gradient . We can illustrate this concept by drawing a tangent line, which is a straight line that touches the curve at a particular point In our example, after the initial acceleration, the car's speed reaches a peak and then begins to decelerate again. By recording the slope of these tangent lines at every point, we could plot an entirely new graph which would show us acceleration versus time rather than speed versus time. Note Before you look at such a plot, think about what this acceleration-time graph would look like for a car traveling at constant speed. For constant speed, we get a flat horizontal line on our speed-time graph. It's gradient is, therefore, zero and so the acceleration-time graph would also just be a horizontal line! Initially, the gradient is positive and fairly constant before it drops to zero at the peak. It then becomes negative for a period before returning to zero. Overlaying the acceleration-time graph on our axes, we get Note Don't forget the vertical axis for the blue line is speed and will have units of distance per time, whereas the vertical axis for the orange line is acceleration and will have units of distance per time squared. Because they have different units, we can scale either of these two lines vertically in this block, and the meaning would still be identical. However, these have been scaled just to make the most use of this plot area available. You can see the points at which the acceleration function is zero, i.e. where it crosses the horizontal axis, coincide with where the speed-time graph is flat and has zero gradient. Although we will be discussing the formal definition of a derivative in a later section, what we've just done by eye is the essence of calculus . We took a continuous function and described its slope at every point by constructing a new function, which is its derivative. We can, of course, plot the derivative of the acceleration function following the same procedure, which would give us the rate of change of acceleration (which we can also think of as being the second derivative of the speed) referred to jerk . Note Think of the jerky motion of a car as it stops and starts. Now, you may have never heard of this concept before, but hopefully, just by telling you that it's the derivative of the acceleration curve, this should be all you need to know to approximately sketch the jerk. Also very interesting is the idea of taking our baseline speed function and trying to imagine what function this would have been the gradient of, as in applying the inverse procedure to the one that we have just discussed. We can refer to this as the anti-derivative , which is closely related to something called the integral . For the example we are discussing here, it would represent the distance of the car from its starting position. This should make more sense when you consider that the change in distance with respect to time, i.e. the slope of the distance-time graph, is how much distance you are covering per unit time, or speed!","title":"Rise Over Run"},{"location":"multivariate_calculus/week_1/#conclusions_1","text":"This analysis of slopes is all we are going to discuss in the section. Even though we haven't yet laid out the formal definition of calculus, you are already in a very strong position to start thinking about differentiation more formally.","title":"Conclusions"},{"location":"multivariate_calculus/week_1/#definition-of-a-derivative","text":"Tip Watch this video first before reading through this section. Now that we've explored the relationship between functions and their gradient, we should be ready to lay out a more formal definition of a derivative. All we're going to do is translate the understanding about gradients that we saw in the previous section into some mathematical notation that we can write down. We talked in the last section about horizontal lines having a gradient of zero, whilst upwards and downward sloping lines having positive or negative gradients respectively. We can write down a definition of this concept by taking the example of a linear function, which has the same gradient everywhere. If we start by picking any two points, we can say that the gradient of this line is equal to the amount that the function increases in this interval , divided by the length of the interval . This description is often just condensed to the expression \" rise over run \", where rise is the increase in the vertical direction, and run is the distance along the horizontal axis \\text{gradient} = \\frac{\\text{rise}}{\\text{run}} \\text{gradient} = \\frac{\\text{rise}}{\\text{run}} Fairly straightforward so far; but how does it relate to the more complicated functions we saw previously, where the gradient is different at every point? Let's pick a single point on our speed-time graph from earlier, which we'll call x x . The value of our function at this point is clearly just f(x) f(x) . We now need to pick a second point to draw our rise over run triangle. We can call the horizontal distance between our two points \\Delta x \\Delta x , where \\Delta \\Delta is being used to express a small change in something . By definition, our second point must be at position x + \\Delta x x + \\Delta x . We can also write down the vertical position of our second point as a function f f evaluated at our new location x + \\Delta x x + \\Delta x , i.e. f(x + \\Delta x) f(x + \\Delta x) . We can now build an expression for the approximate gradient of the function f f at our point x x , based on the rise over run gradient between point x x and \\Delta x \\Delta x The last step in this process is simply to notice that for nice smooth continuous functions like the one we're showing here, as \\Delta x \\Delta x gets smaller, the line connecting two points becomes a better and better approximation of the actual gradient at our point x x ! We can express this concept formally by using the limit notation scheme \\lim_{\\Delta x \\rightarrow 0} \\lim_{\\Delta x \\rightarrow 0} Our expression will give us a function for our gradient at any point we choose, which we write as f'(x) f'(x) or \\frac{d_f}{d_x} \\frac{d_f}{d_x} , depending on which notation scheme you prefer. This is a slightly strange concept as we're not allowing \\Delta x = 0 \\Delta x = 0 , because dividing by zero is not defined, but instead investigating what happens when x x is extremely close to zero . This, in a nutshell, is the process of differentiation . Note We could fill up several videos with more robust interpretations of this infinitely small but non-zero \\Delta x \\Delta x ; but for now, don't worry about it too much. We know more than enough to continue on our journey. Let's now put our new derivative expression into practice and see if it works. First, we should try this out on a linear function, once again, as we know that the answer is just going to be a constant. Given the function f(x) = 3x + 2 f(x) = 3x + 2 what is its derivative? f'(x) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{3(x + \\Delta x) + 2 - (3x + 2)}{\\Delta x}\\Biggl) f'(x) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{3(x + \\Delta x) + 2 - (3x + 2)}{\\Delta x}\\Biggl) = \\lim_{\\Delta x \\rightarrow 0}\\Biggl(\\frac{3x + 3 \\Delta x + 2 - 3x - 2}{\\Delta x}\\Biggl) = \\lim_{\\Delta x \\rightarrow 0}\\Biggl(\\frac{3x + 3 \\Delta x + 2 - 3x - 2}{\\Delta x}\\Biggl) = \\lim_{\\Delta x \\rightarrow 0}\\Biggl(\\frac{3 \\Delta x}{\\Delta x}\\Biggl) = \\lim_{\\Delta x \\rightarrow 0}\\Biggl(\\frac{3 \\Delta x}{\\Delta x}\\Biggl) = 3 = 3 Therefore, the gradient of our linear function is a constant, just as we expected. Note We actually differentiated two things at once. A 3x 3x term, and a + 2 + 2 term. We could have differentiated them separately, and then added them together, and still got the same result. This interchangeability of the approach is called the Sum Rule , and it's pretty handy. Let's now try a slightly more complicated example f(x) = 5x^2 f(x) = 5x^2 Again, lets take this thing and put it into the differentiation expression we derived earlier. So, f'(x) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{5(x + \\Delta x)^2 - 5x^2}{\\Delta x} \\Biggl) f'(x) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{5(x + \\Delta x)^2 - 5x^2}{\\Delta x} \\Biggl) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{ 5x^2 + 10 x \\Delta x + 5 \\Delta x^2 - 5x^2}{\\Delta x} \\Biggl) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{ 5x^2 + 10 x \\Delta x + 5 \\Delta x^2 - 5x^2}{\\Delta x} \\Biggl) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(10x + 5 \\Delta x \\Biggl) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(10x + 5 \\Delta x \\Biggl) = 10 x = 10 x We can generalize the lesson from this example to a rule for handling functions with powers of x x . For example, if we take the function f(x)= ax^b f(x)= ax^b and substitute it into our differentiation expression, we will find that the derivative is always f'(x) = abx^{b-1} f'(x) = abx^{b-1} The original power gets multiplied by the front and then the new power is just one less than it was before. This is known as the power rule , and you can put this into your calculus toolbox along with sum rule , which we saw earlier.","title":"Definition of a derivative"},{"location":"multivariate_calculus/week_1/#conclusions_2","text":"You've now seen two examples in which we apply the limit of rise over run method to differentiate two simple functions. As you can probably imagine, if we wanted to differentiate a long complicated expression, this process is going to become quite tedious. What we are going to see in later videos are more rules (like the sum rule and the power rule) which will help us speed up the process. However, before we do that, we're going to look at some special case functions, which differentiate in an interesting way.","title":"Conclusions"},{"location":"multivariate_calculus/week_1/#differentiation-examples-special-cases","text":"In this section, we're going to run through three special cases : functions which give us interesting results when differentiated.","title":"Differentiation examples &amp; special cases"},{"location":"multivariate_calculus/week_1/#special-case-1-discontinuities","text":"The first example we're going to work through is the function f(x) = \\frac{1}{x} f(x) = \\frac{1}{x} , which you can see plotted here Take a minute to notice that the gradient of this function is negative everywhere except the point x = 0 x = 0 , where it is undefined. On the negative side, the function drops down, presumably towards negative infinity, but then it somehow reemerges from above on the positive side. This sudden break in our otherwise smooth function is what we refer to as a discontinuity . We've mentioned already that dividing by zero is undefined, which means that this function simply doesn't have a value at the point x=0 x=0 . But, what about the gradient at this point? Well, let's sub our function into the differentiation expression to investigate f(x) = \\frac{1}{x} f(x) = \\frac{1}{x} \\Rightarrow f'(x) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{\\frac{1}{x + \\Delta x} - \\frac{1}{x}}{\\Delta x} \\Biggl) \\Rightarrow f'(x) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{\\frac{1}{x + \\Delta x} - \\frac{1}{x}}{\\Delta x} \\Biggl) \\Rightarrow f'(x) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{\\frac{-\\Delta x}{x(x + \\Delta x)}}{\\Delta x} \\Biggl) \\Rightarrow f'(x) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{\\frac{-\\Delta x}{x(x + \\Delta x)}}{\\Delta x} \\Biggl) \\Rightarrow f'(x) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{-1}{x^2 + x \\Delta x} \\Biggl) \\Rightarrow f'(x) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{-1}{x^2 + x \\Delta x} \\Biggl) \\Rightarrow f'(x) = \\frac{-1}{x^2} \\Rightarrow f'(x) = \\frac{-1}{x^2} As we realized just by looking, this derivative function is negative everywhere, and like our base function, the derivative is also undefined at x = 0 x = 0 !","title":"Special case 1: discontinuities"},{"location":"multivariate_calculus/week_1/#special-case-2-exponentials","text":"For our second function, lets start by describing a few special properties of the function. The first property is that f(x) f(x) is always equal to the value of its own gradient f'(x) f'(x) . As it turns out, f(x) = 0 f(x) = 0 has this property, but it is not the function we are talking about. This means that our mystery function must always either be positive or negative, as if it ever tried to cross the horizontal axis, then both the function and the gradient would be zero, and we would be back to f(x) = 0 f(x) = 0 . Another property of our mystery function is that it is always increasing or always decreasing, i.e. it can never return to the same value again. Plenty of functions could fit these criteria, and focusing on the positive case, they all look something like this However, besides the zero function, there is only one function that will satisfy all our demands. This is the exponential function, f(x) = e^x f(x) = e^x , where e e is Euler's number, named after the 18th century Mathematician. Note The number e e , which is approximately 2.718 2.718 , is very important for the study of Calculus. But more than that, e e like \\pi \\pi , turns up all over mathematics and seems to be written all over the fabric of the universe. Differentiating e^x e^x gives us e^x e^x so, clearly, we can just keep differentiating this thing as many times as we'd like and nothing is going to change. This self similarity is going to come in very handy.","title":"Special case 2: exponentials"},{"location":"multivariate_calculus/week_1/#special-case-3-trigonometric-functions","text":"The last special case function that we're going to talk about are the trigonometric functions, Sine and Cosine . You may recall that for a right angled triangle, r\\sin(x) r\\sin(x) gives you the length of the opposite side to the angle Let's take a look at the function \\sin(x) \\sin(x) and see if we can work out what shape its derivative would be by eye. So, \\sin(x) \\sin(x) , starts with a positive gradient which gently decreases until at zero at the top of the bump, and then it starts being negative again until it gets to the bottom of the next bump, and so forth and so on As it turns out, the derivative of \\sin(x) \\sin(x) is actually just \\cos(x) \\cos(x) ! Now, what happens when we differentiate \\cos(x) \\cos(x) ? As it turns out, we get -\\sin(x) -\\sin(x) . Differentiating a third time gives us -\\cos(x) -\\cos(x) . Amazingly, differentiating a fourth time brings us all the way back to our original function, \\sin x \\sin x and then the pattern of repeats. This self similarity may remind you of the exponential function we discussed above, and that is because these trigonometric functions are actually just exponentials in disguise, albeit quite a convincing disguise. Note Indeed, \\sin(x) = \\frac{e^{ix} - e^{ix}}{2i} \\sin(x) = \\frac{e^{ix} - e^{ix}}{2i} .","title":"Special case 3: trigonometric functions"},{"location":"multivariate_calculus/week_1/#conclusions_3","text":"Many of the details of the functions that we talked about in this section were skimmed over rather quickly. For the benefit of this particular course, all you need to understand is that differentiation is fundamentally quite a simple concept. Even when you might not be able to battle through all the algebra, you're ultimately still just looking for the rise over run gradient at each point. This pragmatic approach to calculus is going to come up again when we start talking about calculating gradients with computers.","title":"Conclusions"},{"location":"multivariate_calculus/week_1/#time-saving-rules","text":"Tip Before going through this section, consider watching this and this video.","title":"Time saving rules"},{"location":"multivariate_calculus/week_1/#product-rule","text":"Now that we've defined the differentiation operation in terms of an exact mathematical formula, it's become clear that even for relatively simple functions, calculating derivatives can be quite tedious. However, there exists convenient rules that allow us to avoid working through the limit of the rise over run operation whenever possible. So far, we've met the sum rule and the power rule . In this section, we will cover a convenient shortcut for differentiating the product of two functions, known as the product rule . Note It is of course possible to derive the product rule purely algebraically but it doesn't really help you to develop much insight into what's going on. Instead we will take a geometric approach to deriving the product rule. Imagine a rectangle where the length of one side is the function f(x) f(x) and the other side is the function g(x) g(x) . The product of these two functions, therefore, give us the rectangle's area, which we can call A(x) A(x) Now, consider that if we differentiate f(x)g(x) f(x)g(x) , what we're really looking for is the change in area of our rectangle as we vary x x . Let's see what happens to the area when we increase x x by some small amount, \\Delta x \\Delta x Note For the case we've shown, we've picked a particular friendly pair of functions, where they both happen to increase with x x . However, this won't necessarily always be the case. It does however, make drawing things a lot easier and the conclusions would ultimately be the same. We can now divide up our rectangle into four regions, one of which was our original area, A(x) A(x) . As the total edge length along the top, is now f(x + \\Delta x) f(x + \\Delta x) , this means that the width of the new region must be the difference between the original width and the new width. Of course, the same logic applies to the height We can now write an expression for just the extra area, which we will call \\Delta A \\Delta A . This is the sum of the area of the three new rectangles Although we are trying to avoid a drawn out conversation about limits, you should notice that as \\Delta x \\Delta x goes to 0 0 , although all of the new rectangles will shrink, it's the smallest rectangle that is going to shrink the fastest ! This is the intuition that justifies how we can ultimately ignore the small rectangle and leave its contribution to the area out of our differential expression altogether Now that we've got our expression for approximating \\Delta x \\Delta x , we return to our original question. What is the derivative of A A with respect to x x ? We want the limit of \\Delta A \\Delta A divided by x x , i.e. rise over run, which means we also need to divide the right hand side by delta x x \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{\\Delta A(x)}{\\Delta x} \\Biggl) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl( \\frac{f(x)(g(x + \\Delta x) - g(x)) + g(x)(f(x + \\Delta x) - f(x))}{\\Delta x} \\Biggl) \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{\\Delta A(x)}{\\Delta x} \\Biggl) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl( \\frac{f(x)(g(x + \\Delta x) - g(x)) + g(x)(f(x + \\Delta x) - f(x))}{\\Delta x} \\Biggl) We are so close at this point, all we need to do is slightly rearrange this equation. Firstly, by splitting it into two fractions, and then secondly, by moving f(x) f(x) and g(x) g(x) out of the numerators \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{\\Delta A(x)}{\\Delta x} \\Biggl) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl( f(x) \\frac{(g(x + \\Delta x) - g(x))}{\\Delta x} + g(x) \\frac{(f(x + \\Delta x) - f(x))}{\\Delta x} \\Biggl) \\lim_{\\Delta x \\rightarrow 0} \\Biggl(\\frac{\\Delta A(x)}{\\Delta x} \\Biggl) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl( f(x) \\frac{(g(x + \\Delta x) - g(x))}{\\Delta x} + g(x) \\frac{(f(x + \\Delta x) - f(x))}{\\Delta x} \\Biggl) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(f(x)g'(x) + g(x)f'(x)\\Biggl) = \\lim_{\\Delta x \\rightarrow 0} \\Biggl(f(x)g'(x) + g(x)f'(x)\\Biggl) What I hope you can see now is that first part contains the definition of the derivative of g(x) g(x) , and the second part contains the derivative of f(x) f(x) . We're now ready to write down our final expression for the derivative of A A with respect to x x , which has now just been reduced to this A'(x) = f(x)g'(x) + g(x)f'(x) A'(x) = f(x)g'(x) + g(x)f'(x)","title":"Product rule"},{"location":"multivariate_calculus/week_1/#conclusions_4","text":"So, we can now add the product rule to the list of tools in our calculus toolbox. If we want to differentiate the product of two functions, f(x) f(x) and g(x) g(x) , we simply find the sum of f(x)g'(x) f(x)g'(x) and g(x)f'(x) g(x)f'(x) . This is going to come in really handy in later videos as we start to deal with some more complicated functions. See you then.","title":"Conclusions"},{"location":"multivariate_calculus/week_1/#chain-rule","text":"So far, we have learned about the sum rule , the power rule , and the product rule . In this section, we introduce the chain rule . Note After this, our toolbox will then be sufficiently well-stocked that we'll be ready to start tackling some heftier, more interesting problems. Sometimes, we use functions as the inputs of other functions. As you can probably imagine, describing this can get a little bit confusing. What we're going to do is give each of our functions meanings, so that we can hopefully keep track of what's going on. Consider the nested function h(p(m)) h(p(m)) We have the function h h , which is describing how happy I am, as a function of p p , how many pizzas I've eaten that day. Then, the pizzas I get to eat per day is itself a function of m m , which is how much money I make. We're still ultimately relating money to happiness, but via the concept of pizza. Note This nested function scenario comes up a whole lot in science and engineering, as you relate chains of concepts together. First I'm going to give you the function relating happiness and pizza, which has the following polynomial form: h(p) = - \\frac{1}{3}p^2 + p + \\frac{1}{5} h(p) = - \\frac{1}{3}p^2 + p + \\frac{1}{5} which is easily understandable from a plot What we can see is that, although without any pizza it's still possible to be happy, in principle, my peak happiness is with about one and a half pizzas. Any more pizza than this and I become less happy, and then, beyond about three pizzas, my happiness becomes rapidly negative. Next is our function relating pizza and money p(m) = e^m - 1 p(m) = e^m - 1 which is also fairly straightforward to understand by looking at a plot. If you've got no money, you can't buy any pizza. But the more money you have, your pizza purchasing power increases exponentially. As at first you can take advantage of bulk-buy discounts, but as you start getting really rich, you can buy your own pizza oven, and eventually even build your own pizza factory. What we'd like to know is, by considering how much money I have now, how much effort should I put into making more, if my aim is to be happy? To work this out, we're going to need to know what the rate of change of happiness is, with respect to money, which is of course just \\frac{dh}{dm} \\frac{dh}{dm} . For now, this relatively simple example, we could just directly substitute our pizza-money function into our happiness-pizza function, and then differentiate the resulting function directily, h(p(m)) = - \\frac{1}{3}(e^m - 1)^2 + (e^m - 1) + \\frac{1}{5} h(p(m)) = - \\frac{1}{3}(e^m - 1)^2 + (e^m - 1) + \\frac{1}{5} h'(p(m)) = \\frac{1}{3}e^m(5 - 2e^m) h'(p(m)) = \\frac{1}{3}e^m(5 - 2e^m) However, the chain rule provides us with a more elegant approach, which importantly will still work even for complicated functions, where direct substitution like this may not be an option. Consider the derivatives \\frac{dh}{dp} \\frac{dh}{dp} and \\frac{dp}{dm} \\frac{dp}{dm} . You'll notice that in this particular notation convention, where the derivatives are represented by quotients, the product of these two quantities looks like it would give you the desired function \\frac{dh}{dm} \\frac{dh}{dm} , i.e., \\frac{dh}{dp} \\times \\frac{dp}{dm} = \\frac{dh}{dm} \\frac{dh}{dp} \\times \\frac{dp}{dm} = \\frac{dh}{dm} And in actual fact, this is a perfectly sensible way to think about what's going on. This approach is called the chain rule, because, in a sense, we are making a chain of derivative relationships. Now, this is certainly not what you might describe as a formal derivation, but it is already good enough to enable you to make use of the chain rule effectively. So, let's now apply this rule to our function. Firstly, let's differentiate our two functions, h(p) = - \\frac{1}{3}p^2 + p + \\frac{1}{5} h(p) = - \\frac{1}{3}p^2 + p + \\frac{1}{5} \\Rightarrow \\frac{dh}{dp} = 1 - \\frac{2}{3}p \\Rightarrow \\frac{dh}{dp} = 1 - \\frac{2}{3}p p(m) = e^m - 1 p(m) = e^m - 1 \\Rightarrow \\frac{dp}{dm} = e^m \\Rightarrow \\frac{dp}{dm} = e^m And then multiplying these together is simple enough. \\frac{dh}{dp} \\times \\frac{dp}{dm} = \\Biggl(1 - \\frac{2}{3}p\\Biggl)e^m \\frac{dh}{dp} \\times \\frac{dp}{dm} = \\Biggl(1 - \\frac{2}{3}p\\Biggl)e^m However, we just need to remember that if we don't want pizzas, p p , to appear in our final expression, then we need to sub in our expression for p p in terms of m m . And then finally, by simply rearranging the terms, we recover the expression that we saw at the start of the video. \\frac{dh}{dm} = \\Biggl(1 - \\frac{2}{3}(e^m-1)\\Biggl)e^m \\frac{dh}{dm} = \\Biggl(1 - \\frac{2}{3}(e^m-1)\\Biggl)e^m = \\frac{1}{3}e^m(5 - 2e^m) = \\frac{1}{3}e^m(5 - 2e^m) Now, don't forget, that although for this simple example the chain rule didn't save us a huge amount of time, compared to substituting in before the differentiation, don't let that fool you. What's magic about the chain rule is that for some real-world applications, we may not have a nice analytical expression for our function. But we may still have the derivatives. So, being able to simply combine them with the chain rule becomes very powerful, indeed. Before we finish, let's have a quick look at our money-happiness function and its derivative on a graph. As we can see, if you're broke, and it's really worthwhile making some money, but the benefit of getting more, especially once you have enough pizza, decreases dramatically and quickly becomes negative.","title":"Chain rule"},{"location":"multivariate_calculus/week_1/#conclusisons","text":"We now have a fourth and final tool for the module, and in the next section we'll be putting them all to use in an example.","title":"Conclusisons"},{"location":"multivariate_calculus/week_1/#assessment","text":"","title":"Assessment"},{"location":"multivariate_calculus/week_1/#taming-a-beast","text":"In this section, we're going to work through a nasty looking function that will require us to use all four of the time-saving rules that we've learned so far. To make matters worse, this function isn't going to describe anything familiar. So, we'll just be flying blind and have to trust the maths. This will hopefully give you the confidence to dive into questions in the following exercise. Consider the rather nasty function f(x) = \\frac{\\sin(2x^5 + 3x)}{e^{7x}} f(x) = \\frac{\\sin(2x^5 + 3x)}{e^{7x}} The essence of the sum, product and the chain rules are all about breaking your function down into manageable pieces. So, the first thing to spot is that although it is currently expressed as a fraction, we can rewrite f(x) f(x) as a product by moving the denominator up and raising it to the power of minus one. Note There is actually another rule specifically for dealing with fractions directly called the quotient rule, but it requires memorizing an extra expression. So we're not going to cover it in this course as you can always use the approach that we've used here and rewrite your friction as a product. Next, we'll split f(x) f(x) up into the two parts of the product and work out how to differentiate each parts separately, ready to apply the product rule later on. Let's start with the first part, which we can call g(x) g(x) . We've got the trigonometric functions sin sin applied to a polynomial 2x^5 + 3x 2x^5 + 3x . This is a classic target for the chain rule. So, all we need to do is take our function and split up into the two parts in order to apply the chain rule. g(u) = sin(u) g(u) = sin(u) u(x) = 2x^5 + 3x u(x) = 2x^5 + 3x Now, we've got these two separate functions, and we're going to want to differentiate each of them in order to apply the chain rule. So we say, g'(u) = cos(u) g'(u) = cos(u) u'(x) = 10x^4 + 3 u'(x) = 10x^4 + 3 Now we've got these two expressions, and I'm going to use a mix of notation for convenience. And I'm going to say, \\frac{dg}{du} \\times frac{du}{dx} = cos(u)(10x^4 + 3) \\frac{dg}{du} \\times frac{du}{dx} = cos(u)(10x^4 + 3) \\Rightarrow \\frac{dg}{dx} = cos(2x^5 + 3x)(10x^4 + 3) \\Rightarrow \\frac{dg}{dx} = cos(2x^5 + 3x)(10x^4 + 3) And that's it. We now have an expression for the derivative g(x) g(x) and already we've made use of the chain rule, the sum rule, and the power rule. Now, for the second half of our original expression, which we will call h(x) h(x) , once again, we can simply apply the chain rule after splitting our function up. So we can say that h(v) = e^v h(v) = e^v v(x) = -7x v(x) = -7x Now, once again, we've got our two functions. We just want to find the derivatives. So, h'(v) = e^v h'(v) = e^v v'(x) = -7 v'(x) = -7 So, combining these two back together and using different notation, we can say that \\frac{dh}{dv} \\times \\frac{dv}{du} = -7e^{-7x} \\frac{dh}{dv} \\times \\frac{dv}{du} = -7e^{-7x} As we now have expressions for the derivatives of both parts of our product, we can just apply the product rule to generate the final answer, and that's it. \\frac{df}{dx} = \\frac{dg}{dx}h + g\\frac{dh}{dx} \\frac{df}{dx} = \\frac{dg}{dx}h + g\\frac{dh}{dx} We could rearrange and factorize this in various fancy ways or even express it in terms of the original function. However, there is a saying amongst coders that I think can be applied quite generally, which is that premature optimization is the root of all evil, which in this case means, don't spend time tidying things up and rearranging them until that you're sure that you've finished making a mess.","title":"Taming a beast"},{"location":"multivariate_calculus/week_1/#conclusions_5","text":"I hope that you've managed to follow along with this example, and will now have the confidence to apply our four rules to other problems yourself. In calculus, it's often the case that some initially scary looking functions, like the ones we worked with just now, turn out to be easy to tame if you have the right tools. Meanwhile, other seemingly simple functions occasionally turn out to be beasts, but this can also be fun. So, happy hunting.","title":"Conclusions"},{"location":"multivariate_calculus/week_2/","text":"Week 2: Multivariate calculus Building on the foundations of the previous module, we can now generalize our calculus tools to handle multivariable systems. This means we can take a function with multiple inputs and determine the influence of each of them separately. It would not be unusual for a machine learning method to require the analysis of a function with thousands of inputs, so we will also introduce the linear algebra structures necessary for storing the results of our multivariate calculus analysis in an orderly fashion. Learning Objectives Recognize that differentiation can be applied to multiple variables in an equation. Use multivariate calculus tools on example equations. Recognize the utility of vector/matrix structures in multivariate calculus. Examine two dimensional problems using the Jacobian. Moving to multivariate Welcome to module two. If you're comfortable with the concept of differentiation, then in many ways, everything we add from here will just be extensions of this core idea, as well as some interesting applications. The title of this course is Multivariate Calculus, so you won't be surprised to hear that we are now going to generalize the concept of gradients to multivariable systems. With more than one variable to play with, we will now be able to use calculus to help us navigate around high-dimensional spaces. Note The words multi variable and multi variate are typically used interchangeably. There is, however, a subtle difference between these two terms, which is related to whether there are multiple input variables or multiple output variables or both . Typically, we use multi variate : when multiple outcome variables exists (outputs); and multi variable : when multiple explanatory variables exist (inputs). Variables, constants & context In the first module, we started by trying to develop a strong visual intuition relating derivatives to the gradient of a function at each point. We then followed this up by describing four handy rules to help speed up the process of finding derivatives ( sum rule , power rule , product rule , and chain rule ). However, all of the examples we looked at were for systems involving a single variable. We are now going to have to see what happens when we apply the same idea to systems with many variables, known as multivariate systems . Before we do that, we need to talk about what a variable is in the first place. Previously, we've shown examples of problems where one of the variables is a function of the other i.e., y = f(x) y = f(x) , but where it wouldn't necessarily make sense to say that x = g(y) x = g(y) . Example A vehicles speed is clearly a function of time, as in each time the vehicle can only have one speed. However, we can not say that time was a function of the vehicles speed, as there might be multiple times at which the vehicle was traveling any given speed. This is why we typically refer to the speed as a dependent variable, as it depends on time. Conversely, time can be thought of as an independent variable in this particular context. Typically, when you first learn calculus, you take functions containing variables and constants and then differentiate the dependent variables , such as speed, with respect to independent variables , like time. However, what gets labelled as a constant or a variable can be subtler than you might think and will require you to understand the context of the problem being described. Let's continue using the example of a car to see how this might come up. Consider the following (highly simplified) expression, F = ma + dv^2 F = ma + dv^2 Relating the force F F generated by a car's engine to its mass m m , acceleration a a , aerodynamic drive d d , and velocity v v . Which variables are constant, which variables are independent and which variables are dependent depends on context! From the perspective of the driver The cars speed and acceleration can be changed by pressing the accelerator pedal to adjust the force, but the mass and the drag are fixed features of the car's design. Therefore, in this context, we would call the force an independent variable, but speed and acceleration dependent variables as they are consequences of the applied force. Additionally, the mass and drag coefficients are clearly constants . Car designer If you are the cars designer looking to design each engine size in a new fleet, perhaps settle on a specific acceleration-speed target. In this context, while your force is still the independent variable, speed and acceleration are constants . Additionally the mass and drag have become variables which you can adjust by redesigning your car. We refer to these slightly confusing variable design constants as parameters . The key takeaway here is that you can, in principle, differentiate any term with respect to any other. So don't get caught off guard when things you thought were constants suddenly start to vary. Let's now look at a different example. Note We often think of varying parameters as exploring a family of similar functions rather than describing them as variables in their own right. Imagine that you wanted to manufacture a metal can. You need to understand the relationship between the various key design parameters. We can start by writing a reasonable approximation for the can's empty mass m m by breaking the area down into pieces. The circles on top and bottom are \\pi r^2 \\pi r^2 each. When we unroll the body we get a rectangle where the width must be the same as the circumference of the circle i.e. 2 \\pi r 2 \\pi r . Finally, call the height h h . Taking these areas and multiplying them by a thickness t t , we get the total volume of metal in the can. v = 2 \\pi r^2 t + 2 \\pi r h t v = 2 \\pi r^2 t + 2 \\pi r h t Finally, multiplying this by the metal's density \\rho \\rho , we get its mass m = 2 \\pi r^2 t \\rho + 2 \\pi r h t \\rho m = 2 \\pi r^2 t \\rho + 2 \\pi r h t \\rho At this point, what should we label as a constant or a variable? Well, with the exception of \\pi \\pi (which is definitely a constant for this universe) it's not entirely clear. We could in principle change any of the radius, the height, the wall thickness, or even the material's density. Lets do that, by calculating the derivative of the cans mass with respect to any of these variables. To calculate these partial derivatives (as they are called) all we do is differentiate with respect to a certain variable and consider all of the others to behave as constants. Starting with an easy one, let's try h h \\frac{\\partial m}{\\partial h} = 2 \\pi r \\rho \\frac{\\partial m}{\\partial h} = 2 \\pi r \\rho As the first term did not contain the variable h h (and we're treating all the other terms as constants) the partial deriviative just gives us the contants of the second term multiplied by the derivative of h h . As we can see from this expression, the partial derivative of m m with respect to h h no longer contains h h , which is what we'd expect as the mass will vary linearly with the height when all else is kept constant. Notice that instead of using the normal d d that we saw in last module, we must use the curly partial symbol ( \\partial \\partial ) which signifies that you've differentiated a function of more than one variable. Let's now find the partial derivative with respect to the other variables, starting with r r , \\frac{\\partial m}{\\partial r} = 4 \\pi r t \\rho + 2 \\pi h t \\rho \\frac{\\partial m}{\\partial r} = 4 \\pi r t \\rho + 2 \\pi h t \\rho then t t , \\frac{\\partial m}{\\partial t} = 2 \\pi r^2 \\rho + 2 \\pi r h \\rho \\frac{\\partial m}{\\partial t} = 2 \\pi r^2 \\rho + 2 \\pi r h \\rho and finally, \\rho \\rho \\frac{\\partial m}{\\partial \\rho} = 2 \\pi r^2 t + 2 \\pi r h t \\frac{\\partial m}{\\partial \\rho} = 2 \\pi r^2 t + 2 \\pi r h t Although this is quite a straightforward example, that's basically it for partial differentiation. It's no more complicated than the univariate calculus we met last module. The only difference being that you have to be careful to keep track of what you are considering to be held constant when you take each derivative. Conclusions I hope this short introduction to multivariate calculus has helped you see that this concept is nothing to be intimidated by. Partial differentiation is essentially just taking a multi dimensional problem and pretending that it's just a standard 1D problem when we consider each variable separately. I look forward to showing you the pretty amazing things that we can use this concept for later in the module. Differentiate with respect to anything We've already seen how to think about partial differentiation as just a simple extension of the single variable method that we derived in the last module. In this section, we're going to explore some slightly more complicated partial differential examples and we're also going to build something called the total derivative of a function. Let's dive straight in. Consider the function, f(x,y,z) = sin(x) e^{yz^2} f(x,y,z) = sin(x) e^{yz^2} Lets first find the derivatives with respect to each of these three variables, \\frac{\\partial f}{\\partial x} = \\cos(x)e^{yz^2} \\frac{\\partial f}{\\partial x} = \\cos(x)e^{yz^2} \\frac{\\partial f}{\\partial y} = \\sin(x)e^{yz^2}z^2 \\frac{\\partial f}{\\partial y} = \\sin(x)e^{yz^2}z^2 \\frac{\\partial f}{\\partial z} = \\sin(x)e^{yz^2}2yz \\frac{\\partial f}{\\partial z} = \\sin(x)e^{yz^2}2yz So, now that we have these three partial derivatives, we are going to introduce a new idea called the total derivative . Imagine that the variables x x , y y , and z z were actually themselves a function of a single other parameter, t t , where x = t-1 x = t-1 , y = t^2 y = t^2 and z = \\frac{1}{t} z = \\frac{1}{t} . What we're looking for is the derivative of x x with respect to t t . In this simple case, we could just substitute for all our three variables directly in terms of t t , simplify a little bit and then differentiate directly with respect to t t , which gives us, \\cos(t-1)e \\cos(t-1)e . However, in a more complicated scenario with many variables, the expression we needed to differentiate might become unmanageably complex. The alternative approach is to once again use the logic of chain rule to solve this problem, where the derivative with respect to a new variable, t t , is the sum of the chains of the other three variable. As shown in this expression, \\frac{df(x,y,z)}{dt} = \\frac{\\partial f}{\\partial x}\\frac{dx}{dt} + \\frac{\\partial f}{\\partial y}\\frac{dy}{dt} + \\frac{\\partial f}{\\partial z}\\frac{dz}{dt} \\frac{df(x,y,z)}{dt} = \\frac{\\partial f}{\\partial x}\\frac{dx}{dt} + \\frac{\\partial f}{\\partial y}\\frac{dy}{dt} + \\frac{\\partial f}{\\partial z}\\frac{dz}{dt} Since we've already got our three partial derivatives of f f with respect to x x , y y and z z , we just need to find the derivatives of the three variables with respect to t t , and we'll have all the things we need to evaluate our expression. \\frac{dx}{dt} = 1 \\frac{dx}{dt} = 1 \\frac{dx}{dy} = 2t \\frac{dx}{dy} = 2t \\frac{dx}{dz} = -t^{-2} \\frac{dx}{dz} = -t^{-2} Nothing hugely complicated here. However, when we then sub into our total derivative expression, the expression is a bit of a monster, \\frac{df(x,y,z)}{dt} = (cos(x)e^{yz^2})(1) + (\\sin(x)e^{yz^2}z^2)(2t) + (\\sin(x)e^{yz^2}2yz)(-t^{-2}) \\frac{df(x,y,z)}{dt} = (cos(x)e^{yz^2})(1) + (\\sin(x)e^{yz^2}z^2)(2t) + (\\sin(x)e^{yz^2}2yz)(-t^{-2}) However, after substituting for x x , y y and z z all in terms of t t and then simplifying again, we can see that the second and third terms are the same, just with opposite sign, and so they will cancel each other out (left as an exercise!). Then kind of amazingly, we arrive at the same result as we saw at the beginning of the lecture \\frac{df(x,y,z)}{dt} = \\cos(t-1)e \\frac{df(x,y,z)}{dt} = \\cos(t-1)e Conclusions Hopefully you are feeling reasonably comfortable with partial differentiation, and maybe you can even see why the total derivative function might be quite handy. We now have all the pieces that for us to the build our partial derivatives into something really useful. Jacobians - vectors of derivatives Previously, we saw that we can differentiate functions of multiple variables and that it isn't much harder than the univariate calculus we met at the start of the course. In this section, we're going to introduce the Jacobian , which brings in some of the ideas from linear algebra to build these partial derivatives into something particularly useful. The Jacobian The concept of the Jacobian can be applied to a variety of different problems. But in the context of getting started with optimization and machine learning, there is a particular scenario that comes up a lot, which is the Jacobian of a single function of many variables . In short, if you have a function of many variables i.e., f(x1, x2, x3, ...) f(x1, x2, x3, ...) then the Jacobian is simply a vector where each entry is the partial derivative of f f with respect to each one of those variables in turn. By convention, we write this as a row vector rather than a column vector, for reasons that will become clear later in the course J = [\\frac{\\partial f}{x_1}, \\frac{\\partial f}{x_2}, \\frac{\\partial f}{x_3}, ...] J = [\\frac{\\partial f}{x_1}, \\frac{\\partial f}{x_2}, \\frac{\\partial f}{x_3}, ...] Let's start by looking at a nice simple function f(x, y, z) = x^2y + 3z f(x, y, z) = x^2y + 3z to build the Jacobian, we just find each of the partial derivatives of the function one by one \\frac{df}{dx} = 2xy \\frac{df}{dx} = 2xy \\frac{df}{dy} = x^2 \\frac{df}{dy} = x^2 \\frac{df}{dz} = 3 \\frac{df}{dz} = 3 Now bringing all of those together, we end up with a Jacobian J J J = [2xy, x^2, 3] J = [2xy, x^2, 3] We now have an algebraic expression for a vector which when we give it a specific x, y, z x, y, z coordinate, will return a vector pointing in the direction of steepest slope of this function. For example, at the point (0, 0, 0) (0, 0, 0) , J(0, 0, 0) = [0, 0, 3] J(0, 0, 0) = [0, 0, 3] so our Jacobian is a vector of length 3 pointing directly in the z z direction. Some of the numerical methods that we will discuss later in this course require us to calculate Jacobians in hundreds of dimensions. However, even for the 3D example we just solved, graphical representation is already quite difficult. We are now going to drop down to 2 dimensions so that we can actually see what's going on here. To keep things interesting, we're going to look at a particularly complicated but rather attractive function Note Even though it is a bit of a monster, I hope you would agree that with the tools we've seen so far, we really could calculate the partial derivatives of this thing. But you wouldn't really learn anything new from grinding through this. Instead, we will look at the results graphically. Here is that same function, with the x x axis horizontal, the y y axis vertical and a color map indicating the value of z z , with the bright region suggesting high values and the dark region suggesting low values Although this plot is fairly clear, our intuition about the gradient is a bit lacking in this format. So let's briefly make things 3D, where the values of z z and now also represented by the height. As we said at the start of the video, the Jacobian is simply a vector that we can calculate for each location on this plot which points in the direction of the steepest uphill slope . Furthermore, the steeper the slope , the greater the magnitude of Jacobian at that point . Hold the image of this 3D space in your mind as we now go back to 2D. Rather than showing all of the grid points I used to plot the graph, lets instead convert to a contour plot, where just like a map of actual mountains, we will draw lines along the regions of the same height (which for us means the same value of z z ). This removes a lot of the clutter from the plot which is useful for the final step that I'd like to show you, which will be adding lots of Jacobian vectors on top of our contour plot. However, before I do that, let's take a look at these four points and see if your intuition is now in tune by guessing which one will have the Jacobian with the largest magnitude. Overlaying the Jacobian vector field, we can see that they are clearly all pointing uphill, away from the low dark regions and towards the high bright regions Also, we see that where the contour lines are tightly packed, this is where we find our largest Jacobian vectors such as at point A. Whereas the peaks of the mountains and in the bottom of the valleys or even out on a wide flat plains, our gradients, and therefore our Jacobians are small. Conclusions My hope is that this clear two-dimensional example will give you the confidence to trust the maths when we come up against much higher dimensional problems later in the course. See you then. Jacobian applied In this section, we're going to extend the concept of the Jacobian, from vectors up to matrices, which will allow us to describe the rates of change of a vector valued function. However, before we do this, we are first going to recap by applying what we've learned so far about Jacobians, to another simple system. If we consider the function f(x,y) = e^{-(x^2 + y^2)} f(x,y) = e^{-(x^2 + y^2)} then using our knowledge of partial differentiation, it's fairly straightforward to find its Jacobian vector J = [-2xe^{-(x^2+y^2)}, -2ye^{-(x^2+y^2)}] J = [-2xe^{-(x^2+y^2)}, -2ye^{-(x^2+y^2)}] We're now going to do the reverse of our approach from the last section and start by looking at the vector field in the Jacobians, and then see if we can understand how the function must look. Let's start by finding the Jacobian at a few specific points. Firstly, the point (-1, 1) (-1, 1) which we'll highlight on our axis in pink. Substituting these coordinates into our Jacobian expression and simplifying, we can see a vector pointing directly towards the origin. Next, if we move further out to the point (2, 2) (2, 2) and find the Jacobian, we are now going to get a much smaller vector but pointing once again directly at the origin Lastly, before I reveal the whole vector field, let's look at what's going on at the origin itself. Subbing in the point (0, 0) (0, 0) returns the zero vector, suggesting that the function is flat at this point, which must mean one of three things. Either, this point is a maximum, minimum, or something called a saddle, which we'll cover later in this module However, if we now reveal the rest of the Jacobian vector field, it becomes clear that the origin must be the maximum of this system Let's now move back to the colour map representation of this function, where the brighter regions represent high values of f. So, finally, we can remove the vector field and observe the function in 3D, which I hope matches up with your expectations. Next, we're going to build a Jacobian matrix which describes functions that take a vector as an input, but unlike our previous examples, also give a vector as the output . If we consider the two functions, u(x,y) = x - 2y u(x,y) = x - 2y v(x,y) = 3y - 2x v(x,y) = 3y - 2x we can think of these as two vector spaces, one containing vectors with coordinates in u, v u, v and the other with coordinates in x, y x, y , i.e., each point in x, y x, y has a corresponding location in u, v u, v . As we move around x, y x, y space, we would expect our corresponding path in u, v u, v space to be quite different, We can of course, make separate row vector Jacobians for u u and v v . However, as we are considering u u and v v to be components of a single vector, it makes more sense to extend our Jacobian by stacking these vectors as rows of a matrix like this J_u = \\begin{bmatrix} \\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\end{bmatrix} J_u = \\begin{bmatrix} \\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\end{bmatrix} J_v = \\begin{bmatrix} \\frac{\\partial v}{\\partial x} & \\frac{\\partial v}{\\partial y} \\end{bmatrix} J_v = \\begin{bmatrix} \\frac{\\partial v}{\\partial x} & \\frac{\\partial v}{\\partial y} \\end{bmatrix} J = \\begin{bmatrix} \\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\\\ \\frac{\\partial v}{\\partial x} & \\frac{\\partial v}{\\partial y}\\end{bmatrix} J = \\begin{bmatrix} \\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\\\ \\frac{\\partial v}{\\partial x} & \\frac{\\partial v}{\\partial y}\\end{bmatrix} Now that we have the structure and motivation for building a Jacobian matrix for vector valued functions, let's apply this to our example functions and see what we get. We have u(x, y) = x - 2y u(x, y) = x - 2y v(x,y) = 3y - 2x v(x,y) = 3y - 2x We can build the Jacobian from these two, by simply saying well, the Jacobian is going to be, J = \\begin{bmatrix} \\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\\\ \\frac{\\partial v}{\\partial x} & \\frac{\\partial v}{\\partial y}\\end{bmatrix} J = \\begin{bmatrix} \\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\\\ \\frac{\\partial v}{\\partial x} & \\frac{\\partial v}{\\partial y}\\end{bmatrix} = \\begin{bmatrix} 1 & -2 \\\\ -2 & 3\\end{bmatrix} = \\begin{bmatrix} 1 & -2 \\\\ -2 & 3\\end{bmatrix} Our Jacobian matrix no longer even contains any variables, which is what we should expect when we consider that clearly, both u u and v v are linear functions of x x and y y . So the gradient must be constant everywhere. Also, this matrix is just the linear transformation from xy xy space to uv uv space. So, if we were to apply the xy xy vector two three, we'd get the following, \\begin{bmatrix} 1 & -2 \\\\ -2 & 3\\end{bmatrix}\\begin{bmatrix} 2 \\\\ 3\\end{bmatrix} = \\begin{bmatrix} -4 \\\\ 5\\end{bmatrix} \\begin{bmatrix} 1 & -2 \\\\ -2 & 3\\end{bmatrix}\\begin{bmatrix} 2 \\\\ 3\\end{bmatrix} = \\begin{bmatrix} -4 \\\\ 5\\end{bmatrix} Now, this is all well and good. But of course, many of the functions that you'll be confronted with, will be highly nonlinear, and generally much more complicated than the simple linear example we've just looked at here. However, often they may still be smooth, which means that if we zoom in close enough, we can consider each little region of space to be approximately linear and therefore, by adding up all the contributions from the Jacobian determinants at each point in space, we can still calculate the change in the size of a region after transformation. A classic example of this occurs when transforming between cartesian and polar coordinate systems. If we have a vector expressed in terms of a radius r r , and the angle up from the x-axis is theta, but we'd like them expressed in terms of x x and y y instead. We can write the following expressions just by thinking about trigonometry Now, we can build the Jacobian matrix and take its determinant. The fact that the result is simply the radius r r , and not the function \\theta \\theta , tells us that as we move along r r , away from the origin, small regions of space will scale as a function of r r , which I hope will make a lot of sense to you when we look at our little animation here Conclusions That's all for this video. I hope you will now be able to build Jacobian vectors and matrices for yourself, with confidence in the exercises. And more importantly, that your intuition on the meaning of this concept, is starting to develop. See you next time.","title":"Week 2"},{"location":"multivariate_calculus/week_2/#week-2-multivariate-calculus","text":"Building on the foundations of the previous module, we can now generalize our calculus tools to handle multivariable systems. This means we can take a function with multiple inputs and determine the influence of each of them separately. It would not be unusual for a machine learning method to require the analysis of a function with thousands of inputs, so we will also introduce the linear algebra structures necessary for storing the results of our multivariate calculus analysis in an orderly fashion. Learning Objectives Recognize that differentiation can be applied to multiple variables in an equation. Use multivariate calculus tools on example equations. Recognize the utility of vector/matrix structures in multivariate calculus. Examine two dimensional problems using the Jacobian.","title":"Week 2: Multivariate calculus"},{"location":"multivariate_calculus/week_2/#moving-to-multivariate","text":"Welcome to module two. If you're comfortable with the concept of differentiation, then in many ways, everything we add from here will just be extensions of this core idea, as well as some interesting applications. The title of this course is Multivariate Calculus, so you won't be surprised to hear that we are now going to generalize the concept of gradients to multivariable systems. With more than one variable to play with, we will now be able to use calculus to help us navigate around high-dimensional spaces. Note The words multi variable and multi variate are typically used interchangeably. There is, however, a subtle difference between these two terms, which is related to whether there are multiple input variables or multiple output variables or both . Typically, we use multi variate : when multiple outcome variables exists (outputs); and multi variable : when multiple explanatory variables exist (inputs).","title":"Moving to multivariate"},{"location":"multivariate_calculus/week_2/#variables-constants-context","text":"In the first module, we started by trying to develop a strong visual intuition relating derivatives to the gradient of a function at each point. We then followed this up by describing four handy rules to help speed up the process of finding derivatives ( sum rule , power rule , product rule , and chain rule ). However, all of the examples we looked at were for systems involving a single variable. We are now going to have to see what happens when we apply the same idea to systems with many variables, known as multivariate systems . Before we do that, we need to talk about what a variable is in the first place. Previously, we've shown examples of problems where one of the variables is a function of the other i.e., y = f(x) y = f(x) , but where it wouldn't necessarily make sense to say that x = g(y) x = g(y) . Example A vehicles speed is clearly a function of time, as in each time the vehicle can only have one speed. However, we can not say that time was a function of the vehicles speed, as there might be multiple times at which the vehicle was traveling any given speed. This is why we typically refer to the speed as a dependent variable, as it depends on time. Conversely, time can be thought of as an independent variable in this particular context. Typically, when you first learn calculus, you take functions containing variables and constants and then differentiate the dependent variables , such as speed, with respect to independent variables , like time. However, what gets labelled as a constant or a variable can be subtler than you might think and will require you to understand the context of the problem being described. Let's continue using the example of a car to see how this might come up. Consider the following (highly simplified) expression, F = ma + dv^2 F = ma + dv^2 Relating the force F F generated by a car's engine to its mass m m , acceleration a a , aerodynamic drive d d , and velocity v v . Which variables are constant, which variables are independent and which variables are dependent depends on context!","title":"Variables, constants &amp; context"},{"location":"multivariate_calculus/week_2/#from-the-perspective-of-the-driver","text":"The cars speed and acceleration can be changed by pressing the accelerator pedal to adjust the force, but the mass and the drag are fixed features of the car's design. Therefore, in this context, we would call the force an independent variable, but speed and acceleration dependent variables as they are consequences of the applied force. Additionally, the mass and drag coefficients are clearly constants .","title":"From the perspective of the driver"},{"location":"multivariate_calculus/week_2/#car-designer","text":"If you are the cars designer looking to design each engine size in a new fleet, perhaps settle on a specific acceleration-speed target. In this context, while your force is still the independent variable, speed and acceleration are constants . Additionally the mass and drag have become variables which you can adjust by redesigning your car. We refer to these slightly confusing variable design constants as parameters . The key takeaway here is that you can, in principle, differentiate any term with respect to any other. So don't get caught off guard when things you thought were constants suddenly start to vary. Let's now look at a different example. Note We often think of varying parameters as exploring a family of similar functions rather than describing them as variables in their own right. Imagine that you wanted to manufacture a metal can. You need to understand the relationship between the various key design parameters. We can start by writing a reasonable approximation for the can's empty mass m m by breaking the area down into pieces. The circles on top and bottom are \\pi r^2 \\pi r^2 each. When we unroll the body we get a rectangle where the width must be the same as the circumference of the circle i.e. 2 \\pi r 2 \\pi r . Finally, call the height h h . Taking these areas and multiplying them by a thickness t t , we get the total volume of metal in the can. v = 2 \\pi r^2 t + 2 \\pi r h t v = 2 \\pi r^2 t + 2 \\pi r h t Finally, multiplying this by the metal's density \\rho \\rho , we get its mass m = 2 \\pi r^2 t \\rho + 2 \\pi r h t \\rho m = 2 \\pi r^2 t \\rho + 2 \\pi r h t \\rho At this point, what should we label as a constant or a variable? Well, with the exception of \\pi \\pi (which is definitely a constant for this universe) it's not entirely clear. We could in principle change any of the radius, the height, the wall thickness, or even the material's density. Lets do that, by calculating the derivative of the cans mass with respect to any of these variables. To calculate these partial derivatives (as they are called) all we do is differentiate with respect to a certain variable and consider all of the others to behave as constants. Starting with an easy one, let's try h h \\frac{\\partial m}{\\partial h} = 2 \\pi r \\rho \\frac{\\partial m}{\\partial h} = 2 \\pi r \\rho As the first term did not contain the variable h h (and we're treating all the other terms as constants) the partial deriviative just gives us the contants of the second term multiplied by the derivative of h h . As we can see from this expression, the partial derivative of m m with respect to h h no longer contains h h , which is what we'd expect as the mass will vary linearly with the height when all else is kept constant. Notice that instead of using the normal d d that we saw in last module, we must use the curly partial symbol ( \\partial \\partial ) which signifies that you've differentiated a function of more than one variable. Let's now find the partial derivative with respect to the other variables, starting with r r , \\frac{\\partial m}{\\partial r} = 4 \\pi r t \\rho + 2 \\pi h t \\rho \\frac{\\partial m}{\\partial r} = 4 \\pi r t \\rho + 2 \\pi h t \\rho then t t , \\frac{\\partial m}{\\partial t} = 2 \\pi r^2 \\rho + 2 \\pi r h \\rho \\frac{\\partial m}{\\partial t} = 2 \\pi r^2 \\rho + 2 \\pi r h \\rho and finally, \\rho \\rho \\frac{\\partial m}{\\partial \\rho} = 2 \\pi r^2 t + 2 \\pi r h t \\frac{\\partial m}{\\partial \\rho} = 2 \\pi r^2 t + 2 \\pi r h t Although this is quite a straightforward example, that's basically it for partial differentiation. It's no more complicated than the univariate calculus we met last module. The only difference being that you have to be careful to keep track of what you are considering to be held constant when you take each derivative.","title":"Car designer"},{"location":"multivariate_calculus/week_2/#conclusions","text":"I hope this short introduction to multivariate calculus has helped you see that this concept is nothing to be intimidated by. Partial differentiation is essentially just taking a multi dimensional problem and pretending that it's just a standard 1D problem when we consider each variable separately. I look forward to showing you the pretty amazing things that we can use this concept for later in the module.","title":"Conclusions"},{"location":"multivariate_calculus/week_2/#differentiate-with-respect-to-anything","text":"We've already seen how to think about partial differentiation as just a simple extension of the single variable method that we derived in the last module. In this section, we're going to explore some slightly more complicated partial differential examples and we're also going to build something called the total derivative of a function. Let's dive straight in. Consider the function, f(x,y,z) = sin(x) e^{yz^2} f(x,y,z) = sin(x) e^{yz^2} Lets first find the derivatives with respect to each of these three variables, \\frac{\\partial f}{\\partial x} = \\cos(x)e^{yz^2} \\frac{\\partial f}{\\partial x} = \\cos(x)e^{yz^2} \\frac{\\partial f}{\\partial y} = \\sin(x)e^{yz^2}z^2 \\frac{\\partial f}{\\partial y} = \\sin(x)e^{yz^2}z^2 \\frac{\\partial f}{\\partial z} = \\sin(x)e^{yz^2}2yz \\frac{\\partial f}{\\partial z} = \\sin(x)e^{yz^2}2yz So, now that we have these three partial derivatives, we are going to introduce a new idea called the total derivative . Imagine that the variables x x , y y , and z z were actually themselves a function of a single other parameter, t t , where x = t-1 x = t-1 , y = t^2 y = t^2 and z = \\frac{1}{t} z = \\frac{1}{t} . What we're looking for is the derivative of x x with respect to t t . In this simple case, we could just substitute for all our three variables directly in terms of t t , simplify a little bit and then differentiate directly with respect to t t , which gives us, \\cos(t-1)e \\cos(t-1)e . However, in a more complicated scenario with many variables, the expression we needed to differentiate might become unmanageably complex. The alternative approach is to once again use the logic of chain rule to solve this problem, where the derivative with respect to a new variable, t t , is the sum of the chains of the other three variable. As shown in this expression, \\frac{df(x,y,z)}{dt} = \\frac{\\partial f}{\\partial x}\\frac{dx}{dt} + \\frac{\\partial f}{\\partial y}\\frac{dy}{dt} + \\frac{\\partial f}{\\partial z}\\frac{dz}{dt} \\frac{df(x,y,z)}{dt} = \\frac{\\partial f}{\\partial x}\\frac{dx}{dt} + \\frac{\\partial f}{\\partial y}\\frac{dy}{dt} + \\frac{\\partial f}{\\partial z}\\frac{dz}{dt} Since we've already got our three partial derivatives of f f with respect to x x , y y and z z , we just need to find the derivatives of the three variables with respect to t t , and we'll have all the things we need to evaluate our expression. \\frac{dx}{dt} = 1 \\frac{dx}{dt} = 1 \\frac{dx}{dy} = 2t \\frac{dx}{dy} = 2t \\frac{dx}{dz} = -t^{-2} \\frac{dx}{dz} = -t^{-2} Nothing hugely complicated here. However, when we then sub into our total derivative expression, the expression is a bit of a monster, \\frac{df(x,y,z)}{dt} = (cos(x)e^{yz^2})(1) + (\\sin(x)e^{yz^2}z^2)(2t) + (\\sin(x)e^{yz^2}2yz)(-t^{-2}) \\frac{df(x,y,z)}{dt} = (cos(x)e^{yz^2})(1) + (\\sin(x)e^{yz^2}z^2)(2t) + (\\sin(x)e^{yz^2}2yz)(-t^{-2}) However, after substituting for x x , y y and z z all in terms of t t and then simplifying again, we can see that the second and third terms are the same, just with opposite sign, and so they will cancel each other out (left as an exercise!). Then kind of amazingly, we arrive at the same result as we saw at the beginning of the lecture \\frac{df(x,y,z)}{dt} = \\cos(t-1)e \\frac{df(x,y,z)}{dt} = \\cos(t-1)e","title":"Differentiate with respect to anything"},{"location":"multivariate_calculus/week_2/#conclusions_1","text":"Hopefully you are feeling reasonably comfortable with partial differentiation, and maybe you can even see why the total derivative function might be quite handy. We now have all the pieces that for us to the build our partial derivatives into something really useful.","title":"Conclusions"},{"location":"multivariate_calculus/week_2/#jacobians-vectors-of-derivatives","text":"Previously, we saw that we can differentiate functions of multiple variables and that it isn't much harder than the univariate calculus we met at the start of the course. In this section, we're going to introduce the Jacobian , which brings in some of the ideas from linear algebra to build these partial derivatives into something particularly useful.","title":"Jacobians - vectors of derivatives"},{"location":"multivariate_calculus/week_2/#the-jacobian","text":"The concept of the Jacobian can be applied to a variety of different problems. But in the context of getting started with optimization and machine learning, there is a particular scenario that comes up a lot, which is the Jacobian of a single function of many variables . In short, if you have a function of many variables i.e., f(x1, x2, x3, ...) f(x1, x2, x3, ...) then the Jacobian is simply a vector where each entry is the partial derivative of f f with respect to each one of those variables in turn. By convention, we write this as a row vector rather than a column vector, for reasons that will become clear later in the course J = [\\frac{\\partial f}{x_1}, \\frac{\\partial f}{x_2}, \\frac{\\partial f}{x_3}, ...] J = [\\frac{\\partial f}{x_1}, \\frac{\\partial f}{x_2}, \\frac{\\partial f}{x_3}, ...] Let's start by looking at a nice simple function f(x, y, z) = x^2y + 3z f(x, y, z) = x^2y + 3z to build the Jacobian, we just find each of the partial derivatives of the function one by one \\frac{df}{dx} = 2xy \\frac{df}{dx} = 2xy \\frac{df}{dy} = x^2 \\frac{df}{dy} = x^2 \\frac{df}{dz} = 3 \\frac{df}{dz} = 3 Now bringing all of those together, we end up with a Jacobian J J J = [2xy, x^2, 3] J = [2xy, x^2, 3] We now have an algebraic expression for a vector which when we give it a specific x, y, z x, y, z coordinate, will return a vector pointing in the direction of steepest slope of this function. For example, at the point (0, 0, 0) (0, 0, 0) , J(0, 0, 0) = [0, 0, 3] J(0, 0, 0) = [0, 0, 3] so our Jacobian is a vector of length 3 pointing directly in the z z direction. Some of the numerical methods that we will discuss later in this course require us to calculate Jacobians in hundreds of dimensions. However, even for the 3D example we just solved, graphical representation is already quite difficult. We are now going to drop down to 2 dimensions so that we can actually see what's going on here. To keep things interesting, we're going to look at a particularly complicated but rather attractive function Note Even though it is a bit of a monster, I hope you would agree that with the tools we've seen so far, we really could calculate the partial derivatives of this thing. But you wouldn't really learn anything new from grinding through this. Instead, we will look at the results graphically. Here is that same function, with the x x axis horizontal, the y y axis vertical and a color map indicating the value of z z , with the bright region suggesting high values and the dark region suggesting low values Although this plot is fairly clear, our intuition about the gradient is a bit lacking in this format. So let's briefly make things 3D, where the values of z z and now also represented by the height. As we said at the start of the video, the Jacobian is simply a vector that we can calculate for each location on this plot which points in the direction of the steepest uphill slope . Furthermore, the steeper the slope , the greater the magnitude of Jacobian at that point . Hold the image of this 3D space in your mind as we now go back to 2D. Rather than showing all of the grid points I used to plot the graph, lets instead convert to a contour plot, where just like a map of actual mountains, we will draw lines along the regions of the same height (which for us means the same value of z z ). This removes a lot of the clutter from the plot which is useful for the final step that I'd like to show you, which will be adding lots of Jacobian vectors on top of our contour plot. However, before I do that, let's take a look at these four points and see if your intuition is now in tune by guessing which one will have the Jacobian with the largest magnitude. Overlaying the Jacobian vector field, we can see that they are clearly all pointing uphill, away from the low dark regions and towards the high bright regions Also, we see that where the contour lines are tightly packed, this is where we find our largest Jacobian vectors such as at point A. Whereas the peaks of the mountains and in the bottom of the valleys or even out on a wide flat plains, our gradients, and therefore our Jacobians are small.","title":"The Jacobian"},{"location":"multivariate_calculus/week_2/#conclusions_2","text":"My hope is that this clear two-dimensional example will give you the confidence to trust the maths when we come up against much higher dimensional problems later in the course. See you then.","title":"Conclusions"},{"location":"multivariate_calculus/week_2/#jacobian-applied","text":"In this section, we're going to extend the concept of the Jacobian, from vectors up to matrices, which will allow us to describe the rates of change of a vector valued function. However, before we do this, we are first going to recap by applying what we've learned so far about Jacobians, to another simple system. If we consider the function f(x,y) = e^{-(x^2 + y^2)} f(x,y) = e^{-(x^2 + y^2)} then using our knowledge of partial differentiation, it's fairly straightforward to find its Jacobian vector J = [-2xe^{-(x^2+y^2)}, -2ye^{-(x^2+y^2)}] J = [-2xe^{-(x^2+y^2)}, -2ye^{-(x^2+y^2)}] We're now going to do the reverse of our approach from the last section and start by looking at the vector field in the Jacobians, and then see if we can understand how the function must look. Let's start by finding the Jacobian at a few specific points. Firstly, the point (-1, 1) (-1, 1) which we'll highlight on our axis in pink. Substituting these coordinates into our Jacobian expression and simplifying, we can see a vector pointing directly towards the origin. Next, if we move further out to the point (2, 2) (2, 2) and find the Jacobian, we are now going to get a much smaller vector but pointing once again directly at the origin Lastly, before I reveal the whole vector field, let's look at what's going on at the origin itself. Subbing in the point (0, 0) (0, 0) returns the zero vector, suggesting that the function is flat at this point, which must mean one of three things. Either, this point is a maximum, minimum, or something called a saddle, which we'll cover later in this module However, if we now reveal the rest of the Jacobian vector field, it becomes clear that the origin must be the maximum of this system Let's now move back to the colour map representation of this function, where the brighter regions represent high values of f. So, finally, we can remove the vector field and observe the function in 3D, which I hope matches up with your expectations. Next, we're going to build a Jacobian matrix which describes functions that take a vector as an input, but unlike our previous examples, also give a vector as the output . If we consider the two functions, u(x,y) = x - 2y u(x,y) = x - 2y v(x,y) = 3y - 2x v(x,y) = 3y - 2x we can think of these as two vector spaces, one containing vectors with coordinates in u, v u, v and the other with coordinates in x, y x, y , i.e., each point in x, y x, y has a corresponding location in u, v u, v . As we move around x, y x, y space, we would expect our corresponding path in u, v u, v space to be quite different, We can of course, make separate row vector Jacobians for u u and v v . However, as we are considering u u and v v to be components of a single vector, it makes more sense to extend our Jacobian by stacking these vectors as rows of a matrix like this J_u = \\begin{bmatrix} \\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\end{bmatrix} J_u = \\begin{bmatrix} \\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\end{bmatrix} J_v = \\begin{bmatrix} \\frac{\\partial v}{\\partial x} & \\frac{\\partial v}{\\partial y} \\end{bmatrix} J_v = \\begin{bmatrix} \\frac{\\partial v}{\\partial x} & \\frac{\\partial v}{\\partial y} \\end{bmatrix} J = \\begin{bmatrix} \\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\\\ \\frac{\\partial v}{\\partial x} & \\frac{\\partial v}{\\partial y}\\end{bmatrix} J = \\begin{bmatrix} \\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\\\ \\frac{\\partial v}{\\partial x} & \\frac{\\partial v}{\\partial y}\\end{bmatrix} Now that we have the structure and motivation for building a Jacobian matrix for vector valued functions, let's apply this to our example functions and see what we get. We have u(x, y) = x - 2y u(x, y) = x - 2y v(x,y) = 3y - 2x v(x,y) = 3y - 2x We can build the Jacobian from these two, by simply saying well, the Jacobian is going to be, J = \\begin{bmatrix} \\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\\\ \\frac{\\partial v}{\\partial x} & \\frac{\\partial v}{\\partial y}\\end{bmatrix} J = \\begin{bmatrix} \\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\\\ \\frac{\\partial v}{\\partial x} & \\frac{\\partial v}{\\partial y}\\end{bmatrix} = \\begin{bmatrix} 1 & -2 \\\\ -2 & 3\\end{bmatrix} = \\begin{bmatrix} 1 & -2 \\\\ -2 & 3\\end{bmatrix} Our Jacobian matrix no longer even contains any variables, which is what we should expect when we consider that clearly, both u u and v v are linear functions of x x and y y . So the gradient must be constant everywhere. Also, this matrix is just the linear transformation from xy xy space to uv uv space. So, if we were to apply the xy xy vector two three, we'd get the following, \\begin{bmatrix} 1 & -2 \\\\ -2 & 3\\end{bmatrix}\\begin{bmatrix} 2 \\\\ 3\\end{bmatrix} = \\begin{bmatrix} -4 \\\\ 5\\end{bmatrix} \\begin{bmatrix} 1 & -2 \\\\ -2 & 3\\end{bmatrix}\\begin{bmatrix} 2 \\\\ 3\\end{bmatrix} = \\begin{bmatrix} -4 \\\\ 5\\end{bmatrix} Now, this is all well and good. But of course, many of the functions that you'll be confronted with, will be highly nonlinear, and generally much more complicated than the simple linear example we've just looked at here. However, often they may still be smooth, which means that if we zoom in close enough, we can consider each little region of space to be approximately linear and therefore, by adding up all the contributions from the Jacobian determinants at each point in space, we can still calculate the change in the size of a region after transformation. A classic example of this occurs when transforming between cartesian and polar coordinate systems. If we have a vector expressed in terms of a radius r r , and the angle up from the x-axis is theta, but we'd like them expressed in terms of x x and y y instead. We can write the following expressions just by thinking about trigonometry Now, we can build the Jacobian matrix and take its determinant. The fact that the result is simply the radius r r , and not the function \\theta \\theta , tells us that as we move along r r , away from the origin, small regions of space will scale as a function of r r , which I hope will make a lot of sense to you when we look at our little animation here","title":"Jacobian applied"},{"location":"multivariate_calculus/week_2/#conclusions_3","text":"That's all for this video. I hope you will now be able to build Jacobian vectors and matrices for yourself, with confidence in the exercises. And more importantly, that your intuition on the meaning of this concept, is starting to develop. See you next time.","title":"Conclusions"}]}