{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A single resource, in the form of a simple website built with GitHub Pages, which will hopefully serve as a complete crash course on the various areas of mathematics essential to understanding machine learning. To do this, I will use the Mathematics for Machine Learning Specialization on Coursera as my guide, but also pull from other resources, such as Khan Academy and 3Blue1Brown 's various video playlists on Youtube. The end goal will be to produce a simple site that someone (including myself) could use to quickly bring themselves up to speed on the fundamental mathematical concepts necessary for machine learning. The target audience are those who have at least some highschool math, but who should really have taken introductory courses on Linear Algebra and Calculus in college. The website can be accessed here . Full credit to the team behind the Mathematics for Machine Learning Specialization course on Coursera for creating such an awesome resource. I highly encourage anyone who needs to brush up on their mathematics for machine learning to check that course out. Notebooks Notebooks contains Jupyter notebooks for each course in the specialization , which each contain python implementations for many of the discussed concepts. Attribution Just like the Mathematics for Machine Learning Specialization , This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License .","title":"About"},{"location":"#notebooks","text":"Notebooks contains Jupyter notebooks for each course in the specialization , which each contain python implementations for many of the discussed concepts.","title":"Notebooks"},{"location":"#attribution","text":"Just like the Mathematics for Machine Learning Specialization , This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License .","title":"Attribution"},{"location":"calculus/","text":"Coming soon!","title":"Calculus"},{"location":"calculus/#coming-soon","text":"","title":"Coming soon!"},{"location":"linear_algebra/course_resources/","text":"Course resources There are lots of useful web resources on linear algebra . Typically they go a bit slower or have a different emphasis or way of explaining things, but it can be handy to see how someone else explains something. Khan Academy is a great resource right up to 1st or 2nd year undergraduate material. For this course, there's a handy group of videos here . Grant Sanderson has a great series of videos developing mathematical intuition on YouTube, which you can reach through his site here . Wikipedia gets better every year - and the linear algebra wikipedia pages are actually pretty good.","title":"Course Resources"},{"location":"linear_algebra/course_resources/#course-resources","text":"There are lots of useful web resources on linear algebra . Typically they go a bit slower or have a different emphasis or way of explaining things, but it can be handy to see how someone else explains something. Khan Academy is a great resource right up to 1st or 2nd year undergraduate material. For this course, there's a handy group of videos here . Grant Sanderson has a great series of videos developing mathematical intuition on YouTube, which you can reach through his site here . Wikipedia gets better every year - and the linear algebra wikipedia pages are actually pretty good.","title":"Course resources"},{"location":"linear_algebra/week_1/","text":"Week 1: Introduction to Linear Algebra In this first module we look at how linear algebra is relevant to machine learning and data science. Then we'll wind up the module with an initial introduction to vectors. Throughout, we're focussing on developing your mathematical intuition, not of crunching through algebra or doing long pen-and-paper examples. For many of these operations, there are callable functions in Python that can do the math - the point is to appreciate what they do and how they work, so that when things go wrong or there are special cases, you can understand why and what to do. Learning Objectives Recall how machine learning and vectors and matrices are related Interpret how changes in the model parameters affect the quality of the fit to the training data Recognize that variations in the model parameters are vectors on the response surface - that vectors are a generic concept not limited to a physical real space Use substitution / elimination to solve a fairly easy linear algebra problem Understand how to add vectors and multiply by a scalar number The relationship between machine learning, linear algebra, vectors and matrices Motivations for linear algebra Lets take a look at the types of problems we might want to solve, in order to expose what linear algebra is and how it might help us to solve them. Toy problem 1 The first problem we might think of is price discovery . We can illustrate this problem with a toy example. Say we go shopping on two occasions, and the first time we buy two apples and three bananas and they cost eight Euros 2a + 3b = 8 2a + 3b = 8 and the second time we buy ten apples and one banana, for a cost of 13 Euros. 10a + 1b = 13 10a + 1b = 13 The a a 's and b b 's here, are the price of a single apple and a single banana. What we're going to have to do is solve these simultaneous equations in order to discover the price of individual apples and bananas . Now in the general case, with lots of different types of items and lots of shopping trips, finding out the prices might be quite hard . This is an example of a linear algebra problem. I have some constant linear coefficients here (2, 10, 3, 1), that relate the input variables , a a and b b , to the outputs 8 and 13. That is if, we think about a vector [a,b] [a,b] that describes the prices of apples and bananas, we can write this down as a matrix problem where the 2, 3 is my first trip, and the 10, 1 is my second trip, \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix} 8 \\\\\\ 3\\end{bmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix} 8 \\\\\\ 3\\end{bmatrix} What we're going to do over the course of weeks one to three, is to look at these mathematical objects, vectors and matrices , in order to understand what they are and how to work with them. Toy problem 2 Another type of problem we might be interested in solving is fitting an equation to some data . In fact, with neural networks and machine learning, we want the computer to, in effect, not only fit the equation to the data but to figure out what equation to use . Let's say, we have some data like this histogram here: This looks like a population with an average and some variation . A common problem we might want to solve is how to find the optimal value of the parameters in the equation describing this line, i.e., the ones that fit the data in the histogram best. That might be really handy, because with that fitted equation we'd have an easy \"portable\" description of the population we could carry around, without needing all the original data which would free us, for example, from privacy concerns. Conclusions In this video, we've set up two problems that can be solved with linear algebra. First, the problem of solving simultaneous equations . And secondly, the optimization problem of fitting and equation with some parameters to data. These problems and others will motivate our work right through the course on linear algebra, and it's partner course on multivariate calculus . Geometric and Numeric Interpretations It is helpful to draw a distinction from the numerical operations we can perform using linear algebra, and the geometric intuitions underlying them (which are frequently not taught in may introductory courses). Roughly speaking, the geometric understanding or intuition is what lets us judge what tools to use to solve specific problems, feel why they work, and know how to interpret the results. The numerical understanding is what lets us actually carry through the application of those tools. If you learn linear algebra without getting a solid foundation in that geometric understanding, the problems can go unnoticed for a while, until you go deeper into whatever field you happen to pursue (e.g. computer science, engineering, statistics, economics, etc.), at which point you may feel disheartened by your lack of understanding of the fundamentals of linear algebra. With linear algebra (much like trigonometry, for example), there are a handful of useful visual/geometric intuitions underlying much of the subject. When you digest these and really understand the relationship between the geometry and the numbers, the details of the subject as well as how it's used in practice start to feel a lot more reasonable. Note Full credit for this section goes to 3Blue1Brown . Video here . Vectors The first thing we need to do in this course on linear algebra is to get a handle on vectors , which will turn out to be really useful to us in solving the linear algebra problems we introduced earlier (along with many more!). That is, problems described by equations which are linear in their coefficients , such as most fitting parameters. Tip This section maps most closely the the set of Khan Academy courses here . Take these for more practice. Getting a handle on vectors We're going to first step back and look in some detail at the sort of things we're trying to do with data. And why those vectors you first learned about in high school were even relevant. This will hopefully make all the work with vectors later on in the course a lot more intuitive. Tip This is not a great introduction to vectors (IMO). I recommend you first watch this video, then come back and read this section. Let's go back to that simpler problem from the last video, the histogram distribution of heights of people in the population: Say we wanted to try fitting that distribution with an equation describing the variation of height in the population. It turns our that such an equation has just two parameters; one describing the center of the distribution (the average ), which we'll call \\mu \\mu , and one describing how wide it is (or the variance ), which we'll call \\sigma \\sigma . This equation turns out to be the equation for the normal or ( Gaussian ) distribution : f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}} f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}} So how do we arrive at the best possible values for \\mu \\mu and \\sigma \\sigma ? Well, one way is gradient descent . If we think of some goodness value which measures how well our parameters fit our data (say, the mean squared error ) we could imagine plotting this goodness value as a function of our parameters, often called a cost or loss function. The closer our loss function is to zero, the better our parameters fit our data. Gradient descent allows us to choose values for our parameters that minimize the error , as measured by our loss function, by taking small incremental steps towards the bottom of the parameter space defined by our loss function. Note Gradient descent on a 3D surface. This process involves computing the partial derivative of our loss function w.r.t w.r.t to all possible parameters (also known as the gradient ). If our parameters are stored in a vector, \\begin{bmatrix}\\mu&\\sigma \\end{bmatrix} \\begin{bmatrix}\\mu&\\sigma \\end{bmatrix} we could subtract from this vector the vector of gradients, \\begin{bmatrix}\\frac{\\partial f}{\\partial \\mu} & \\frac{\\partial f}{\\partial \\sigma}\\end{bmatrix} \\begin{bmatrix}\\frac{\\partial f}{\\partial \\mu} & \\frac{\\partial f}{\\partial \\sigma}\\end{bmatrix} in order to complete the computation in (effectively) one step. So vectors (and calculus) give us a computational means of navigating a parameter space, in this case by determining the set of parameters for a function f(x) f(x) which best explain the data. Vectors as abstract lists of numbers We can also think of vectors as simply lists of numbers . For example, we could describe a car in terms of its price, top speed, safety rating, emissions performance, etc. and store these numbers in a single vector . car = \\begin{bmatrix}\\text{price,} & \\text{top speed,} & \\text{safety rating, }& ...\\end{bmatrix} car = \\begin{bmatrix}\\text{price,} & \\text{top speed,} & \\text{safety rating, }& ...\\end{bmatrix} Note This is more of a 'computer science' perspective of vectors. To summarize, a vector is, at the simplest level: lists of numbers something which moves in a space of parameters Operations with vectors Lets now explore the operations we can do with vectors, how these mathematical operations define what vectors are in the first place, and the sort of spaces they can apply to. We can think of a vector as an object that moves us about space. This could be a physical space, or a space of data (often called a vector space ). Note At school, you probably thought of a vector as something that moved you around a physical space, but in computer and data science, we generalize that idea to think of a vector as just a list of attributes of an objects. More formally, mathematics generalizes the definition of a vector to be an object for which the following two operations are defined : addition multiplication by a scalar Tip This is really important, so make sure you understand it. If a mathematical object can be added to another object of the same type, and it can be scaled (i.e. multiplied by a scaler), then its a vector! Vector addition Intuitively, we can introduce vector addition as being the resulting vector of the two vectors we want to add ( s s and r r ) being placed head-to-tail , s + r s + r . Multiplication by a scalar Multiplying a vector by a scalar is also easy to understand. In this case, we simply multiply all elements of our vector r r by the scalar, a a for example. Coordinate systems At this point, it's convenient to define a coordinate system . Imagine we had two dimensions defined by the vectors: \\hat i = \\begin{bmatrix}1 \\\\\\ 0 \\end{bmatrix} \\; \\hat j = \\begin{bmatrix} 1 \\\\\\ 0 \\end{bmatrix} \\hat i = \\begin{bmatrix}1 \\\\\\ 0 \\end{bmatrix} \\; \\hat j = \\begin{bmatrix} 1 \\\\\\ 0 \\end{bmatrix} Note These are known as basis vectors , and they define the basis . We could define any vector in this 2D space using the vectors \\hat i \\hat i and \\hat j \\hat j . For example, the vector \\begin{bmatrix}3 \\\\\\ 2 \\end{bmatrix} = 3 \\hat i + 2 \\hat j \\begin{bmatrix}3 \\\\\\ 2 \\end{bmatrix} = 3 \\hat i + 2 \\hat j Note This is also a extremely important point. A vector space is itself defined by vectors . We will explore this further later in the course. This also nicely illustrates that vectors are associative , meaning, the sum of a series of vectors is the same regardless of the order we add them in, e.g., \\begin{bmatrix} 3 \\\\\\ 2 \\end{bmatrix} = 3 \\hat i + 2 \\hat{j} = 2 \\hat{j} + 3 \\hat i \\begin{bmatrix} 3 \\\\\\ 2 \\end{bmatrix} = 3 \\hat i + 2 \\hat{j} = 2 \\hat{j} + 3 \\hat i Conclusions We've defined two fundamental operations that vectors satisfy: addition , (e.g. r + s r + s ), and multiplication by a scalar , (e.g. 2r 2r ). We've noted that it can be useful to define a coordinate system in which to do our addition and scaling , e.g., r = \\begin{bmatrix}3 \\\\\\ 2 \\end{bmatrix} = 3 \\hat i + 2 \\hat{j} r = \\begin{bmatrix}3 \\\\\\ 2 \\end{bmatrix} = 3 \\hat i + 2 \\hat{j} using these fundamental basis vectors, \\hat i \\hat i and \\hat{j} \\hat{j} , and explored the properties that this implies, like associativity of addition and subtraction. We've also seen that although, perhaps, it's easiest to think of vector operations geometrically , we don't have to do it in a real (number) space. We can also define vector operations on vectors that list different types of objects, like the attributes of a house .","title":"Week 1"},{"location":"linear_algebra/week_1/#week-1-introduction-to-linear-algebra","text":"In this first module we look at how linear algebra is relevant to machine learning and data science. Then we'll wind up the module with an initial introduction to vectors. Throughout, we're focussing on developing your mathematical intuition, not of crunching through algebra or doing long pen-and-paper examples. For many of these operations, there are callable functions in Python that can do the math - the point is to appreciate what they do and how they work, so that when things go wrong or there are special cases, you can understand why and what to do. Learning Objectives Recall how machine learning and vectors and matrices are related Interpret how changes in the model parameters affect the quality of the fit to the training data Recognize that variations in the model parameters are vectors on the response surface - that vectors are a generic concept not limited to a physical real space Use substitution / elimination to solve a fairly easy linear algebra problem Understand how to add vectors and multiply by a scalar number","title":"Week 1: Introduction to Linear Algebra"},{"location":"linear_algebra/week_1/#the-relationship-between-machine-learning-linear-algebra-vectors-and-matrices","text":"","title":"The relationship between machine learning, linear algebra, vectors and matrices"},{"location":"linear_algebra/week_1/#motivations-for-linear-algebra","text":"Lets take a look at the types of problems we might want to solve, in order to expose what linear algebra is and how it might help us to solve them. Toy problem 1 The first problem we might think of is price discovery . We can illustrate this problem with a toy example. Say we go shopping on two occasions, and the first time we buy two apples and three bananas and they cost eight Euros 2a + 3b = 8 2a + 3b = 8 and the second time we buy ten apples and one banana, for a cost of 13 Euros. 10a + 1b = 13 10a + 1b = 13 The a a 's and b b 's here, are the price of a single apple and a single banana. What we're going to have to do is solve these simultaneous equations in order to discover the price of individual apples and bananas . Now in the general case, with lots of different types of items and lots of shopping trips, finding out the prices might be quite hard . This is an example of a linear algebra problem. I have some constant linear coefficients here (2, 10, 3, 1), that relate the input variables , a a and b b , to the outputs 8 and 13. That is if, we think about a vector [a,b] [a,b] that describes the prices of apples and bananas, we can write this down as a matrix problem where the 2, 3 is my first trip, and the 10, 1 is my second trip, \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix} 8 \\\\\\ 3\\end{bmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix} 8 \\\\\\ 3\\end{bmatrix} What we're going to do over the course of weeks one to three, is to look at these mathematical objects, vectors and matrices , in order to understand what they are and how to work with them. Toy problem 2 Another type of problem we might be interested in solving is fitting an equation to some data . In fact, with neural networks and machine learning, we want the computer to, in effect, not only fit the equation to the data but to figure out what equation to use . Let's say, we have some data like this histogram here: This looks like a population with an average and some variation . A common problem we might want to solve is how to find the optimal value of the parameters in the equation describing this line, i.e., the ones that fit the data in the histogram best. That might be really handy, because with that fitted equation we'd have an easy \"portable\" description of the population we could carry around, without needing all the original data which would free us, for example, from privacy concerns.","title":"Motivations for linear algebra"},{"location":"linear_algebra/week_1/#conclusions","text":"In this video, we've set up two problems that can be solved with linear algebra. First, the problem of solving simultaneous equations . And secondly, the optimization problem of fitting and equation with some parameters to data. These problems and others will motivate our work right through the course on linear algebra, and it's partner course on multivariate calculus .","title":"Conclusions"},{"location":"linear_algebra/week_1/#geometric-and-numeric-interpretations","text":"It is helpful to draw a distinction from the numerical operations we can perform using linear algebra, and the geometric intuitions underlying them (which are frequently not taught in may introductory courses). Roughly speaking, the geometric understanding or intuition is what lets us judge what tools to use to solve specific problems, feel why they work, and know how to interpret the results. The numerical understanding is what lets us actually carry through the application of those tools. If you learn linear algebra without getting a solid foundation in that geometric understanding, the problems can go unnoticed for a while, until you go deeper into whatever field you happen to pursue (e.g. computer science, engineering, statistics, economics, etc.), at which point you may feel disheartened by your lack of understanding of the fundamentals of linear algebra. With linear algebra (much like trigonometry, for example), there are a handful of useful visual/geometric intuitions underlying much of the subject. When you digest these and really understand the relationship between the geometry and the numbers, the details of the subject as well as how it's used in practice start to feel a lot more reasonable. Note Full credit for this section goes to 3Blue1Brown . Video here .","title":"Geometric and Numeric Interpretations"},{"location":"linear_algebra/week_1/#vectors","text":"The first thing we need to do in this course on linear algebra is to get a handle on vectors , which will turn out to be really useful to us in solving the linear algebra problems we introduced earlier (along with many more!). That is, problems described by equations which are linear in their coefficients , such as most fitting parameters. Tip This section maps most closely the the set of Khan Academy courses here . Take these for more practice.","title":"Vectors"},{"location":"linear_algebra/week_1/#getting-a-handle-on-vectors","text":"We're going to first step back and look in some detail at the sort of things we're trying to do with data. And why those vectors you first learned about in high school were even relevant. This will hopefully make all the work with vectors later on in the course a lot more intuitive. Tip This is not a great introduction to vectors (IMO). I recommend you first watch this video, then come back and read this section. Let's go back to that simpler problem from the last video, the histogram distribution of heights of people in the population: Say we wanted to try fitting that distribution with an equation describing the variation of height in the population. It turns our that such an equation has just two parameters; one describing the center of the distribution (the average ), which we'll call \\mu \\mu , and one describing how wide it is (or the variance ), which we'll call \\sigma \\sigma . This equation turns out to be the equation for the normal or ( Gaussian ) distribution : f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}} f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}} So how do we arrive at the best possible values for \\mu \\mu and \\sigma \\sigma ? Well, one way is gradient descent . If we think of some goodness value which measures how well our parameters fit our data (say, the mean squared error ) we could imagine plotting this goodness value as a function of our parameters, often called a cost or loss function. The closer our loss function is to zero, the better our parameters fit our data. Gradient descent allows us to choose values for our parameters that minimize the error , as measured by our loss function, by taking small incremental steps towards the bottom of the parameter space defined by our loss function. Note Gradient descent on a 3D surface. This process involves computing the partial derivative of our loss function w.r.t w.r.t to all possible parameters (also known as the gradient ). If our parameters are stored in a vector, \\begin{bmatrix}\\mu&\\sigma \\end{bmatrix} \\begin{bmatrix}\\mu&\\sigma \\end{bmatrix} we could subtract from this vector the vector of gradients, \\begin{bmatrix}\\frac{\\partial f}{\\partial \\mu} & \\frac{\\partial f}{\\partial \\sigma}\\end{bmatrix} \\begin{bmatrix}\\frac{\\partial f}{\\partial \\mu} & \\frac{\\partial f}{\\partial \\sigma}\\end{bmatrix} in order to complete the computation in (effectively) one step. So vectors (and calculus) give us a computational means of navigating a parameter space, in this case by determining the set of parameters for a function f(x) f(x) which best explain the data. Vectors as abstract lists of numbers We can also think of vectors as simply lists of numbers . For example, we could describe a car in terms of its price, top speed, safety rating, emissions performance, etc. and store these numbers in a single vector . car = \\begin{bmatrix}\\text{price,} & \\text{top speed,} & \\text{safety rating, }& ...\\end{bmatrix} car = \\begin{bmatrix}\\text{price,} & \\text{top speed,} & \\text{safety rating, }& ...\\end{bmatrix} Note This is more of a 'computer science' perspective of vectors. To summarize, a vector is, at the simplest level: lists of numbers something which moves in a space of parameters","title":"Getting a handle on vectors"},{"location":"linear_algebra/week_1/#operations-with-vectors","text":"Lets now explore the operations we can do with vectors, how these mathematical operations define what vectors are in the first place, and the sort of spaces they can apply to. We can think of a vector as an object that moves us about space. This could be a physical space, or a space of data (often called a vector space ). Note At school, you probably thought of a vector as something that moved you around a physical space, but in computer and data science, we generalize that idea to think of a vector as just a list of attributes of an objects. More formally, mathematics generalizes the definition of a vector to be an object for which the following two operations are defined : addition multiplication by a scalar Tip This is really important, so make sure you understand it. If a mathematical object can be added to another object of the same type, and it can be scaled (i.e. multiplied by a scaler), then its a vector! Vector addition Intuitively, we can introduce vector addition as being the resulting vector of the two vectors we want to add ( s s and r r ) being placed head-to-tail , s + r s + r . Multiplication by a scalar Multiplying a vector by a scalar is also easy to understand. In this case, we simply multiply all elements of our vector r r by the scalar, a a for example. Coordinate systems At this point, it's convenient to define a coordinate system . Imagine we had two dimensions defined by the vectors: \\hat i = \\begin{bmatrix}1 \\\\\\ 0 \\end{bmatrix} \\; \\hat j = \\begin{bmatrix} 1 \\\\\\ 0 \\end{bmatrix} \\hat i = \\begin{bmatrix}1 \\\\\\ 0 \\end{bmatrix} \\; \\hat j = \\begin{bmatrix} 1 \\\\\\ 0 \\end{bmatrix} Note These are known as basis vectors , and they define the basis . We could define any vector in this 2D space using the vectors \\hat i \\hat i and \\hat j \\hat j . For example, the vector \\begin{bmatrix}3 \\\\\\ 2 \\end{bmatrix} = 3 \\hat i + 2 \\hat j \\begin{bmatrix}3 \\\\\\ 2 \\end{bmatrix} = 3 \\hat i + 2 \\hat j Note This is also a extremely important point. A vector space is itself defined by vectors . We will explore this further later in the course. This also nicely illustrates that vectors are associative , meaning, the sum of a series of vectors is the same regardless of the order we add them in, e.g., \\begin{bmatrix} 3 \\\\\\ 2 \\end{bmatrix} = 3 \\hat i + 2 \\hat{j} = 2 \\hat{j} + 3 \\hat i \\begin{bmatrix} 3 \\\\\\ 2 \\end{bmatrix} = 3 \\hat i + 2 \\hat{j} = 2 \\hat{j} + 3 \\hat i Conclusions We've defined two fundamental operations that vectors satisfy: addition , (e.g. r + s r + s ), and multiplication by a scalar , (e.g. 2r 2r ). We've noted that it can be useful to define a coordinate system in which to do our addition and scaling , e.g., r = \\begin{bmatrix}3 \\\\\\ 2 \\end{bmatrix} = 3 \\hat i + 2 \\hat{j} r = \\begin{bmatrix}3 \\\\\\ 2 \\end{bmatrix} = 3 \\hat i + 2 \\hat{j} using these fundamental basis vectors, \\hat i \\hat i and \\hat{j} \\hat{j} , and explored the properties that this implies, like associativity of addition and subtraction. We've also seen that although, perhaps, it's easiest to think of vector operations geometrically , we don't have to do it in a real (number) space. We can also define vector operations on vectors that list different types of objects, like the attributes of a house .","title":"Operations with vectors"},{"location":"linear_algebra/week_2/","text":"Week 2: Vectors are Objects that Move Around Space In this module, we will look at the types operations we can do with vectors - finding the modulus or magnitude (size), finding the angle between vectors (dot or inner product) and projecting one vector onto another. We will then examine how the entries describing a vector will depend on what vectors we use to define the axes - the basis. That will then let us determine whether a proposed set of basis vectors are linearly independent . This will complete our examination of vectors, allowing us to move on to matrices and then to begin solving linear algebra problems. Learning Objectives Calculate basic operations (dot product, modulus, negation) on vectors Calculate a change of basis Recall linear independence Identify a linearly independent basis and relate this to the dimensionality of the space Finding the size of a vector, its angle, and projection Tip It is probably worth it to watch this 3Blue1Brown video first before reading through this section. However, be warned, it is the most confusing one in the series. If you want even more practice, check out this Khan Academy track. Modulus & inner product Previously we looked at the two main vector operations of addition and scaling by a number (multiplication by a scalar ). As it turns out, those are really the only operations we need to be able to do in order define something as a vector. Now, we can move on to define two new ideas: the length of a vector , also called its size , and the dot product of a vector, also called its inner , scalar or projection product. Note The dot product is a huge and amazing concept in linear algebra with a huge number of implications. We'll only be able to touch on a few parts of it here, but enjoy. It's one of the most beautiful parts of linear algebra. Length of a vector Lets define a vector r r using the basis vectors we introduced earlier, i i and j j , r = a i + b j = \\begin{bmatrix} a \\\\\\ b \\end{bmatrix} r = a i + b j = \\begin{bmatrix} a \\\\\\ b \\end{bmatrix} To calculate the length of r r , also called the norm \\vert r\\vert \\vert r\\vert (or \\Vert r\\Vert \\Vert r\\Vert ), we could imagine drawing a triangle, with our vector r r as the hypotenuse: Note The length, magnitude, modulus and norm of a vector are all the same thing, and just represent a difference in terminology. If we are thinking of a vector as representing the line segment from the origin to a given point (i.e., the geometric interpretation), we may interpret the norm as the length of this line segment. If we are thinking of a vector as representing a physical quantity like acceleration or velocity, we may interpret the norm as the magnitude of this quantity (how \" large \" it is, regardless of its direction). By Pythagorus's Theorem , \\vert r \\vert = \\sqrt{a^2 + b^2} \\vert r \\vert = \\sqrt{a^2 + b^2} Vector dot product The dot product is one of several ways of multiplying two vectors together, specifically, it is an algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors) and returns a single number . The dot product has an algebraic and geometric interpretation. Algebraically , the dot product is the sum of the products of the corresponding entries of the two sequences of numbers . Geometrically , it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them . Algebraic definition of the dot product To illustrate the algebraic definition of the dot product, lets define two vectors r r and s s : r = \\begin{bmatrix}r_i \\\\ r_j\\end{bmatrix}, s = \\begin{bmatrix}s_i \\\\ s_j\\end{bmatrix} r = \\begin{bmatrix}r_i \\\\ r_j\\end{bmatrix}, s = \\begin{bmatrix}s_i \\\\ s_j\\end{bmatrix} The dot product is then: r \\cdot s = r_is_i + r_j s_j = (3)(-1) + (2)(2) = 1 r \\cdot s = r_is_i + r_j s_j = (3)(-1) + (2)(2) = 1 More formally, the algebraic definition of the dot product is: r \\cdot s = \\sum^n_{i=1}a_ib_i = a_1b_1 + a_2b_2 + ... + a_nb_n r \\cdot s = \\sum^n_{i=1}a_ib_i = a_1b_1 + a_2b_2 + ... + a_nb_n Note The algebraic definition of the dot product is simply the sum of the products obtained by multiplying each component from both vectors. Properties of the dot product The dot product is, commutative , e.g., r \\cdot s = s \\cdot r r \\cdot s = s \\cdot r distributive , e.g., r \\cdot (s + {t}) = r \\cdot s + r \\cdot {t} r \\cdot (s + {t}) = r \\cdot s + r \\cdot {t} associative over scalar multiplication, e.g., r \\cdot (a s) = a ( r \\cdot s) r \\cdot (a s) = a ( r \\cdot s) Lets prove the distributive property in the general case. Let: r = \\begin{bmatrix} r_1 \\\\\\ r_2 \\\\\\ . \\\\\\ . \\\\\\ . \\\\\\ r_n \\end{bmatrix}, \\; s = \\begin{bmatrix} s_1 \\\\\\ s_2 \\\\\\ . \\\\\\ . \\\\\\ . \\\\\\ s_n \\end{bmatrix}, \\; {t} = \\begin{bmatrix} t_1 \\\\\\ t_2 \\\\\\ . \\\\\\ . \\\\\\ . \\\\\\ t_n \\end{bmatrix} r = \\begin{bmatrix} r_1 \\\\\\ r_2 \\\\\\ . \\\\\\ . \\\\\\ . \\\\\\ r_n \\end{bmatrix}, \\; s = \\begin{bmatrix} s_1 \\\\\\ s_2 \\\\\\ . \\\\\\ . \\\\\\ . \\\\\\ s_n \\end{bmatrix}, \\; {t} = \\begin{bmatrix} t_1 \\\\\\ t_2 \\\\\\ . \\\\\\ . \\\\\\ . \\\\\\ t_n \\end{bmatrix} then, r \\cdot (s + {t}) = r_1 (s_1 + t_1) + r_2 (s_2 + t_2) + ... + r_n (s_n + t_n) r \\cdot (s + {t}) = r_1 (s_1 + t_1) + r_2 (s_2 + t_2) + ... + r_n (s_n + t_n) = r_1 s_1 + r_1t_1 + r_2s_2 + r_2t_2 + ... + r_ns_n + r_nt_n = r_1 s_1 + r_1t_1 + r_2s_2 + r_2t_2 + ... + r_ns_n + r_nt_n = r \\cdot s + r \\cdot {t} = r \\cdot s + r \\cdot {t} Note Proofs for the remaining properties are left as an exercise. Link between the dot product and the size of the vector If we take r r and dot it with itself, we get: r_i r_i + r_j r_j + ... + r_nr_n = r_i^2 + r_j^2 + ... + r_n^2 = \\vert r \\vert^2 r_i r_i + r_j r_j + ... + r_nr_n = r_i^2 + r_j^2 + ... + r_n^2 = \\vert r \\vert^2 So, the size of the vector is just given by r r dotted with itself and squared. Cosine & dot product Lets take the time to derive the geometric definition of the dot product. Note Recall, geometrically , the dot product is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them . We start with the law of cosines (also known as the cosine formula or cosine rule ) from algebra, which you'll remember, probably vaguely, from school. The law of cosines states that if we had a triangle with sides a a , b b , and c c , then: c^2 = a^2 + b^2 - 2ab \\cos \\theta c^2 = a^2 + b^2 - 2ab \\cos \\theta Now, we can translate this into our vector notation: \\vert r - s\\vert ^2 = \\vert r\\vert ^2 + \\vert s\\vert ^2 - 2\\vert r\\vert \\vert s\\vert \\cos \\theta \\vert r - s\\vert ^2 = \\vert r\\vert ^2 + \\vert s\\vert ^2 - 2\\vert r\\vert \\vert s\\vert \\cos \\theta LHS \\Rightarrow (r-s) \\cdot (r-s) = r \\cdot r - s \\cdot r - s \\cdot r - s \\cdot s \\Rightarrow (r-s) \\cdot (r-s) = r \\cdot r - s \\cdot r - s \\cdot r - s \\cdot s = \\vert r\\vert ^2 - 2 s \\cdot r + \\vert s\\vert ^2 = \\vert r\\vert ^2 - 2 s \\cdot r + \\vert s\\vert ^2 Note \\vert r - s\\vert ^2 = (r-s) \\cdot (r-s) \\vert r - s\\vert ^2 = (r-s) \\cdot (r-s) comes straight from the definition of the dot product. LHS = RHS \\Rightarrow \\vert r\\vert ^2 - 2 s \\cdot r + \\vert s\\vert ^2 = \\vert r\\vert ^2 + \\vert s\\vert ^2 - 2\\vert r\\vert \\vert s\\vert \\cos \\theta \\Rightarrow \\vert r\\vert ^2 - 2 s \\cdot r + \\vert s\\vert ^2 = \\vert r\\vert ^2 + \\vert s\\vert ^2 - 2\\vert r\\vert \\vert s\\vert \\cos \\theta \\Rightarrow r \\cdot s = \\vert r\\vert \\vert s\\vert \\cos \\theta \\Rightarrow r \\cdot s = \\vert r\\vert \\vert s\\vert \\cos \\theta So what we notice is that the dot product does something quite profound . It takes the size of the two vectors ( \\vert r\\vert , \\vert s \\vert \\vert r\\vert , \\vert s \\vert ) and multiplies them by \\cos \\cos of the angle between them. It tells us something about the extent to which the two vectors go in the same direction. If \\theta \\theta is zero, then \\cos \\theta \\cos \\theta is one and r \\cdot s r \\cdot s would just be the size of the two vectors multiplied together. If \\theta \\theta is 90 90 degrees ( i.e. r r and s s are orthogonal), then \\cos 90 \\cos 90 , is 0 0 and r \\cdot s r \\cdot s is 0 0 . More generally, Note Ignore the word \"score\" here, this image was taken from a blog post about machine learning. The blog post is worth checking out though. Full credit to Christian S. Perone for the image. In this way, the dot product captures whether the two vectors are pointing in similar directions (positive) or opposite directions (negative). Projection The vector projection of a vector s s on (or onto) a nonzero vector r r (also known as the vector component or vector resolution of s s in the direction of r r ) is the orthogonal projection of s s onto a straight line parallel to r r . Tip Understanding projection can be a little tricky. If you want even more practice, check out this Khan Academy series. For the following triangle, Recall the geometric definition of the dot product: r \\cdot s = \\vert r\\vert \\vert s\\vert \\cos \\theta r \\cdot s = \\vert r\\vert \\vert s\\vert \\cos \\theta Notice that \\vert s\\vert \\cos \\theta \\vert s\\vert \\cos \\theta is the length of the adjacent side (adjacent to the angle shown). This term is the projection of the vector s s into (or onto) the vector r r . This is why the dot product is also called the projection product , because it takes the projection of one vector ( s s ) onto another ( r r ) times the magnitude or length of the other ( \\vert r \\vert \\vert r \\vert ). Note Note again that if s s was orthogonal to r r then \\vert s\\vert \\cos \\theta = \\vert s\\vert \\cos 90 = 0 = r \\cdot s \\vert s\\vert \\cos \\theta = \\vert s\\vert \\cos 90 = 0 = r \\cdot s . This provides a convenient way to check for orthogonality. Rearranging, we can compute the scalar projection of s s on r r : r \\cdot s = \\vert r\\vert\\vert s\\vert \\cos \\theta r \\cdot s = \\vert r\\vert\\vert s\\vert \\cos \\theta \\Rightarrow \\frac{ r \\cdot s}{\\vert r\\vert } = \\vert s\\vert \\cos \\theta \\Rightarrow \\frac{ r \\cdot s}{\\vert r\\vert } = \\vert s\\vert \\cos \\theta The scalar projection is a scalar , equal to the length of the orthogonal projection of s s on r r , with a negative sign if the projection has an opposite direction with respect to r r . We can also define the vector projection r \\cdot s = \\vert r\\vert\\vert s\\vert \\cos \\theta r \\cdot s = \\vert r\\vert\\vert s\\vert \\cos \\theta \\Rightarrow \\frac{ r \\cdot s}{\\vert r\\vert } = \\vert s\\vert \\cos \\theta \\Rightarrow \\frac{ r \\cdot s}{\\vert r\\vert } = \\vert s\\vert \\cos \\theta \\Rightarrow \\frac{r}{\\vert r \\vert} \\cdot \\frac{r \\cdot s}{\\vert r\\vert } = \\vert s\\vert \\cos \\theta \\cdot \\frac{r}{\\vert r \\vert} \\Rightarrow \\frac{r}{\\vert r \\vert} \\cdot \\frac{r \\cdot s}{\\vert r\\vert } = \\vert s\\vert \\cos \\theta \\cdot \\frac{r}{\\vert r \\vert} which is the orthogonal projection of s s onto a straight line parallel to r r . Notice that this formula is intuitive, we take the scaler projection of s s onto r r (the length of the orthogonal projection of s s on r r ) and multiply it by a unit vector in the direction of r r , \\frac{r}{\\vert r \\vert} \\frac{r}{\\vert r \\vert} . Conclusions This was really the core video for this week. We found the size of a vector and we defined the dot product . We've then found out some mathematical operations we can do with the dot product (multiplication by a scalar and the dot product). We also proved that mathematical operations with vectors obey the following properties: commutative distributive over vector addition associative with scalar multiplication We then found that the dot product actually captures the angle between two vectors, the extent to which they go in the same direction, and also finds the projection of one vector onto another. Changing the reference frame Tip It is best to watch this video first, then return to and read through this section. If you want even more exposure to these ideas, try the first three sections of the Khan Academy course here . Changing basis So far we haven't really talked about the coordinate system of our vector space , the coordinates in which all of our vectors exist. In this section we'll look at what we mean by coordinate systems, and walk through a few examples of changing from one coordinate system to another. Remember that a vector (e.g. r r ) is just an object that takes us from the origin to some point in space . This could be some physical space or it could be some data space, like the attributes of a house (bedrooms, price, etc.). We could use a coordinate system defined itself by vectors, such as the vectors \\hat{i} \\hat{i} and \\hat{j} \\hat{j} that we defined before. Lets give them names \\hat{e_1} \\hat{e_1} and \\hat{e_2} \\hat{e_2} instead. We will define them to be of unit length, meaning they're of length 1. Note The little hat ( \\hat i \\hat i ) denotes unit length. So, \\hat{e_1} = \\begin{bmatrix} 1 \\\\\\ 0 \\end{bmatrix}, \\; \\hat{e_2} = \\begin{bmatrix} 0 \\\\\\ 1 \\end{bmatrix} \\hat{e_1} = \\begin{bmatrix} 1 \\\\\\ 0 \\end{bmatrix}, \\; \\hat{e_2} = \\begin{bmatrix} 0 \\\\\\ 1 \\end{bmatrix} Note if we had more dimensions in our space, we could just use more one-hot encoded vectors ( \\hat{e_n} \\hat{e_n} ) of dimension equal to the dimensions in our space. We can then define any other vector in our space in terms of \\hat{e_1} \\hat{e_1} and \\hat{e_2} \\hat{e_2} . For example, r_e = 3\\hat{e_1} + 3 \\hat{e_2} = \\begin{bmatrix} 3 \\\\\\ 4 \\end{bmatrix} r_e = 3\\hat{e_1} + 3 \\hat{e_2} = \\begin{bmatrix} 3 \\\\\\ 4 \\end{bmatrix} Here, the instruction is that r_e r_e is going to be equal to doing a vector sum of 3 \\hat{e_1} 3 \\hat{e_1} and 4 \\hat{e_2} 4 \\hat{e_2} . If you think about it, our choice of \\hat{e_1} \\hat{e_1} and \\hat{e_2} \\hat{e_2} is kind of arbitrary. There's no reason we couldn't have used different vectors to define our coordinate system Note These vectors don't even need to be at 90 degrees to each other or of the same length In any case, I could still have described r r as being some sum of some vectors I used to define the space. We call the vectors we use to define our vector space (e.g. \\hat{e_1} \\hat{e_1} and \\hat{e_2} \\hat{e_2} ) basis vectors . What we realize here, is that our vector r r exists independently of the coordinate system we use. The vector still takes us from the origin to some point in space, even when we change the coordinate system, more specifically, even when we change the basis vectors used to describe our vector space . It turns out, we can actually change the basis of the vector r r (call this r_e r_e ) to a new set of basis vectors, i.e. \\hat{b_1} \\hat{b_1} and \\hat{b_2} \\hat{b_2} , which we will denote r_b r_b . Furthermore, we can do this using the dot product so long as The new basis vectors are orthogonal to each other, i.e. \\hat{b_1} \\cdot \\hat{b_2} = 0 \\hat{b_1} \\cdot \\hat{b_2} = 0 We know the position of \\hat{b_1} \\hat{b_1} and \\hat{b_2} \\hat{b_2} in the space defined by \\hat{e_1} \\hat{e_1} and \\hat{e_2} \\hat{e_2} . Note We can still change basis even when the new basis vectors are not orthogonal to one another, but for this we will need matrices. See later parts of the course. Lets define \\hat{b_1} \\hat{b_1} and \\hat{b_2} \\hat{b_2} in the space defined by \\hat{e_1} \\hat{e_1} and \\hat{e_2} \\hat{e_2} : \\hat{b_1} = \\begin{bmatrix} -2 \\\\\\ 4 \\end{bmatrix}, \\; \\hat{b_2} = \\begin{bmatrix} 2 \\\\\\ 1 \\end{bmatrix} \\hat{b_1} = \\begin{bmatrix} -2 \\\\\\ 4 \\end{bmatrix}, \\; \\hat{b_2} = \\begin{bmatrix} 2 \\\\\\ 1 \\end{bmatrix} In order to determine r_b r_b , i.e. the vector r r defined in terms of the basis vectors \\hat{b_1} \\hat{b_1} and \\hat{b_2} \\hat{b_2} , we need to take sum the vector projection of r_e r_e onto \\hat{b_1} \\hat{b_1} and the vector projection of r_e r_e onto \\hat{b_2} \\hat{b_2} So, lets do it: Vector projection of r_e r_e onto \\hat{b_1} \\hat{b_1} \\hat{b_1}\\frac{r_e \\cdot \\hat{b_1}}{\\vert \\hat{b_1}\\vert ^2} = \\frac{3 \\times 2 + 4 \\times 1}{2^2 + 1^2} = \\frac{10}{5} = 2 \\hat{b_1} = 2 \\begin{bmatrix} 2 \\\\\\ 1\\end{bmatrix} = \\begin{bmatrix} 4 \\\\\\ 2\\end{bmatrix} \\hat{b_1}\\frac{r_e \\cdot \\hat{b_1}}{\\vert \\hat{b_1}\\vert ^2} = \\frac{3 \\times 2 + 4 \\times 1}{2^2 + 1^2} = \\frac{10}{5} = 2 \\hat{b_1} = 2 \\begin{bmatrix} 2 \\\\\\ 1\\end{bmatrix} = \\begin{bmatrix} 4 \\\\\\ 2\\end{bmatrix} Vector projection of r_e r_e onto \\hat{b_2} \\hat{b_2} \\hat{b_2}\\frac{r_e \\cdot \\hat{b_2}}{\\vert \\hat{b_2}\\vert ^2} = \\frac{3 \\times -2 + 4 \\times 4}{-2^2 + 4^2} = \\frac{10}{20} = \\frac{1}{2} \\hat{b_2} = \\frac{1}{2} \\begin{bmatrix} -2 \\\\\\ 4\\end{bmatrix} = \\begin{bmatrix} -1 \\\\\\ 2\\end{bmatrix} \\hat{b_2}\\frac{r_e \\cdot \\hat{b_2}}{\\vert \\hat{b_2}\\vert ^2} = \\frac{3 \\times -2 + 4 \\times 4}{-2^2 + 4^2} = \\frac{10}{20} = \\frac{1}{2} \\hat{b_2} = \\frac{1}{2} \\begin{bmatrix} -2 \\\\\\ 4\\end{bmatrix} = \\begin{bmatrix} -1 \\\\\\ 2\\end{bmatrix} Thus, r_b = \\hat{b_1} \\frac{r_e \\cdot \\hat{b_1}}{\\vert \\hat{b_1}\\vert ^2} + \\hat{b_2} \\frac{r_e \\cdot \\hat{b_2}}{\\vert \\hat{b_2}\\vert ^2} = 2 \\hat{b_1} + \\frac{1}{2} \\hat{b_2} = \\begin{bmatrix}2 \\\\\\ \\frac{1}{2}\\end{bmatrix} r_b = \\hat{b_1} \\frac{r_e \\cdot \\hat{b_1}}{\\vert \\hat{b_1}\\vert ^2} + \\hat{b_2} \\frac{r_e \\cdot \\hat{b_2}}{\\vert \\hat{b_2}\\vert ^2} = 2 \\hat{b_1} + \\frac{1}{2} \\hat{b_2} = \\begin{bmatrix}2 \\\\\\ \\frac{1}{2}\\end{bmatrix} Finally, notice that r_b = 2 \\hat{b_1} + \\frac{1}{2} \\hat{b_2} = 2 \\begin{bmatrix} 2 \\\\\\ 1\\end{bmatrix} + \\frac{1}{2} \\begin{bmatrix} -2 \\\\\\ 4\\end{bmatrix} = \\begin{bmatrix} 3 \\\\\\ 4\\end{bmatrix} = r_e r_b = 2 \\hat{b_1} + \\frac{1}{2} \\hat{b_2} = 2 \\begin{bmatrix} 2 \\\\\\ 1\\end{bmatrix} + \\frac{1}{2} \\begin{bmatrix} -2 \\\\\\ 4\\end{bmatrix} = \\begin{bmatrix} 3 \\\\\\ 4\\end{bmatrix} = r_e Conclusions We've seen that our vector describing our data isn't tied to the axis that we originally used to describe it . We can redescribe it using some other axis , some other basis vectors . It turns out that choosing basis vectors we use to describe the space of data carefully to help us solve our problem will be a very important thing in linear algebra, and in general. We can move the numbers in the vector we used to describe a data item from one basis to another. We can do that change just by taking the dot or projection product so long as the new basis factors are orthogonal to each other. Basis, vector space, and linear independence Tip Linear independence is really only brushed on here. To go deeper, check out this Khan Academy series. Previously we've seen that our basis vectors do not have to be the so called standard (or natural) basis . We can actually choose any basis vectors we want, which redefine how we we move about space. Standard Basis The set of unit vectors pointing in the direction of the axes of a Cartesian coordinate system. \\hat{e_x} = \\begin{bmatrix} 1 \\\\\\ 0 \\\\\\ 0 \\end{bmatrix}, \\; \\hat{e_y} = \\begin{bmatrix} 0 \\\\\\ 1 \\\\\\ 0 \\end{bmatrix}, \\; \\hat{e_z} = \\begin{bmatrix} 0 \\\\\\ 0 \\\\\\ 1 \\end{bmatrix} \\hat{e_x} = \\begin{bmatrix} 1 \\\\\\ 0 \\\\\\ 0 \\end{bmatrix}, \\; \\hat{e_y} = \\begin{bmatrix} 0 \\\\\\ 1 \\\\\\ 0 \\end{bmatrix}, \\; \\hat{e_z} = \\begin{bmatrix} 0 \\\\\\ 0 \\\\\\ 1 \\end{bmatrix} Note Also known as the orthonormal basis . Lets formally define what we mean by a basis (vector space), and define linear independence , which is going to let us understand how many dimensions our vector space actually has. Basis The basis is a set of n n vectors that: are not linear combinations of each other (linear independent) span the space that they describe If these two qualities are fulfilled, then the space defined by the basis is n n -dimensional. Linear independence A set of vectors is said to be linearly dependent if one of the vectors in the set can be defined as a linear combination of the others; if no vector in the set can be written in this way, then the vectors are said to be linearly independent . For example, imagine we had some candidate vector {b_3} {b_3} . If we could write {b_3} {b_3} as a linear combination of, say, {b_1} {b_1} and {b_2} {b_2} : {b_3} = c_1 {b_1} + c_2 {b_2} {b_3} = c_1 {b_1} + c_2 {b_2} where c_1 c_1 and c_2 c_2 were constants, then we would say that {b_3} {b_3} is linearly dependent on {b_1} {b_1} and {b_2} {b_2} . To drive the point home, we note that the following are true if {b_3} {b_3} is linearly dependent to the vectors {b_1} {b_1} and {b_2} {b_2} : {b_3} {b_3} does not lie in the plane spanned by {b_1} {b_1} and {b_2} {b_2} {b_3} \\ne c_1 {b_1} + c_2 {b_2} {b_3} \\ne c_1 {b_1} + c_2 {b_2} for any c_1 c_1 , c_2 \\in \\mathbb R c_2 \\in \\mathbb R OR, equivalently, 0 = c_1 {b_1} + c_2 {b_2} + c_3 {b_3} 0 = c_1 {b_1} + c_2 {b_2} + c_3 {b_3} implies that c_1 = c_2 = c_3 = 0 c_1 = c_2 = c_3 = 0 These concepts are central to the definition of dimension . As stated previously, if we have a set of n n basis vectors, then these vectors describe an n n -dimensional space, as we can express any n n -dimensional vector as a linear combination of our n n basis vectors. Now, notice what our basis vectors {b_n} {b_n} don't have to be. they don't have to be unit vectors, by which we mean vectors of length 1 and they don't have to be orthogonal (or normal ) to each other But, as it turns out, everything is going to be much easier if they are. So if at all possible, you want to construct what's called an orthonormal basic vector set , where all vectors of the set are at 90 90 degrees to each other and are all of unit length. Now, let's think about what happens when we map from one basis to another. The axes of the original grid are projected onto the new grid ; and potentially have different values on that new grid, but crucially , the projection keeps the grid being evenly spaced. Therefore, any mapping that we do from one set of basis vectors, (one coordinate system) to another set of basis vectors (another coordinate system), keeps the vector space being a regularly spaced grid , where our original rules of vector addition and multiplication by a scaler still work. Note Basically, it doesn't warp or fold space, which is what the linear bit in linear algebra means geometrically. Things might be stretched or rotated or inverted, but everything remains evenly spaced and linear combinations still work. Now, when the new basis vectors aren't orthogonal, then we won't be able to use the dot product (really, the projection) to map from one basis to another. We'll have to use matrices instead, which we'll meet in the next module. Tip Honestly, this part is tricky. It might be worth it to watch the first three videos of the Essence of Linear Algebra series. For the lazy, jumpy straight to this video . Conclusions In this section, we've talked about the dimensionality of a vector space in terms of the number of independent basis factors that it has. We found a test for independence: vectors are independent if one of them is not a linear combination of the others. Finally, we discussed what that means to map vectors from one space to another and how that is going to be useful in data science and machine learning. Summary of week 2 We've looked at vectors as being objects that describe where we are in space which could be a physical space, a space of data, or a parameter space of the parameters of a function. It doesn't really matter. It's just some space. Then we've defined vector addition and scaling a vector by a number, making it bigger or reversing its direction. Then we've gone on to find the magnitude or modulus of a vector, and the dot scalar and vector projection product. We've defined the basis of a vector space, its dimension, and the ideas of linear independence and linear combinations. We've used projections to look at one case of changes from one basis to another, for the case where the new basis is orthogonal.","title":"Week 2"},{"location":"linear_algebra/week_2/#week-2-vectors-are-objects-that-move-around-space","text":"In this module, we will look at the types operations we can do with vectors - finding the modulus or magnitude (size), finding the angle between vectors (dot or inner product) and projecting one vector onto another. We will then examine how the entries describing a vector will depend on what vectors we use to define the axes - the basis. That will then let us determine whether a proposed set of basis vectors are linearly independent . This will complete our examination of vectors, allowing us to move on to matrices and then to begin solving linear algebra problems. Learning Objectives Calculate basic operations (dot product, modulus, negation) on vectors Calculate a change of basis Recall linear independence Identify a linearly independent basis and relate this to the dimensionality of the space","title":"Week 2: Vectors are Objects that Move Around Space"},{"location":"linear_algebra/week_2/#finding-the-size-of-a-vector-its-angle-and-projection","text":"Tip It is probably worth it to watch this 3Blue1Brown video first before reading through this section. However, be warned, it is the most confusing one in the series. If you want even more practice, check out this Khan Academy track.","title":"Finding the size of a vector, its angle, and projection"},{"location":"linear_algebra/week_2/#modulus-inner-product","text":"Previously we looked at the two main vector operations of addition and scaling by a number (multiplication by a scalar ). As it turns out, those are really the only operations we need to be able to do in order define something as a vector. Now, we can move on to define two new ideas: the length of a vector , also called its size , and the dot product of a vector, also called its inner , scalar or projection product. Note The dot product is a huge and amazing concept in linear algebra with a huge number of implications. We'll only be able to touch on a few parts of it here, but enjoy. It's one of the most beautiful parts of linear algebra. Length of a vector Lets define a vector r r using the basis vectors we introduced earlier, i i and j j , r = a i + b j = \\begin{bmatrix} a \\\\\\ b \\end{bmatrix} r = a i + b j = \\begin{bmatrix} a \\\\\\ b \\end{bmatrix} To calculate the length of r r , also called the norm \\vert r\\vert \\vert r\\vert (or \\Vert r\\Vert \\Vert r\\Vert ), we could imagine drawing a triangle, with our vector r r as the hypotenuse: Note The length, magnitude, modulus and norm of a vector are all the same thing, and just represent a difference in terminology. If we are thinking of a vector as representing the line segment from the origin to a given point (i.e., the geometric interpretation), we may interpret the norm as the length of this line segment. If we are thinking of a vector as representing a physical quantity like acceleration or velocity, we may interpret the norm as the magnitude of this quantity (how \" large \" it is, regardless of its direction). By Pythagorus's Theorem , \\vert r \\vert = \\sqrt{a^2 + b^2} \\vert r \\vert = \\sqrt{a^2 + b^2} Vector dot product The dot product is one of several ways of multiplying two vectors together, specifically, it is an algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors) and returns a single number . The dot product has an algebraic and geometric interpretation. Algebraically , the dot product is the sum of the products of the corresponding entries of the two sequences of numbers . Geometrically , it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them . Algebraic definition of the dot product To illustrate the algebraic definition of the dot product, lets define two vectors r r and s s : r = \\begin{bmatrix}r_i \\\\ r_j\\end{bmatrix}, s = \\begin{bmatrix}s_i \\\\ s_j\\end{bmatrix} r = \\begin{bmatrix}r_i \\\\ r_j\\end{bmatrix}, s = \\begin{bmatrix}s_i \\\\ s_j\\end{bmatrix} The dot product is then: r \\cdot s = r_is_i + r_j s_j = (3)(-1) + (2)(2) = 1 r \\cdot s = r_is_i + r_j s_j = (3)(-1) + (2)(2) = 1 More formally, the algebraic definition of the dot product is: r \\cdot s = \\sum^n_{i=1}a_ib_i = a_1b_1 + a_2b_2 + ... + a_nb_n r \\cdot s = \\sum^n_{i=1}a_ib_i = a_1b_1 + a_2b_2 + ... + a_nb_n Note The algebraic definition of the dot product is simply the sum of the products obtained by multiplying each component from both vectors. Properties of the dot product The dot product is, commutative , e.g., r \\cdot s = s \\cdot r r \\cdot s = s \\cdot r distributive , e.g., r \\cdot (s + {t}) = r \\cdot s + r \\cdot {t} r \\cdot (s + {t}) = r \\cdot s + r \\cdot {t} associative over scalar multiplication, e.g., r \\cdot (a s) = a ( r \\cdot s) r \\cdot (a s) = a ( r \\cdot s) Lets prove the distributive property in the general case. Let: r = \\begin{bmatrix} r_1 \\\\\\ r_2 \\\\\\ . \\\\\\ . \\\\\\ . \\\\\\ r_n \\end{bmatrix}, \\; s = \\begin{bmatrix} s_1 \\\\\\ s_2 \\\\\\ . \\\\\\ . \\\\\\ . \\\\\\ s_n \\end{bmatrix}, \\; {t} = \\begin{bmatrix} t_1 \\\\\\ t_2 \\\\\\ . \\\\\\ . \\\\\\ . \\\\\\ t_n \\end{bmatrix} r = \\begin{bmatrix} r_1 \\\\\\ r_2 \\\\\\ . \\\\\\ . \\\\\\ . \\\\\\ r_n \\end{bmatrix}, \\; s = \\begin{bmatrix} s_1 \\\\\\ s_2 \\\\\\ . \\\\\\ . \\\\\\ . \\\\\\ s_n \\end{bmatrix}, \\; {t} = \\begin{bmatrix} t_1 \\\\\\ t_2 \\\\\\ . \\\\\\ . \\\\\\ . \\\\\\ t_n \\end{bmatrix} then, r \\cdot (s + {t}) = r_1 (s_1 + t_1) + r_2 (s_2 + t_2) + ... + r_n (s_n + t_n) r \\cdot (s + {t}) = r_1 (s_1 + t_1) + r_2 (s_2 + t_2) + ... + r_n (s_n + t_n) = r_1 s_1 + r_1t_1 + r_2s_2 + r_2t_2 + ... + r_ns_n + r_nt_n = r_1 s_1 + r_1t_1 + r_2s_2 + r_2t_2 + ... + r_ns_n + r_nt_n = r \\cdot s + r \\cdot {t} = r \\cdot s + r \\cdot {t} Note Proofs for the remaining properties are left as an exercise. Link between the dot product and the size of the vector If we take r r and dot it with itself, we get: r_i r_i + r_j r_j + ... + r_nr_n = r_i^2 + r_j^2 + ... + r_n^2 = \\vert r \\vert^2 r_i r_i + r_j r_j + ... + r_nr_n = r_i^2 + r_j^2 + ... + r_n^2 = \\vert r \\vert^2 So, the size of the vector is just given by r r dotted with itself and squared.","title":"Modulus &amp; inner product"},{"location":"linear_algebra/week_2/#cosine-dot-product","text":"Lets take the time to derive the geometric definition of the dot product. Note Recall, geometrically , the dot product is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them . We start with the law of cosines (also known as the cosine formula or cosine rule ) from algebra, which you'll remember, probably vaguely, from school. The law of cosines states that if we had a triangle with sides a a , b b , and c c , then: c^2 = a^2 + b^2 - 2ab \\cos \\theta c^2 = a^2 + b^2 - 2ab \\cos \\theta Now, we can translate this into our vector notation: \\vert r - s\\vert ^2 = \\vert r\\vert ^2 + \\vert s\\vert ^2 - 2\\vert r\\vert \\vert s\\vert \\cos \\theta \\vert r - s\\vert ^2 = \\vert r\\vert ^2 + \\vert s\\vert ^2 - 2\\vert r\\vert \\vert s\\vert \\cos \\theta LHS \\Rightarrow (r-s) \\cdot (r-s) = r \\cdot r - s \\cdot r - s \\cdot r - s \\cdot s \\Rightarrow (r-s) \\cdot (r-s) = r \\cdot r - s \\cdot r - s \\cdot r - s \\cdot s = \\vert r\\vert ^2 - 2 s \\cdot r + \\vert s\\vert ^2 = \\vert r\\vert ^2 - 2 s \\cdot r + \\vert s\\vert ^2 Note \\vert r - s\\vert ^2 = (r-s) \\cdot (r-s) \\vert r - s\\vert ^2 = (r-s) \\cdot (r-s) comes straight from the definition of the dot product. LHS = RHS \\Rightarrow \\vert r\\vert ^2 - 2 s \\cdot r + \\vert s\\vert ^2 = \\vert r\\vert ^2 + \\vert s\\vert ^2 - 2\\vert r\\vert \\vert s\\vert \\cos \\theta \\Rightarrow \\vert r\\vert ^2 - 2 s \\cdot r + \\vert s\\vert ^2 = \\vert r\\vert ^2 + \\vert s\\vert ^2 - 2\\vert r\\vert \\vert s\\vert \\cos \\theta \\Rightarrow r \\cdot s = \\vert r\\vert \\vert s\\vert \\cos \\theta \\Rightarrow r \\cdot s = \\vert r\\vert \\vert s\\vert \\cos \\theta So what we notice is that the dot product does something quite profound . It takes the size of the two vectors ( \\vert r\\vert , \\vert s \\vert \\vert r\\vert , \\vert s \\vert ) and multiplies them by \\cos \\cos of the angle between them. It tells us something about the extent to which the two vectors go in the same direction. If \\theta \\theta is zero, then \\cos \\theta \\cos \\theta is one and r \\cdot s r \\cdot s would just be the size of the two vectors multiplied together. If \\theta \\theta is 90 90 degrees ( i.e. r r and s s are orthogonal), then \\cos 90 \\cos 90 , is 0 0 and r \\cdot s r \\cdot s is 0 0 . More generally, Note Ignore the word \"score\" here, this image was taken from a blog post about machine learning. The blog post is worth checking out though. Full credit to Christian S. Perone for the image. In this way, the dot product captures whether the two vectors are pointing in similar directions (positive) or opposite directions (negative).","title":"Cosine &amp; dot product"},{"location":"linear_algebra/week_2/#projection","text":"The vector projection of a vector s s on (or onto) a nonzero vector r r (also known as the vector component or vector resolution of s s in the direction of r r ) is the orthogonal projection of s s onto a straight line parallel to r r . Tip Understanding projection can be a little tricky. If you want even more practice, check out this Khan Academy series. For the following triangle, Recall the geometric definition of the dot product: r \\cdot s = \\vert r\\vert \\vert s\\vert \\cos \\theta r \\cdot s = \\vert r\\vert \\vert s\\vert \\cos \\theta Notice that \\vert s\\vert \\cos \\theta \\vert s\\vert \\cos \\theta is the length of the adjacent side (adjacent to the angle shown). This term is the projection of the vector s s into (or onto) the vector r r . This is why the dot product is also called the projection product , because it takes the projection of one vector ( s s ) onto another ( r r ) times the magnitude or length of the other ( \\vert r \\vert \\vert r \\vert ). Note Note again that if s s was orthogonal to r r then \\vert s\\vert \\cos \\theta = \\vert s\\vert \\cos 90 = 0 = r \\cdot s \\vert s\\vert \\cos \\theta = \\vert s\\vert \\cos 90 = 0 = r \\cdot s . This provides a convenient way to check for orthogonality. Rearranging, we can compute the scalar projection of s s on r r : r \\cdot s = \\vert r\\vert\\vert s\\vert \\cos \\theta r \\cdot s = \\vert r\\vert\\vert s\\vert \\cos \\theta \\Rightarrow \\frac{ r \\cdot s}{\\vert r\\vert } = \\vert s\\vert \\cos \\theta \\Rightarrow \\frac{ r \\cdot s}{\\vert r\\vert } = \\vert s\\vert \\cos \\theta The scalar projection is a scalar , equal to the length of the orthogonal projection of s s on r r , with a negative sign if the projection has an opposite direction with respect to r r . We can also define the vector projection r \\cdot s = \\vert r\\vert\\vert s\\vert \\cos \\theta r \\cdot s = \\vert r\\vert\\vert s\\vert \\cos \\theta \\Rightarrow \\frac{ r \\cdot s}{\\vert r\\vert } = \\vert s\\vert \\cos \\theta \\Rightarrow \\frac{ r \\cdot s}{\\vert r\\vert } = \\vert s\\vert \\cos \\theta \\Rightarrow \\frac{r}{\\vert r \\vert} \\cdot \\frac{r \\cdot s}{\\vert r\\vert } = \\vert s\\vert \\cos \\theta \\cdot \\frac{r}{\\vert r \\vert} \\Rightarrow \\frac{r}{\\vert r \\vert} \\cdot \\frac{r \\cdot s}{\\vert r\\vert } = \\vert s\\vert \\cos \\theta \\cdot \\frac{r}{\\vert r \\vert} which is the orthogonal projection of s s onto a straight line parallel to r r . Notice that this formula is intuitive, we take the scaler projection of s s onto r r (the length of the orthogonal projection of s s on r r ) and multiply it by a unit vector in the direction of r r , \\frac{r}{\\vert r \\vert} \\frac{r}{\\vert r \\vert} . Conclusions This was really the core video for this week. We found the size of a vector and we defined the dot product . We've then found out some mathematical operations we can do with the dot product (multiplication by a scalar and the dot product). We also proved that mathematical operations with vectors obey the following properties: commutative distributive over vector addition associative with scalar multiplication We then found that the dot product actually captures the angle between two vectors, the extent to which they go in the same direction, and also finds the projection of one vector onto another.","title":"Projection"},{"location":"linear_algebra/week_2/#changing-the-reference-frame","text":"Tip It is best to watch this video first, then return to and read through this section. If you want even more exposure to these ideas, try the first three sections of the Khan Academy course here .","title":"Changing the reference frame"},{"location":"linear_algebra/week_2/#changing-basis","text":"So far we haven't really talked about the coordinate system of our vector space , the coordinates in which all of our vectors exist. In this section we'll look at what we mean by coordinate systems, and walk through a few examples of changing from one coordinate system to another. Remember that a vector (e.g. r r ) is just an object that takes us from the origin to some point in space . This could be some physical space or it could be some data space, like the attributes of a house (bedrooms, price, etc.). We could use a coordinate system defined itself by vectors, such as the vectors \\hat{i} \\hat{i} and \\hat{j} \\hat{j} that we defined before. Lets give them names \\hat{e_1} \\hat{e_1} and \\hat{e_2} \\hat{e_2} instead. We will define them to be of unit length, meaning they're of length 1. Note The little hat ( \\hat i \\hat i ) denotes unit length. So, \\hat{e_1} = \\begin{bmatrix} 1 \\\\\\ 0 \\end{bmatrix}, \\; \\hat{e_2} = \\begin{bmatrix} 0 \\\\\\ 1 \\end{bmatrix} \\hat{e_1} = \\begin{bmatrix} 1 \\\\\\ 0 \\end{bmatrix}, \\; \\hat{e_2} = \\begin{bmatrix} 0 \\\\\\ 1 \\end{bmatrix} Note if we had more dimensions in our space, we could just use more one-hot encoded vectors ( \\hat{e_n} \\hat{e_n} ) of dimension equal to the dimensions in our space. We can then define any other vector in our space in terms of \\hat{e_1} \\hat{e_1} and \\hat{e_2} \\hat{e_2} . For example, r_e = 3\\hat{e_1} + 3 \\hat{e_2} = \\begin{bmatrix} 3 \\\\\\ 4 \\end{bmatrix} r_e = 3\\hat{e_1} + 3 \\hat{e_2} = \\begin{bmatrix} 3 \\\\\\ 4 \\end{bmatrix} Here, the instruction is that r_e r_e is going to be equal to doing a vector sum of 3 \\hat{e_1} 3 \\hat{e_1} and 4 \\hat{e_2} 4 \\hat{e_2} . If you think about it, our choice of \\hat{e_1} \\hat{e_1} and \\hat{e_2} \\hat{e_2} is kind of arbitrary. There's no reason we couldn't have used different vectors to define our coordinate system Note These vectors don't even need to be at 90 degrees to each other or of the same length In any case, I could still have described r r as being some sum of some vectors I used to define the space. We call the vectors we use to define our vector space (e.g. \\hat{e_1} \\hat{e_1} and \\hat{e_2} \\hat{e_2} ) basis vectors . What we realize here, is that our vector r r exists independently of the coordinate system we use. The vector still takes us from the origin to some point in space, even when we change the coordinate system, more specifically, even when we change the basis vectors used to describe our vector space . It turns out, we can actually change the basis of the vector r r (call this r_e r_e ) to a new set of basis vectors, i.e. \\hat{b_1} \\hat{b_1} and \\hat{b_2} \\hat{b_2} , which we will denote r_b r_b . Furthermore, we can do this using the dot product so long as The new basis vectors are orthogonal to each other, i.e. \\hat{b_1} \\cdot \\hat{b_2} = 0 \\hat{b_1} \\cdot \\hat{b_2} = 0 We know the position of \\hat{b_1} \\hat{b_1} and \\hat{b_2} \\hat{b_2} in the space defined by \\hat{e_1} \\hat{e_1} and \\hat{e_2} \\hat{e_2} . Note We can still change basis even when the new basis vectors are not orthogonal to one another, but for this we will need matrices. See later parts of the course. Lets define \\hat{b_1} \\hat{b_1} and \\hat{b_2} \\hat{b_2} in the space defined by \\hat{e_1} \\hat{e_1} and \\hat{e_2} \\hat{e_2} : \\hat{b_1} = \\begin{bmatrix} -2 \\\\\\ 4 \\end{bmatrix}, \\; \\hat{b_2} = \\begin{bmatrix} 2 \\\\\\ 1 \\end{bmatrix} \\hat{b_1} = \\begin{bmatrix} -2 \\\\\\ 4 \\end{bmatrix}, \\; \\hat{b_2} = \\begin{bmatrix} 2 \\\\\\ 1 \\end{bmatrix} In order to determine r_b r_b , i.e. the vector r r defined in terms of the basis vectors \\hat{b_1} \\hat{b_1} and \\hat{b_2} \\hat{b_2} , we need to take sum the vector projection of r_e r_e onto \\hat{b_1} \\hat{b_1} and the vector projection of r_e r_e onto \\hat{b_2} \\hat{b_2} So, lets do it: Vector projection of r_e r_e onto \\hat{b_1} \\hat{b_1} \\hat{b_1}\\frac{r_e \\cdot \\hat{b_1}}{\\vert \\hat{b_1}\\vert ^2} = \\frac{3 \\times 2 + 4 \\times 1}{2^2 + 1^2} = \\frac{10}{5} = 2 \\hat{b_1} = 2 \\begin{bmatrix} 2 \\\\\\ 1\\end{bmatrix} = \\begin{bmatrix} 4 \\\\\\ 2\\end{bmatrix} \\hat{b_1}\\frac{r_e \\cdot \\hat{b_1}}{\\vert \\hat{b_1}\\vert ^2} = \\frac{3 \\times 2 + 4 \\times 1}{2^2 + 1^2} = \\frac{10}{5} = 2 \\hat{b_1} = 2 \\begin{bmatrix} 2 \\\\\\ 1\\end{bmatrix} = \\begin{bmatrix} 4 \\\\\\ 2\\end{bmatrix} Vector projection of r_e r_e onto \\hat{b_2} \\hat{b_2} \\hat{b_2}\\frac{r_e \\cdot \\hat{b_2}}{\\vert \\hat{b_2}\\vert ^2} = \\frac{3 \\times -2 + 4 \\times 4}{-2^2 + 4^2} = \\frac{10}{20} = \\frac{1}{2} \\hat{b_2} = \\frac{1}{2} \\begin{bmatrix} -2 \\\\\\ 4\\end{bmatrix} = \\begin{bmatrix} -1 \\\\\\ 2\\end{bmatrix} \\hat{b_2}\\frac{r_e \\cdot \\hat{b_2}}{\\vert \\hat{b_2}\\vert ^2} = \\frac{3 \\times -2 + 4 \\times 4}{-2^2 + 4^2} = \\frac{10}{20} = \\frac{1}{2} \\hat{b_2} = \\frac{1}{2} \\begin{bmatrix} -2 \\\\\\ 4\\end{bmatrix} = \\begin{bmatrix} -1 \\\\\\ 2\\end{bmatrix} Thus, r_b = \\hat{b_1} \\frac{r_e \\cdot \\hat{b_1}}{\\vert \\hat{b_1}\\vert ^2} + \\hat{b_2} \\frac{r_e \\cdot \\hat{b_2}}{\\vert \\hat{b_2}\\vert ^2} = 2 \\hat{b_1} + \\frac{1}{2} \\hat{b_2} = \\begin{bmatrix}2 \\\\\\ \\frac{1}{2}\\end{bmatrix} r_b = \\hat{b_1} \\frac{r_e \\cdot \\hat{b_1}}{\\vert \\hat{b_1}\\vert ^2} + \\hat{b_2} \\frac{r_e \\cdot \\hat{b_2}}{\\vert \\hat{b_2}\\vert ^2} = 2 \\hat{b_1} + \\frac{1}{2} \\hat{b_2} = \\begin{bmatrix}2 \\\\\\ \\frac{1}{2}\\end{bmatrix} Finally, notice that r_b = 2 \\hat{b_1} + \\frac{1}{2} \\hat{b_2} = 2 \\begin{bmatrix} 2 \\\\\\ 1\\end{bmatrix} + \\frac{1}{2} \\begin{bmatrix} -2 \\\\\\ 4\\end{bmatrix} = \\begin{bmatrix} 3 \\\\\\ 4\\end{bmatrix} = r_e r_b = 2 \\hat{b_1} + \\frac{1}{2} \\hat{b_2} = 2 \\begin{bmatrix} 2 \\\\\\ 1\\end{bmatrix} + \\frac{1}{2} \\begin{bmatrix} -2 \\\\\\ 4\\end{bmatrix} = \\begin{bmatrix} 3 \\\\\\ 4\\end{bmatrix} = r_e","title":"Changing basis"},{"location":"linear_algebra/week_2/#conclusions","text":"We've seen that our vector describing our data isn't tied to the axis that we originally used to describe it . We can redescribe it using some other axis , some other basis vectors . It turns out that choosing basis vectors we use to describe the space of data carefully to help us solve our problem will be a very important thing in linear algebra, and in general. We can move the numbers in the vector we used to describe a data item from one basis to another. We can do that change just by taking the dot or projection product so long as the new basis factors are orthogonal to each other.","title":"Conclusions"},{"location":"linear_algebra/week_2/#basis-vector-space-and-linear-independence","text":"Tip Linear independence is really only brushed on here. To go deeper, check out this Khan Academy series. Previously we've seen that our basis vectors do not have to be the so called standard (or natural) basis . We can actually choose any basis vectors we want, which redefine how we we move about space. Standard Basis The set of unit vectors pointing in the direction of the axes of a Cartesian coordinate system. \\hat{e_x} = \\begin{bmatrix} 1 \\\\\\ 0 \\\\\\ 0 \\end{bmatrix}, \\; \\hat{e_y} = \\begin{bmatrix} 0 \\\\\\ 1 \\\\\\ 0 \\end{bmatrix}, \\; \\hat{e_z} = \\begin{bmatrix} 0 \\\\\\ 0 \\\\\\ 1 \\end{bmatrix} \\hat{e_x} = \\begin{bmatrix} 1 \\\\\\ 0 \\\\\\ 0 \\end{bmatrix}, \\; \\hat{e_y} = \\begin{bmatrix} 0 \\\\\\ 1 \\\\\\ 0 \\end{bmatrix}, \\; \\hat{e_z} = \\begin{bmatrix} 0 \\\\\\ 0 \\\\\\ 1 \\end{bmatrix} Note Also known as the orthonormal basis . Lets formally define what we mean by a basis (vector space), and define linear independence , which is going to let us understand how many dimensions our vector space actually has. Basis The basis is a set of n n vectors that: are not linear combinations of each other (linear independent) span the space that they describe If these two qualities are fulfilled, then the space defined by the basis is n n -dimensional. Linear independence A set of vectors is said to be linearly dependent if one of the vectors in the set can be defined as a linear combination of the others; if no vector in the set can be written in this way, then the vectors are said to be linearly independent . For example, imagine we had some candidate vector {b_3} {b_3} . If we could write {b_3} {b_3} as a linear combination of, say, {b_1} {b_1} and {b_2} {b_2} : {b_3} = c_1 {b_1} + c_2 {b_2} {b_3} = c_1 {b_1} + c_2 {b_2} where c_1 c_1 and c_2 c_2 were constants, then we would say that {b_3} {b_3} is linearly dependent on {b_1} {b_1} and {b_2} {b_2} . To drive the point home, we note that the following are true if {b_3} {b_3} is linearly dependent to the vectors {b_1} {b_1} and {b_2} {b_2} : {b_3} {b_3} does not lie in the plane spanned by {b_1} {b_1} and {b_2} {b_2} {b_3} \\ne c_1 {b_1} + c_2 {b_2} {b_3} \\ne c_1 {b_1} + c_2 {b_2} for any c_1 c_1 , c_2 \\in \\mathbb R c_2 \\in \\mathbb R OR, equivalently, 0 = c_1 {b_1} + c_2 {b_2} + c_3 {b_3} 0 = c_1 {b_1} + c_2 {b_2} + c_3 {b_3} implies that c_1 = c_2 = c_3 = 0 c_1 = c_2 = c_3 = 0 These concepts are central to the definition of dimension . As stated previously, if we have a set of n n basis vectors, then these vectors describe an n n -dimensional space, as we can express any n n -dimensional vector as a linear combination of our n n basis vectors. Now, notice what our basis vectors {b_n} {b_n} don't have to be. they don't have to be unit vectors, by which we mean vectors of length 1 and they don't have to be orthogonal (or normal ) to each other But, as it turns out, everything is going to be much easier if they are. So if at all possible, you want to construct what's called an orthonormal basic vector set , where all vectors of the set are at 90 90 degrees to each other and are all of unit length. Now, let's think about what happens when we map from one basis to another. The axes of the original grid are projected onto the new grid ; and potentially have different values on that new grid, but crucially , the projection keeps the grid being evenly spaced. Therefore, any mapping that we do from one set of basis vectors, (one coordinate system) to another set of basis vectors (another coordinate system), keeps the vector space being a regularly spaced grid , where our original rules of vector addition and multiplication by a scaler still work. Note Basically, it doesn't warp or fold space, which is what the linear bit in linear algebra means geometrically. Things might be stretched or rotated or inverted, but everything remains evenly spaced and linear combinations still work. Now, when the new basis vectors aren't orthogonal, then we won't be able to use the dot product (really, the projection) to map from one basis to another. We'll have to use matrices instead, which we'll meet in the next module. Tip Honestly, this part is tricky. It might be worth it to watch the first three videos of the Essence of Linear Algebra series. For the lazy, jumpy straight to this video .","title":"Basis, vector space, and linear independence"},{"location":"linear_algebra/week_2/#conclusions_1","text":"In this section, we've talked about the dimensionality of a vector space in terms of the number of independent basis factors that it has. We found a test for independence: vectors are independent if one of them is not a linear combination of the others. Finally, we discussed what that means to map vectors from one space to another and how that is going to be useful in data science and machine learning.","title":"Conclusions"},{"location":"linear_algebra/week_2/#summary-of-week-2","text":"We've looked at vectors as being objects that describe where we are in space which could be a physical space, a space of data, or a parameter space of the parameters of a function. It doesn't really matter. It's just some space. Then we've defined vector addition and scaling a vector by a number, making it bigger or reversing its direction. Then we've gone on to find the magnitude or modulus of a vector, and the dot scalar and vector projection product. We've defined the basis of a vector space, its dimension, and the ideas of linear independence and linear combinations. We've used projections to look at one case of changes from one basis to another, for the case where the new basis is orthogonal.","title":"Summary of week 2"},{"location":"linear_algebra/week_3/","text":"Week 3: Matrices as Objects that Operate on Vectors Lets now turn our attention from vectors to matrices . First we will look at how to use matrices as tools to solve linear algebra problems, before introducing them as objects that transform vectors. We will then explain how to solve systems of linear equations using matrices, which will introduce the concept of inverse matrices and determinants. Finally, we'll look at cases of special matrices: when the determinant is zero, and where the matrix isn't invertible. Because many algorithms require us to invert a matrix as one of their steps, this special case is important. Learning Objectives Understand what a matrix is and how it corresponds to a transformation Explain and calculate inverse and determinant of matrices Identify and explain how to find inverses computationally Explore what it means for a matrix to be inertible Matrices Introduction to matrices At the start of the course, we encountered the \"apples and bananas\" problem: how to find the price of things when we only have the total bill. Now we're going to look at matrices, which can be thought of as objects that rotate and stretch vectors, and how they can be used to solve these sorts of problems. Let's go back to that apples and bananas problem. Say we walk into a shop and we buy two apples, and three bananas and that costs us 8 euros: 2a + 3b = 8 2a + 3b = 8 On another day and we buy 10 apples and 1 banana, and that costs me 13 euros: 10a + 1b = 13 10a + 1b = 13 Note Now you might say this is silly. What shop doesn't have sticker prices after all? But actually, businesses with complicated products and service agreements often use price discovery . Now these are just simultaneous equations but , I can write them down with matrices as follows: \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} We can say that the matrix \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} operates on the vector \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} to give the other vector \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} Our question, our problem to solve, is what vector \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} transforms to give us \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} Now, what if we use our matrix \\begin{pmatrix}2 & 3 \\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}2 & 3 \\\\ 10 & 1\\end{pmatrix} to transform our basis vectors \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 ? \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\cdot \\hat e_1 = \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}1 \\\\\\ 0 \\end{pmatrix} = \\begin{pmatrix}2 \\\\\\ 10 \\end{pmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\cdot \\hat e_1 = \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}1 \\\\\\ 0 \\end{pmatrix} = \\begin{pmatrix}2 \\\\\\ 10 \\end{pmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\cdot \\hat e_2 = \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}0 \\\\\\ 1 \\end{pmatrix} = \\begin{pmatrix}3 \\\\\\ 1 \\end{pmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\cdot \\hat e_2 = \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}0 \\\\\\ 1 \\end{pmatrix} = \\begin{pmatrix}3 \\\\\\ 1 \\end{pmatrix} It becomes clear that what this matrix is doing is actually transforming the basis vectors \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 to give us the vectors \\begin{pmatrix}2 \\\\\\ 10 \\end{pmatrix} \\text{ , } \\begin{pmatrix}3 \\\\\\ 1 \\end{pmatrix} \\begin{pmatrix}2 \\\\\\ 10 \\end{pmatrix} \\text{ , } \\begin{pmatrix}3 \\\\\\ 1 \\end{pmatrix} Generally speaking, we can think of the matrix \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} as a function that operates on input vectors in order to give us output vectors. A set of simultaneous equations, like the ones we have here, is asking, in effect, what input vector I need in order to get a transformed product (the output vector ) at position \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} Conclusions Hopefully, it is a little more clear now what we mean now by the term linear algebra. Linear algebra is linear , because it just takes input values, our a a and b b for example, and multiplies them by constants . Everything is linear . Finally, it's algebra simply because it is a notation describing mathematical objects and a system of manipulating those notations. So linear algebra is a mathematical system for manipulating vectors in the spaces described by vectors . Note This is important! We are noticing some kind of deep connection between simultaneous equations, these things called matrices, and the vectors we were talking about last week. It turns that the key to solving simultaneous equation problems is appreciating how vectors are transformed by matrices, which is the heart of linear algebra. Matrices as objects that operate on vectors How matrices transform space Tip Watch this video before reading through this section. So far, we have introduced the idea of a matrix and related it to the problem of solving simultaneous equations . We showed that the columns of a matrix can be thought of as the transformations applied to unit basis vector along each axis. This is a pretty profound idea, so lets flesh it out. We know that we can make any (2D) vector out of a vector sum of the scaled versions of \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 (our basis vectors). This means that the result of any linear transformation is just going to be some sum of the transformed basis vectors, ( \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 here). This is a bit hard to see but what it means is that the grid lines of our space stay parallel and evenly spaced . They might be stretched or sheared, but the origin stays where it is and there isn't any curviness to the space, it doesn't get warped --- a consequence of our scalar addition and multiplication rules for vectors. If we write down the matrix A A and the vector it is transforming as r r , we can represent our apples and bananas problem introduced earlier as: \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} A r = r' A r = r' Where r' r' is our transformed vector. We can generalize further, and add a scalar, n n : A (n r) = n r' A (n r) = n r' We notice that: A (r + s) = A r + A s A (r + s) = A r + A s Putting it together, we can represent any vector in 2D space as: A (n \\hat e_1 + m \\hat e_2) = n A \\hat e_1 + m A \\hat e_2 A (n \\hat e_1 + m \\hat e_2) = n A \\hat e_1 + m A \\hat e_2 \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;= n \\hat e_1' + m \\hat e_2' \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;= n \\hat e_1' + m \\hat e_2' Now let's try an example. Let, A = \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} A = \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} r = \\begin{bmatrix}3 \\\\\\ 2\\end{bmatrix} r = \\begin{bmatrix}3 \\\\\\ 2\\end{bmatrix} Then, A r = r' A r = r' \\Rightarrow \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}3 \\\\\\ 2\\end{bmatrix} = \\begin{bmatrix}12 \\\\\\ 32\\end{bmatrix} \\Rightarrow \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}3 \\\\\\ 2\\end{bmatrix} = \\begin{bmatrix}12 \\\\\\ 32\\end{bmatrix} Which is no different than how we might have multiplied matrices and vectors in school. But, we can think of this another way: A (n \\hat e_1 + m \\hat e_2) = r' A (n \\hat e_1 + m \\hat e_2) = r' \\Rightarrow \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\Biggl (3 \\begin{bmatrix}1 \\\\\\ 0\\end{bmatrix} + 2 \\begin{bmatrix}0 \\\\\\ 1\\end{bmatrix} \\Biggl) = \\Rightarrow \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\Biggl (3 \\begin{bmatrix}1 \\\\\\ 0\\end{bmatrix} + 2 \\begin{bmatrix}0 \\\\\\ 1\\end{bmatrix} \\Biggl) = 3 \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}1 \\\\\\ 0\\end{bmatrix} + 2 \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}0 \\\\\\ 1\\end{bmatrix} = 3 \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}1 \\\\\\ 0\\end{bmatrix} + 2 \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}0 \\\\\\ 1\\end{bmatrix} = = 3 \\begin{bmatrix}2 \\\\\\ 10\\end{bmatrix} + 2 \\begin{bmatrix}3 \\\\\\ 1\\end{bmatrix} = \\begin{bmatrix}12 \\\\\\ 32\\end{bmatrix} = 3 \\begin{bmatrix}2 \\\\\\ 10\\end{bmatrix} + 2 \\begin{bmatrix}3 \\\\\\ 1\\end{bmatrix} = \\begin{bmatrix}12 \\\\\\ 32\\end{bmatrix} The take home idea here is that the matrix A A just tells us where the basis vectors go . That's the transformation it does. Types of matrix transformation Lets illustrate the type of transformations we can perform with matrices with a number of examples. Note Remember, in linear algebra, linear transformations can be represented by matrices! We are only going to scratch the surface here and to continue to build up our intuition of viewing matrices as functions that apply transformations to some input vector . Identity matrix First, let's think about a matrix that doesn't change anything. Such a matrix is just composed of the basis vectors of the space, \\begin{bmatrix} 1 & 0 \\\\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} this is known as the identity matrix . It's the matrix that does nothing and leaves everything preserved, typically denoted I_m I_m where m m is the number of dimensions in our vector space. Scaling When our matrix contains values other than 0 0 in the diagnonal, we get a scaling : \\begin{bmatrix} m & 0 \\\\\\ 0 & n\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} mx \\\\\\ ny\\end{bmatrix} \\begin{bmatrix} m & 0 \\\\\\ 0 & n\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} mx \\\\\\ ny\\end{bmatrix} If m = 3 m = 3 and n=2 n=2 , visually this looks like: This transformation simply scales each dimension of the space by the value at the corresponding diagonal of the matrix. Note Note that when m \\lt 1 m \\lt 1 and/or n \\lt 1 n \\lt 1 , our space is actually compressed along the axes. Reflections When one or more of our diagonal values is negative, we get a reflection : \\begin{bmatrix} -1 & 0 \\\\\\ 0 & k\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} -x \\\\\\ ky\\end{bmatrix} \\begin{bmatrix} -1 & 0 \\\\\\ 0 & k\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} -x \\\\\\ ky\\end{bmatrix} In this case, our coordinate system is flipped across the vertical axis. When k = 2 k = 2 , visually this looks like: In another example, \\begin{bmatrix} -1 & 0 \\\\\\ 0 & -1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} -x \\\\\\ -y\\end{bmatrix} \\begin{bmatrix} -1 & 0 \\\\\\ 0 & -1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} -x \\\\\\ -y\\end{bmatrix} This transformation inverts all axes, and is known as an inversion . We can also produce mirror reflections over a straight line that crosses through the origin: \\begin{bmatrix} 0 & 1 \\\\\\ 1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} y \\\\\\ x\\end{bmatrix} \\begin{bmatrix} 0 & 1 \\\\\\ 1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} y \\\\\\ x\\end{bmatrix} or, \\begin{bmatrix} 0 & -1 \\\\\\ -1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} -y \\\\\\ -x\\end{bmatrix} \\begin{bmatrix} 0 & -1 \\\\\\ -1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} -y \\\\\\ -x\\end{bmatrix} Note Of course, we can also produce mirror transformations over the x x or y y axis as well, by making one of the diagonals 1 1 and the other -1 -1 . Shears Shears are visually similar to slanting . There are two possibilities: A shear parallel to the x x axis looks like: \\begin{bmatrix} 1 & k \\\\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x + ky \\\\\\ y\\end{bmatrix} \\begin{bmatrix} 1 & k \\\\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x + ky \\\\\\ y\\end{bmatrix} If k=1 k=1 , then visually this looks like: Or a shear parallel to the y y axis: \\begin{bmatrix} 1 & 0 \\\\\\ k & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\\\\\ kx + y\\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\\\ k & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\\\\\ kx + y\\end{bmatrix} Rotations Finally, we can rotate the space about the orign. For example, \\begin{bmatrix} 0 & -1 \\\\\\ 1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} y \\\\\\ -x\\end{bmatrix} \\begin{bmatrix} 0 & -1 \\\\\\ 1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} y \\\\\\ -x\\end{bmatrix} would rotate the entire space 90^0 90^0 counterclockwise. More generally, for a rotation by a angle \\theta \\theta clockwise about the orgin: \\begin{bmatrix} \\cos \\theta & \\sin \\theta \\\\\\ - \\sin \\theta & \\cos \\theta \\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\cos \\theta + y \\sin \\theta \\\\\\ - x \\sin \\theta + y \\cos \\theta\\end{bmatrix} \\begin{bmatrix} \\cos \\theta & \\sin \\theta \\\\\\ - \\sin \\theta & \\cos \\theta \\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\cos \\theta + y \\sin \\theta \\\\\\ - x \\sin \\theta + y \\cos \\theta\\end{bmatrix} and for a rotation by a angle \\theta \\theta counterclockwise about the orgin \\begin{bmatrix} \\cos \\theta & - \\sin \\theta \\\\\\ \\sin \\theta & \\cos \\theta \\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\cos \\theta - y \\sin \\theta \\\\\\ x \\sin \\theta + y \\cos \\theta\\end{bmatrix} \\begin{bmatrix} \\cos \\theta & - \\sin \\theta \\\\\\ \\sin \\theta & \\cos \\theta \\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\cos \\theta - y \\sin \\theta \\\\\\ x \\sin \\theta + y \\cos \\theta\\end{bmatrix} Conclusions In this section, we described the major transformations that a matrix can perform on a vector. Next, we will look at how we can combine these transformations (known as composition ) to produce more complex matrix transformations Composition or combination of matrix transformations Tip Watch this video before reading through this section. So what is the point of introducing these different geometric transformations in this class? Well, if you want to do any kind of shape alteration, say of all the pixels in an image, or a face, then you can always make that shape change out of some combination of rotations , shears , stretches , and inverses . Note One example where these geometric transformations may be useful is in facial recognition , where we may preprocess every image by transforming it so that the person(s) face(s) are directly facing the camera. Lets illustrate this composition of matrix transformations with an example. Here, we will first apply a 90^o 90^o rotation clockwise about the x x -axis, and then a shear parallel to the x x -axis. Let the first transformation matrix be A_1 A_1 : \\begin{bmatrix} 0 & 1 \\\\\\ -1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} y \\\\\\ -x\\end{bmatrix} \\begin{bmatrix} 0 & 1 \\\\\\ -1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} y \\\\\\ -x\\end{bmatrix} And our second transformation matrix A_2 A_2 : \\begin{bmatrix} 1 & 1 \\\\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x + y \\\\\\ y\\end{bmatrix} \\begin{bmatrix} 1 & 1 \\\\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x + y \\\\\\ y\\end{bmatrix} Applying A_1 A_1 to our basis vectors \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 gives us \\hat e_1' \\hat e_1' and \\hat e_1' \\hat e_1' : \\hat e_1' = A_1 \\cdot \\hat e_1 = \\begin{bmatrix} 0 \\\\\\ -1\\end{bmatrix} \\hat e_1' = A_1 \\cdot \\hat e_1 = \\begin{bmatrix} 0 \\\\\\ -1\\end{bmatrix} \\hat e_2' = A_1 \\cdot \\hat e_2 = \\begin{bmatrix} 1 \\\\\\ 0\\end{bmatrix} \\hat e_2' = A_1 \\cdot \\hat e_2 = \\begin{bmatrix} 1 \\\\\\ 0\\end{bmatrix} Applying A_2 A_2 to our new transformed vectors \\hat e_1' \\hat e_1' and \\hat e_2' \\hat e_2' gives us \\hat e_1'' \\hat e_1'' and \\hat e_1'' \\hat e_1'' : \\hat e_1'' = A_2 \\cdot \\hat e_1' = \\begin{bmatrix}-1 \\\\\\ -1\\end{bmatrix} \\hat e_1'' = A_2 \\cdot \\hat e_1' = \\begin{bmatrix}-1 \\\\\\ -1\\end{bmatrix} \\hat e_2'' = A_2 \\cdot \\hat e_2' = \\begin{bmatrix} 1 \\\\\\ 0\\end{bmatrix} \\hat e_2'' = A_2 \\cdot \\hat e_2' = \\begin{bmatrix} 1 \\\\\\ 0\\end{bmatrix} Notice that if we stack our final transformed vectors \\hat e_1'' \\hat e_1'' and \\hat e_1'' \\hat e_1'' as columns we get the matrix: \\begin{bmatrix} -1 & 1 \\\\\\ -1 & 0\\end{bmatrix} \\begin{bmatrix} -1 & 1 \\\\\\ -1 & 0\\end{bmatrix} Which is equal to A_2 \\cdot A_1 A_2 \\cdot A_1 . Geometrically, this looks like: Conclusions The take home message here is that the transformation A_2 \\cdot (A_1 \\cdot r) A_2 \\cdot (A_1 \\cdot r) for some transformation matrices A_1 A_1 and A_2 A_2 and some vector r r , is equivalent to the transformation (A_2A_1) \\cdot r (A_2A_1) \\cdot r Note Note that A_2 \\cdot (A_1 \\cdot r) \\ne A_1 \\cdot (A_2 \\cdot r) A_2 \\cdot (A_1 \\cdot r) \\ne A_1 \\cdot (A_2 \\cdot r) , that is, the order in which we apply our transformations matters . As it turns out, the key to solving simultaneous equation problems is appreciating how vectors are transformed by matrices, which is at the the heart of linear algebra. Matrix inverses In this section, we are finally going to present a way to solve the apples and bananas problem with matrices. Along the way, we're going to find out about a thing called the inverse of a matrix and a method for finding it. Tip Watch this video before reading this section. For more practice with matrix inverses, see Khan Academy sections here and here . Gaussian elimination: Solving the apples and bananas problem First, recall our apples and bananas problem in matrix form: \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix} 8 \\\\\\ 3\\end{bmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix} 8 \\\\\\ 3\\end{bmatrix} Where we want to find the price of individual apples ( a a ) and bananas ( b b ). Simplifying, lets say that: A\\cdot r = s A\\cdot r = s Note This, in effect is saying: \" A A operates on vector r r to give us s s \". To solve for r r , we need to move A A to the other side of the equation. But how? . Well, to \"undo\" a division (and isolate x x ), you multiply by the reciprocal: \\frac{1}{2}x = 4 \\frac{1}{2}x = 4 \\Rightarrow x = 8 \\Rightarrow x = 8 Likewise, to undo a multiplication, we divide by the reciprocal: 2x = 4 2x = 4 \\Rightarrow x = 2 \\Rightarrow x = 2 How do we undo the transformation performed by A A ? The answer is to find the matrix A^{-1} A^{-1} (known as the inverse of A A ) such that: A^{-1}A = I A^{-1}A = I where I I is the identity matrix . We call A^{-1} A^{-1} the inverse of A A because is it reverses whatever transformation A A does, giving us back I I . We note that: A^{-1}A\\cdot r = A^{-1} s A^{-1}A\\cdot r = A^{-1} s \\Rightarrow\\ I \\cdot r = A^{-1} s \\Rightarrow\\ I \\cdot r = A^{-1} s \\Rightarrow\\ r = A^{-1} s \\Rightarrow\\ r = A^{-1} s So, if we could find the inverse of A A (i.e. find A^{-1} A^{-1} ), we can solve our problem (i.e. find a a and b b ). We can solve for A^{-1} A^{-1} with a series of row operations or substitutions (known as Gaussian elimination ). Let's look at a slightly more complicated problem to see how this is done: \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 15 \\\\\\ 21 \\\\\\ 13\\end{bmatrix} \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 15 \\\\\\ 21 \\\\\\ 13\\end{bmatrix} We start by subtracting row 1 from rows 2 and 3, which gives us a matrix in row echelon form (technically, reduced row echelon form ): \\begin{pmatrix}1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 15 \\\\\\ 6 \\\\\\ 2\\end{bmatrix} \\begin{pmatrix}1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 15 \\\\\\ 6 \\\\\\ 2\\end{bmatrix} We then perform two steps of back substitution to get the identify matrix: \\begin{pmatrix}1 & 1 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 9 \\\\\\ 4 \\\\\\ 2\\end{bmatrix} \\begin{pmatrix}1 & 1 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 9 \\\\\\ 4 \\\\\\ 2\\end{bmatrix} \\begin{pmatrix}1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 5 \\\\\\ 4 \\\\\\ 2\\end{bmatrix} \\begin{pmatrix}1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 5 \\\\\\ 4 \\\\\\ 2\\end{bmatrix} We can then read the solution right from the matrices a = 5 a = 5 , b = 4 b = 4 , c = 2 c = 2 . Tip If you are still feeling uneasy about using Gaussian elimination to solve a system of linear equations, see here for an example walked through step-by-step example. Full credit to PatrickJMT . Conclusions As it turns out, we don't have to compute the inverse at all to solve a system of linear equations. Although we showed the process of Gaussian elimination for some vectors r r and s s , we can use it in the general case to solve for any linear equation of the form A\\cdot r = s A\\cdot r = s . This actually one of the most computationally efficient ways to solve this problem, and it's going to work every time. From Gaussian elimination to finding the inverse matrix Now, let's think about how we can apply this idea of elimination to find the inverse matrix, which solves the more general problem no matter what vectors I write down on the right hand side. Say I have a 3 \\times \\times 3 matrix A A and its inverse B B ( B = A^{-1}) B = A^{-1}) . By the definition of the inverse , AB = BA = I_n AB = BA = I_n If we use our matrix A A from the last section: \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = I \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = I Note Where b_{ij} b_{ij} is the element at the i^{th} i^{th} row and j^{th} j^{th} column of matrix B B . we notice that the first column of B B is just a vector. It's a vector that describes what the B B matrix, the inverse of A A , does to space. Note Actually, it's the transformation that that vector does to the x-axis. This means that: \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{pmatrix}b_{11} \\\\\\ b_{21} \\\\\\ b_{31}\\end{pmatrix} = \\begin{pmatrix} 1 \\\\\\ 0 \\\\\\ 0\\end{pmatrix} \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{pmatrix}b_{11} \\\\\\ b_{21} \\\\\\ b_{31}\\end{pmatrix} = \\begin{pmatrix} 1 \\\\\\ 0 \\\\\\ 0\\end{pmatrix} Now, we could solve this by the elimination method and back substitution in just the way we did before. Then, we could do it again for the second column of B B , and finally the third. In this way, we would have solved for B B , in other words, we would have found A^{-1} A^{-1} . It turns out, we can actually solve for B B all at once. So lets do that: \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = I \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = I Subtract row 1 from row 2 and 3: \\Rightarrow \\begin{pmatrix}1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & -1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix}1 & 0 & 0 \\\\\\ -1 & 1 & 0 \\\\\\ -1 & 0 & 1\\end{pmatrix} \\Rightarrow \\begin{pmatrix}1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & -1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix}1 & 0 & 0 \\\\\\ -1 & 1 & 0 \\\\\\ -1 & 0 & 1\\end{pmatrix} Multiply row 3 by -1 -1 : \\Rightarrow \\begin{pmatrix}1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix}1 & 0 & 0 \\\\\\ -1 & 1 & 0 \\\\\\ 1 & 0 & -1\\end{pmatrix} \\Rightarrow \\begin{pmatrix}1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix}1 & 0 & 0 \\\\\\ -1 & 1 & 0 \\\\\\ 1 & 0 & -1\\end{pmatrix} Now that row 3 is in row echelon form , we can substitute it back into row 2 and row 1: \\Rightarrow \\begin{pmatrix}1 & 1 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} -2 & 0 & 3 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} \\Rightarrow \\begin{pmatrix}1 & 1 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} -2 & 0 & 3 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} And finally, back substitute row 2 into row 1: \\Rightarrow \\begin{pmatrix}1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} 0 & -1 & 2 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} \\Rightarrow \\begin{pmatrix}1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} 0 & -1 & 2 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} Because any matrix times its identity is that matrix itself: \\begin{pmatrix}1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} 0 & -1 & 2 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} \\begin{pmatrix}1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} 0 & -1 & 2 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} \\Rightarrow \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} 0 & -1 & 2 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} \\Rightarrow \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} 0 & -1 & 2 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} Note You can actually prove to yourself that we got the right answer by checking that A \\cdot B = I A \\cdot B = I . So that's our answer. We've found the identity matrix, B = A^{-1} B = A^{-1} for A A , and we did this by transforming A A into it's identity matrix via elimination and back substitution. Moreover, because B B could be any matrix, we have solved this in the general case. The solution is the same regardless of the number of dimensions, and this leads to a computationally efficient way to invert a matrix. Note There are computationally faster methods of computing the inverse, one such method is known as a decomposition process . In practice, you will simply call the solver of your problem or function, something like inv(A) , and it will pick the best method by inspecting the matrix you give it and return the answer. Conclusions We have figured out how to solve sets of linear equations in the general case, by a procedure we can implement in a computer really easily (known as Gaussian elimination ), and we've generalized this method to the to find the inverse of a matrix, regardless of what is on the right hand side of our system of equations. Special matrices In the final section of this week, we're going to look at a property of a matrix called the determinant . Note The determinant is a value that can be computed from the elements of a square matrix . The determinant of a matrix A A is denoted det(A) det(A) , det A det A , or \\vert A \\vert \\vert A \\vert . Geometrically, it can be viewed as the scaling factor of the linear transformation described by the matrix. We'll also look at what happens when a matrix doesn't have linearly independent basis vectors. Tip Watch this video before reading this section. For more practice with matrix inverses, see Khan Academy sections here and here . The determinant Let's start by looking at the simple matrix: \\begin{pmatrix} a & 0 \\\\\\ 0 & d\\end{pmatrix} \\begin{pmatrix} a & 0 \\\\\\ 0 & d\\end{pmatrix} If we multiply this matrix by our basis vectors \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 , we get \\hat e_1' = \\begin{bmatrix} a \\\\\\ 0 \\end{bmatrix} \\hat e_1' = \\begin{bmatrix} a \\\\\\ 0 \\end{bmatrix} \\hat e_2' = \\begin{bmatrix} 0 \\\\\\ d \\end{bmatrix} \\hat e_2' = \\begin{bmatrix} 0 \\\\\\ d \\end{bmatrix} So, in plain english, we have stretched our x x -axis by a factor of a a and our y y -axis by a factor of d d and, therefore, scaled the area of the grid cells of the vector space by a factor of ad ad . Note This is a property of the fact that any linear transformation keeps grid lines parallel and evenly spaced. We call this number, ad ad the determinant . Now, if I instead have a matrix: \\begin{pmatrix} a & b \\\\\\ 0 & d\\end{pmatrix} \\begin{pmatrix} a & b \\\\\\ 0 & d\\end{pmatrix} then this is still going to stretch \\hat e_1 \\hat e_1 out by a factor of a a , but on the other axis, I am going to move \\hat e_2 \\hat e_2 hat to: \\hat e_2 = \\begin{pmatrix} b \\\\\\ d\\end{pmatrix} \\hat e_2 = \\begin{pmatrix} b \\\\\\ d\\end{pmatrix} What we have done is taken the original grid and stretched it along the x x -axis by a a and, along the y y -axis by d d and sheared it (parallel to the x x -axis) by b b . Note We've still changed the size, the scale of the space (which is what the determinant really is) by a factor of ad ad . Notice that the area defined by our transformed vectors \\hat e_1' \\hat e_1' and \\hat e_2' \\hat e_2' is still just the base times the perpendicular height, ad ad (the determinant). Lets flesh this out in the general case. Say we have the matrix: A = \\begin{pmatrix} a & b \\\\\\ c & d\\end{pmatrix} A = \\begin{pmatrix} a & b \\\\\\ c & d\\end{pmatrix} Multiplying this matrix by our basis vectors yields a parallelogram (stretched by a a and d d and sheared by b b and c c ). We can actual compute the area of this parallelogram as follows: Note We find the area of this parallelogram by finding the area of the whole box the encloses it, and subtracting off combined area of the the little bits around it. The exact method for solving the area is not important (although it is pretty trivial). What is important, is that the determinant of A A can be computed as \\vert A \\vert = ad - bc \\vert A \\vert = ad - bc , and that this computation has a geometric interpretation . Note This is the formula for the determinant of a square 2 \\times 2 2 \\times 2 matrix. See here for the formula for higher dimensional matrices. Now, in school when you looked at matrices, you probably saw that you could find the inverse in the following way. For a matrix: A = \\begin{pmatrix} a & b \\\\\\ c & d\\end{pmatrix} A = \\begin{pmatrix} a & b \\\\\\ c & d\\end{pmatrix} Exchange a a and the d d , and switch the sign on the b b and the c c Multiply A A by this matrix Scale the transformation by \\frac{1}{ad - bc} \\frac{1}{ad - bc} In the general case, this looks like: \\frac{1}{ad - bc} \\begin{pmatrix} a & b \\\\\\ c & d\\end{pmatrix} \\begin{pmatrix} d & -b \\\\\\ -c & a\\end{pmatrix} = \\frac{1}{ad - bc} \\begin{pmatrix} ad - bc & 0 \\\\\\ 0 & ad - bc\\end{pmatrix} = I \\frac{1}{ad - bc} \\begin{pmatrix} a & b \\\\\\ c & d\\end{pmatrix} \\begin{pmatrix} d & -b \\\\\\ -c & a\\end{pmatrix} = \\frac{1}{ad - bc} \\begin{pmatrix} ad - bc & 0 \\\\\\ 0 & ad - bc\\end{pmatrix} = I This demonstrates that \\frac{1}{ad - bc} \\begin{pmatrix} d & -b \\\\\\ -c & a\\end{pmatrix} \\frac{1}{ad - bc} \\begin{pmatrix} d & -b \\\\\\ -c & a\\end{pmatrix} is in fact the inverse of the matrix A A . This helps capture what the determinant really is. It's the amount by which the original matrix scaled vector space. In the above example, dividing by the determinant normalizes the space back to its original size. Note We could spend a lot of time talking about how to solve for the determinant. However, knowing how to do the operations isn't really a useful skill. Many programming libraries (e.g. python ) have linear algebra libraries (e.g. Numpy ) which makes computing the determinant as easy, for example, as calling det(A) . If you really want to know how to compute determinants by hand, then look up a QR decomposition online. A determinant of zero Now, let's think about the matrix A = \\begin{pmatrix} 1 & 2 \\\\\\ 1 & 2\\end{pmatrix} A = \\begin{pmatrix} 1 & 2 \\\\\\ 1 & 2\\end{pmatrix} If we multiply this matrix by our basis vectors \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 , we get \\hat e_1' = \\begin{bmatrix} 1 \\\\\\ 1 \\end{bmatrix} \\hat e_1' = \\begin{bmatrix} 1 \\\\\\ 1 \\end{bmatrix} \\hat e_2' = \\begin{bmatrix} 2 \\\\\\ 2 \\end{bmatrix} \\hat e_2' = \\begin{bmatrix} 2 \\\\\\ 2 \\end{bmatrix} So this matrix, when applied to our vector space, actually collapses it onto a line . All our y y 's are mapped to the vector \\begin{bmatrix}2 \\\\\\ 2 \\end{bmatrix} \\begin{bmatrix}2 \\\\\\ 2 \\end{bmatrix} and our x x 's to \\begin{bmatrix}1 \\\\\\ 1 \\end{bmatrix} \\begin{bmatrix}1 \\\\\\ 1 \\end{bmatrix} Correspondingly, we notice that \\vert A \\vert = 0 \\vert A \\vert = 0 . Therefore, the area enclosed by the new basis vectors is zero . A negative determinant A negative determinant simply means that the transformation has flipped the orientation of our vector space. This is much easier to see than to explain ; check out this video which presents some awesome visualizations of the determinant. What the determinant means numerically To drive home the numerical interpretation of the determinant, lets start with this set of simultaneous equations: \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 2 & 3 & 7\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 17 \\\\\\ 29\\end{pmatrix} \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 2 & 3 & 7\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 17 \\\\\\ 29\\end{pmatrix} You'll notice that both the rows and the columns are linearly dependent . Thinking about the columns of this matrix as the basis vectors of some 3-dimensional space, we note that this transformation collapses our vector space from being 3D to 2D (by collapsing every point in space onto a plane). Let's see what this means numerically by trying to reduce our matrix to row-echelon form: \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 2 & 3 & 7\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 17 \\\\\\ 29\\end{pmatrix} \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 2 & 3 & 7\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 17 \\\\\\ 29\\end{pmatrix} Subtract row 1 from row 2, \\Rightarrow \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 2 & 3 & 7\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 5 \\\\\\ 29\\end{pmatrix} \\Rightarrow \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 2 & 3 & 7\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 5 \\\\\\ 29\\end{pmatrix} Subtract row 1 plus row 2 from row 3, \\Rightarrow \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & 0\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 5 \\\\\\ 0\\end{pmatrix} \\Rightarrow \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & 0\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 5 \\\\\\ 0\\end{pmatrix} while the matrix is now is row-echelon form, we don't have a final entry of the matrix ( 0\\cdot c = 0 0\\cdot c = 0 ). Because we don't have a solution for c c we can't back substitute, and we can't solve our system of equations. Note The reason we can't solve this system is because we don't have enough information. In keeping with our apples and bananas problem, imagine that when we went in to buy apples and bananas and carrots the third time, we ordered just a the sum of first two orders. Therefore, we didn't get any new information and thus don't have enough data to find out the solution for how much apples and bananas and carrots cost. This third order wasn't linearly independent from our first two, in the language of matrices and vectors. So, when the basis vectors describing a matrix aren't linearly independent, then the determinant is zero , and we can't solve the system. The loss of information when we map from n n -dimensional to (n-x) (n-x) -dimensional space (where x \\ge 1 x \\ge 1 ) means we cannot possibly know what the inverse matrix is (as it is impossible to map a lower dimensional space back to the original, higher dimensional space). When a matrix has no inverse, we say that it is singular . Note There are situations where we might want to do a transformation that collapses the number of dimensions in a space, but it means that we cannot possibly reverse the mapping, meaning the matrix has no inverse. This also means we cannot solve a system of linear equations defined by a singular matrix using Gaussian elimination and back substitution. Summary In the last section of this week, we took a look at the determinant, which is how much a given transformation scales our space. In 2-dimensions, this can be thought as the scalar multiple appleid to any area of our space, and in 3-dimensions any volume of our space. We also looked at the special case where the determinant is zero and found that this means that the basis vectors aren't linearly independent, which in turn means that the inverse doesn't exist. To summarize Week 3 , we introduced matrices as objects that transforms space. looked at different archetypes of matrices, like rotations , inverses , stretches , and shears , how to combine matrices by doing successive transformations, known as m atrix multiplication or composition how to solve systems of linear equations by elimination and how to find inverses and finally, we introduced determinants and showed how that relates to the concept of linear independence .","title":"Week 3"},{"location":"linear_algebra/week_3/#week-3-matrices-as-objects-that-operate-on-vectors","text":"Lets now turn our attention from vectors to matrices . First we will look at how to use matrices as tools to solve linear algebra problems, before introducing them as objects that transform vectors. We will then explain how to solve systems of linear equations using matrices, which will introduce the concept of inverse matrices and determinants. Finally, we'll look at cases of special matrices: when the determinant is zero, and where the matrix isn't invertible. Because many algorithms require us to invert a matrix as one of their steps, this special case is important. Learning Objectives Understand what a matrix is and how it corresponds to a transformation Explain and calculate inverse and determinant of matrices Identify and explain how to find inverses computationally Explore what it means for a matrix to be inertible","title":"Week 3: Matrices as Objects that Operate on Vectors"},{"location":"linear_algebra/week_3/#matrices","text":"","title":"Matrices"},{"location":"linear_algebra/week_3/#introduction-to-matrices","text":"At the start of the course, we encountered the \"apples and bananas\" problem: how to find the price of things when we only have the total bill. Now we're going to look at matrices, which can be thought of as objects that rotate and stretch vectors, and how they can be used to solve these sorts of problems. Let's go back to that apples and bananas problem. Say we walk into a shop and we buy two apples, and three bananas and that costs us 8 euros: 2a + 3b = 8 2a + 3b = 8 On another day and we buy 10 apples and 1 banana, and that costs me 13 euros: 10a + 1b = 13 10a + 1b = 13 Note Now you might say this is silly. What shop doesn't have sticker prices after all? But actually, businesses with complicated products and service agreements often use price discovery . Now these are just simultaneous equations but , I can write them down with matrices as follows: \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} We can say that the matrix \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} operates on the vector \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} to give the other vector \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} Our question, our problem to solve, is what vector \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} transforms to give us \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} Now, what if we use our matrix \\begin{pmatrix}2 & 3 \\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}2 & 3 \\\\ 10 & 1\\end{pmatrix} to transform our basis vectors \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 ? \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\cdot \\hat e_1 = \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}1 \\\\\\ 0 \\end{pmatrix} = \\begin{pmatrix}2 \\\\\\ 10 \\end{pmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\cdot \\hat e_1 = \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}1 \\\\\\ 0 \\end{pmatrix} = \\begin{pmatrix}2 \\\\\\ 10 \\end{pmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\cdot \\hat e_2 = \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}0 \\\\\\ 1 \\end{pmatrix} = \\begin{pmatrix}3 \\\\\\ 1 \\end{pmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\cdot \\hat e_2 = \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}0 \\\\\\ 1 \\end{pmatrix} = \\begin{pmatrix}3 \\\\\\ 1 \\end{pmatrix} It becomes clear that what this matrix is doing is actually transforming the basis vectors \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 to give us the vectors \\begin{pmatrix}2 \\\\\\ 10 \\end{pmatrix} \\text{ , } \\begin{pmatrix}3 \\\\\\ 1 \\end{pmatrix} \\begin{pmatrix}2 \\\\\\ 10 \\end{pmatrix} \\text{ , } \\begin{pmatrix}3 \\\\\\ 1 \\end{pmatrix} Generally speaking, we can think of the matrix \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} as a function that operates on input vectors in order to give us output vectors. A set of simultaneous equations, like the ones we have here, is asking, in effect, what input vector I need in order to get a transformed product (the output vector ) at position \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix}","title":"Introduction to matrices"},{"location":"linear_algebra/week_3/#conclusions","text":"Hopefully, it is a little more clear now what we mean now by the term linear algebra. Linear algebra is linear , because it just takes input values, our a a and b b for example, and multiplies them by constants . Everything is linear . Finally, it's algebra simply because it is a notation describing mathematical objects and a system of manipulating those notations. So linear algebra is a mathematical system for manipulating vectors in the spaces described by vectors . Note This is important! We are noticing some kind of deep connection between simultaneous equations, these things called matrices, and the vectors we were talking about last week. It turns that the key to solving simultaneous equation problems is appreciating how vectors are transformed by matrices, which is the heart of linear algebra.","title":"Conclusions"},{"location":"linear_algebra/week_3/#matrices-as-objects-that-operate-on-vectors","text":"","title":"Matrices as objects that operate on vectors"},{"location":"linear_algebra/week_3/#how-matrices-transform-space","text":"Tip Watch this video before reading through this section. So far, we have introduced the idea of a matrix and related it to the problem of solving simultaneous equations . We showed that the columns of a matrix can be thought of as the transformations applied to unit basis vector along each axis. This is a pretty profound idea, so lets flesh it out. We know that we can make any (2D) vector out of a vector sum of the scaled versions of \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 (our basis vectors). This means that the result of any linear transformation is just going to be some sum of the transformed basis vectors, ( \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 here). This is a bit hard to see but what it means is that the grid lines of our space stay parallel and evenly spaced . They might be stretched or sheared, but the origin stays where it is and there isn't any curviness to the space, it doesn't get warped --- a consequence of our scalar addition and multiplication rules for vectors. If we write down the matrix A A and the vector it is transforming as r r , we can represent our apples and bananas problem introduced earlier as: \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix}8 \\\\\\ 13\\end{bmatrix} A r = r' A r = r' Where r' r' is our transformed vector. We can generalize further, and add a scalar, n n : A (n r) = n r' A (n r) = n r' We notice that: A (r + s) = A r + A s A (r + s) = A r + A s Putting it together, we can represent any vector in 2D space as: A (n \\hat e_1 + m \\hat e_2) = n A \\hat e_1 + m A \\hat e_2 A (n \\hat e_1 + m \\hat e_2) = n A \\hat e_1 + m A \\hat e_2 \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;= n \\hat e_1' + m \\hat e_2' \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;= n \\hat e_1' + m \\hat e_2' Now let's try an example. Let, A = \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} A = \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} r = \\begin{bmatrix}3 \\\\\\ 2\\end{bmatrix} r = \\begin{bmatrix}3 \\\\\\ 2\\end{bmatrix} Then, A r = r' A r = r' \\Rightarrow \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}3 \\\\\\ 2\\end{bmatrix} = \\begin{bmatrix}12 \\\\\\ 32\\end{bmatrix} \\Rightarrow \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}3 \\\\\\ 2\\end{bmatrix} = \\begin{bmatrix}12 \\\\\\ 32\\end{bmatrix} Which is no different than how we might have multiplied matrices and vectors in school. But, we can think of this another way: A (n \\hat e_1 + m \\hat e_2) = r' A (n \\hat e_1 + m \\hat e_2) = r' \\Rightarrow \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\Biggl (3 \\begin{bmatrix}1 \\\\\\ 0\\end{bmatrix} + 2 \\begin{bmatrix}0 \\\\\\ 1\\end{bmatrix} \\Biggl) = \\Rightarrow \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\Biggl (3 \\begin{bmatrix}1 \\\\\\ 0\\end{bmatrix} + 2 \\begin{bmatrix}0 \\\\\\ 1\\end{bmatrix} \\Biggl) = 3 \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}1 \\\\\\ 0\\end{bmatrix} + 2 \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}0 \\\\\\ 1\\end{bmatrix} = 3 \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}1 \\\\\\ 0\\end{bmatrix} + 2 \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}0 \\\\\\ 1\\end{bmatrix} = = 3 \\begin{bmatrix}2 \\\\\\ 10\\end{bmatrix} + 2 \\begin{bmatrix}3 \\\\\\ 1\\end{bmatrix} = \\begin{bmatrix}12 \\\\\\ 32\\end{bmatrix} = 3 \\begin{bmatrix}2 \\\\\\ 10\\end{bmatrix} + 2 \\begin{bmatrix}3 \\\\\\ 1\\end{bmatrix} = \\begin{bmatrix}12 \\\\\\ 32\\end{bmatrix} The take home idea here is that the matrix A A just tells us where the basis vectors go . That's the transformation it does.","title":"How matrices transform space"},{"location":"linear_algebra/week_3/#types-of-matrix-transformation","text":"Lets illustrate the type of transformations we can perform with matrices with a number of examples. Note Remember, in linear algebra, linear transformations can be represented by matrices! We are only going to scratch the surface here and to continue to build up our intuition of viewing matrices as functions that apply transformations to some input vector . Identity matrix First, let's think about a matrix that doesn't change anything. Such a matrix is just composed of the basis vectors of the space, \\begin{bmatrix} 1 & 0 \\\\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} this is known as the identity matrix . It's the matrix that does nothing and leaves everything preserved, typically denoted I_m I_m where m m is the number of dimensions in our vector space. Scaling When our matrix contains values other than 0 0 in the diagnonal, we get a scaling : \\begin{bmatrix} m & 0 \\\\\\ 0 & n\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} mx \\\\\\ ny\\end{bmatrix} \\begin{bmatrix} m & 0 \\\\\\ 0 & n\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} mx \\\\\\ ny\\end{bmatrix} If m = 3 m = 3 and n=2 n=2 , visually this looks like: This transformation simply scales each dimension of the space by the value at the corresponding diagonal of the matrix. Note Note that when m \\lt 1 m \\lt 1 and/or n \\lt 1 n \\lt 1 , our space is actually compressed along the axes. Reflections When one or more of our diagonal values is negative, we get a reflection : \\begin{bmatrix} -1 & 0 \\\\\\ 0 & k\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} -x \\\\\\ ky\\end{bmatrix} \\begin{bmatrix} -1 & 0 \\\\\\ 0 & k\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} -x \\\\\\ ky\\end{bmatrix} In this case, our coordinate system is flipped across the vertical axis. When k = 2 k = 2 , visually this looks like: In another example, \\begin{bmatrix} -1 & 0 \\\\\\ 0 & -1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} -x \\\\\\ -y\\end{bmatrix} \\begin{bmatrix} -1 & 0 \\\\\\ 0 & -1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} -x \\\\\\ -y\\end{bmatrix} This transformation inverts all axes, and is known as an inversion . We can also produce mirror reflections over a straight line that crosses through the origin: \\begin{bmatrix} 0 & 1 \\\\\\ 1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} y \\\\\\ x\\end{bmatrix} \\begin{bmatrix} 0 & 1 \\\\\\ 1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} y \\\\\\ x\\end{bmatrix} or, \\begin{bmatrix} 0 & -1 \\\\\\ -1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} -y \\\\\\ -x\\end{bmatrix} \\begin{bmatrix} 0 & -1 \\\\\\ -1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} -y \\\\\\ -x\\end{bmatrix} Note Of course, we can also produce mirror transformations over the x x or y y axis as well, by making one of the diagonals 1 1 and the other -1 -1 . Shears Shears are visually similar to slanting . There are two possibilities: A shear parallel to the x x axis looks like: \\begin{bmatrix} 1 & k \\\\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x + ky \\\\\\ y\\end{bmatrix} \\begin{bmatrix} 1 & k \\\\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x + ky \\\\\\ y\\end{bmatrix} If k=1 k=1 , then visually this looks like: Or a shear parallel to the y y axis: \\begin{bmatrix} 1 & 0 \\\\\\ k & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\\\\\ kx + y\\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\\\ k & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\\\\\ kx + y\\end{bmatrix} Rotations Finally, we can rotate the space about the orign. For example, \\begin{bmatrix} 0 & -1 \\\\\\ 1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} y \\\\\\ -x\\end{bmatrix} \\begin{bmatrix} 0 & -1 \\\\\\ 1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} y \\\\\\ -x\\end{bmatrix} would rotate the entire space 90^0 90^0 counterclockwise. More generally, for a rotation by a angle \\theta \\theta clockwise about the orgin: \\begin{bmatrix} \\cos \\theta & \\sin \\theta \\\\\\ - \\sin \\theta & \\cos \\theta \\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\cos \\theta + y \\sin \\theta \\\\\\ - x \\sin \\theta + y \\cos \\theta\\end{bmatrix} \\begin{bmatrix} \\cos \\theta & \\sin \\theta \\\\\\ - \\sin \\theta & \\cos \\theta \\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\cos \\theta + y \\sin \\theta \\\\\\ - x \\sin \\theta + y \\cos \\theta\\end{bmatrix} and for a rotation by a angle \\theta \\theta counterclockwise about the orgin \\begin{bmatrix} \\cos \\theta & - \\sin \\theta \\\\\\ \\sin \\theta & \\cos \\theta \\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\cos \\theta - y \\sin \\theta \\\\\\ x \\sin \\theta + y \\cos \\theta\\end{bmatrix} \\begin{bmatrix} \\cos \\theta & - \\sin \\theta \\\\\\ \\sin \\theta & \\cos \\theta \\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x \\cos \\theta - y \\sin \\theta \\\\\\ x \\sin \\theta + y \\cos \\theta\\end{bmatrix}","title":"Types of matrix transformation"},{"location":"linear_algebra/week_3/#conclusions_1","text":"In this section, we described the major transformations that a matrix can perform on a vector. Next, we will look at how we can combine these transformations (known as composition ) to produce more complex matrix transformations","title":"Conclusions"},{"location":"linear_algebra/week_3/#composition-or-combination-of-matrix-transformations","text":"Tip Watch this video before reading through this section. So what is the point of introducing these different geometric transformations in this class? Well, if you want to do any kind of shape alteration, say of all the pixels in an image, or a face, then you can always make that shape change out of some combination of rotations , shears , stretches , and inverses . Note One example where these geometric transformations may be useful is in facial recognition , where we may preprocess every image by transforming it so that the person(s) face(s) are directly facing the camera. Lets illustrate this composition of matrix transformations with an example. Here, we will first apply a 90^o 90^o rotation clockwise about the x x -axis, and then a shear parallel to the x x -axis. Let the first transformation matrix be A_1 A_1 : \\begin{bmatrix} 0 & 1 \\\\\\ -1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} y \\\\\\ -x\\end{bmatrix} \\begin{bmatrix} 0 & 1 \\\\\\ -1 & 0\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} y \\\\\\ -x\\end{bmatrix} And our second transformation matrix A_2 A_2 : \\begin{bmatrix} 1 & 1 \\\\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x + y \\\\\\ y\\end{bmatrix} \\begin{bmatrix} 1 & 1 \\\\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} x \\\\\\ y\\end{bmatrix} = \\begin{bmatrix} x + y \\\\\\ y\\end{bmatrix} Applying A_1 A_1 to our basis vectors \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 gives us \\hat e_1' \\hat e_1' and \\hat e_1' \\hat e_1' : \\hat e_1' = A_1 \\cdot \\hat e_1 = \\begin{bmatrix} 0 \\\\\\ -1\\end{bmatrix} \\hat e_1' = A_1 \\cdot \\hat e_1 = \\begin{bmatrix} 0 \\\\\\ -1\\end{bmatrix} \\hat e_2' = A_1 \\cdot \\hat e_2 = \\begin{bmatrix} 1 \\\\\\ 0\\end{bmatrix} \\hat e_2' = A_1 \\cdot \\hat e_2 = \\begin{bmatrix} 1 \\\\\\ 0\\end{bmatrix} Applying A_2 A_2 to our new transformed vectors \\hat e_1' \\hat e_1' and \\hat e_2' \\hat e_2' gives us \\hat e_1'' \\hat e_1'' and \\hat e_1'' \\hat e_1'' : \\hat e_1'' = A_2 \\cdot \\hat e_1' = \\begin{bmatrix}-1 \\\\\\ -1\\end{bmatrix} \\hat e_1'' = A_2 \\cdot \\hat e_1' = \\begin{bmatrix}-1 \\\\\\ -1\\end{bmatrix} \\hat e_2'' = A_2 \\cdot \\hat e_2' = \\begin{bmatrix} 1 \\\\\\ 0\\end{bmatrix} \\hat e_2'' = A_2 \\cdot \\hat e_2' = \\begin{bmatrix} 1 \\\\\\ 0\\end{bmatrix} Notice that if we stack our final transformed vectors \\hat e_1'' \\hat e_1'' and \\hat e_1'' \\hat e_1'' as columns we get the matrix: \\begin{bmatrix} -1 & 1 \\\\\\ -1 & 0\\end{bmatrix} \\begin{bmatrix} -1 & 1 \\\\\\ -1 & 0\\end{bmatrix} Which is equal to A_2 \\cdot A_1 A_2 \\cdot A_1 . Geometrically, this looks like: Conclusions The take home message here is that the transformation A_2 \\cdot (A_1 \\cdot r) A_2 \\cdot (A_1 \\cdot r) for some transformation matrices A_1 A_1 and A_2 A_2 and some vector r r , is equivalent to the transformation (A_2A_1) \\cdot r (A_2A_1) \\cdot r Note Note that A_2 \\cdot (A_1 \\cdot r) \\ne A_1 \\cdot (A_2 \\cdot r) A_2 \\cdot (A_1 \\cdot r) \\ne A_1 \\cdot (A_2 \\cdot r) , that is, the order in which we apply our transformations matters . As it turns out, the key to solving simultaneous equation problems is appreciating how vectors are transformed by matrices, which is at the the heart of linear algebra.","title":"Composition or combination of matrix transformations"},{"location":"linear_algebra/week_3/#matrix-inverses","text":"In this section, we are finally going to present a way to solve the apples and bananas problem with matrices. Along the way, we're going to find out about a thing called the inverse of a matrix and a method for finding it. Tip Watch this video before reading this section. For more practice with matrix inverses, see Khan Academy sections here and here .","title":"Matrix inverses"},{"location":"linear_algebra/week_3/#gaussian-elimination-solving-the-apples-and-bananas-problem","text":"First, recall our apples and bananas problem in matrix form: \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix} 8 \\\\\\ 3\\end{bmatrix} \\begin{pmatrix}2 & 3 \\\\\\ 10 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b\\end{bmatrix} = \\begin{bmatrix} 8 \\\\\\ 3\\end{bmatrix} Where we want to find the price of individual apples ( a a ) and bananas ( b b ). Simplifying, lets say that: A\\cdot r = s A\\cdot r = s Note This, in effect is saying: \" A A operates on vector r r to give us s s \". To solve for r r , we need to move A A to the other side of the equation. But how? . Well, to \"undo\" a division (and isolate x x ), you multiply by the reciprocal: \\frac{1}{2}x = 4 \\frac{1}{2}x = 4 \\Rightarrow x = 8 \\Rightarrow x = 8 Likewise, to undo a multiplication, we divide by the reciprocal: 2x = 4 2x = 4 \\Rightarrow x = 2 \\Rightarrow x = 2 How do we undo the transformation performed by A A ? The answer is to find the matrix A^{-1} A^{-1} (known as the inverse of A A ) such that: A^{-1}A = I A^{-1}A = I where I I is the identity matrix . We call A^{-1} A^{-1} the inverse of A A because is it reverses whatever transformation A A does, giving us back I I . We note that: A^{-1}A\\cdot r = A^{-1} s A^{-1}A\\cdot r = A^{-1} s \\Rightarrow\\ I \\cdot r = A^{-1} s \\Rightarrow\\ I \\cdot r = A^{-1} s \\Rightarrow\\ r = A^{-1} s \\Rightarrow\\ r = A^{-1} s So, if we could find the inverse of A A (i.e. find A^{-1} A^{-1} ), we can solve our problem (i.e. find a a and b b ). We can solve for A^{-1} A^{-1} with a series of row operations or substitutions (known as Gaussian elimination ). Let's look at a slightly more complicated problem to see how this is done: \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 15 \\\\\\ 21 \\\\\\ 13\\end{bmatrix} \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 15 \\\\\\ 21 \\\\\\ 13\\end{bmatrix} We start by subtracting row 1 from rows 2 and 3, which gives us a matrix in row echelon form (technically, reduced row echelon form ): \\begin{pmatrix}1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 15 \\\\\\ 6 \\\\\\ 2\\end{bmatrix} \\begin{pmatrix}1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 15 \\\\\\ 6 \\\\\\ 2\\end{bmatrix} We then perform two steps of back substitution to get the identify matrix: \\begin{pmatrix}1 & 1 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 9 \\\\\\ 4 \\\\\\ 2\\end{bmatrix} \\begin{pmatrix}1 & 1 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 9 \\\\\\ 4 \\\\\\ 2\\end{bmatrix} \\begin{pmatrix}1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 5 \\\\\\ 4 \\\\\\ 2\\end{bmatrix} \\begin{pmatrix}1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{bmatrix}a \\\\\\ b \\\\\\ c\\end{bmatrix} = \\begin{bmatrix} 5 \\\\\\ 4 \\\\\\ 2\\end{bmatrix} We can then read the solution right from the matrices a = 5 a = 5 , b = 4 b = 4 , c = 2 c = 2 . Tip If you are still feeling uneasy about using Gaussian elimination to solve a system of linear equations, see here for an example walked through step-by-step example. Full credit to PatrickJMT . Conclusions As it turns out, we don't have to compute the inverse at all to solve a system of linear equations. Although we showed the process of Gaussian elimination for some vectors r r and s s , we can use it in the general case to solve for any linear equation of the form A\\cdot r = s A\\cdot r = s . This actually one of the most computationally efficient ways to solve this problem, and it's going to work every time.","title":"Gaussian elimination: Solving the apples and bananas problem"},{"location":"linear_algebra/week_3/#from-gaussian-elimination-to-finding-the-inverse-matrix","text":"Now, let's think about how we can apply this idea of elimination to find the inverse matrix, which solves the more general problem no matter what vectors I write down on the right hand side. Say I have a 3 \\times \\times 3 matrix A A and its inverse B B ( B = A^{-1}) B = A^{-1}) . By the definition of the inverse , AB = BA = I_n AB = BA = I_n If we use our matrix A A from the last section: \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = I \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = I Note Where b_{ij} b_{ij} is the element at the i^{th} i^{th} row and j^{th} j^{th} column of matrix B B . we notice that the first column of B B is just a vector. It's a vector that describes what the B B matrix, the inverse of A A , does to space. Note Actually, it's the transformation that that vector does to the x-axis. This means that: \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{pmatrix}b_{11} \\\\\\ b_{21} \\\\\\ b_{31}\\end{pmatrix} = \\begin{pmatrix} 1 \\\\\\ 0 \\\\\\ 0\\end{pmatrix} \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{pmatrix}b_{11} \\\\\\ b_{21} \\\\\\ b_{31}\\end{pmatrix} = \\begin{pmatrix} 1 \\\\\\ 0 \\\\\\ 0\\end{pmatrix} Now, we could solve this by the elimination method and back substitution in just the way we did before. Then, we could do it again for the second column of B B , and finally the third. In this way, we would have solved for B B , in other words, we would have found A^{-1} A^{-1} . It turns out, we can actually solve for B B all at once. So lets do that: \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = I \\begin{pmatrix}1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 1 & 1 & 2\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = I Subtract row 1 from row 2 and 3: \\Rightarrow \\begin{pmatrix}1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & -1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix}1 & 0 & 0 \\\\\\ -1 & 1 & 0 \\\\\\ -1 & 0 & 1\\end{pmatrix} \\Rightarrow \\begin{pmatrix}1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & -1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix}1 & 0 & 0 \\\\\\ -1 & 1 & 0 \\\\\\ -1 & 0 & 1\\end{pmatrix} Multiply row 3 by -1 -1 : \\Rightarrow \\begin{pmatrix}1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix}1 & 0 & 0 \\\\\\ -1 & 1 & 0 \\\\\\ 1 & 0 & -1\\end{pmatrix} \\Rightarrow \\begin{pmatrix}1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix}1 & 0 & 0 \\\\\\ -1 & 1 & 0 \\\\\\ 1 & 0 & -1\\end{pmatrix} Now that row 3 is in row echelon form , we can substitute it back into row 2 and row 1: \\Rightarrow \\begin{pmatrix}1 & 1 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} -2 & 0 & 3 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} \\Rightarrow \\begin{pmatrix}1 & 1 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} -2 & 0 & 3 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} And finally, back substitute row 2 into row 1: \\Rightarrow \\begin{pmatrix}1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} 0 & -1 & 2 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} \\Rightarrow \\begin{pmatrix}1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} 0 & -1 & 2 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} Because any matrix times its identity is that matrix itself: \\begin{pmatrix}1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} 0 & -1 & 2 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} \\begin{pmatrix}1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1\\end{pmatrix} \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} 0 & -1 & 2 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} \\Rightarrow \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} 0 & -1 & 2 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} \\Rightarrow \\begin{pmatrix}b_{11} & b_{12} & b_{13} \\\\\\ b_{21} & b_{22} & b_{23} \\\\\\ b_{31} & b_{32} & b_{33}\\end{pmatrix} = \\begin{pmatrix} 0 & -1 & 2 \\\\\\ -2 & 1 & 1 \\\\\\ 1 & 0 & -1\\end{pmatrix} Note You can actually prove to yourself that we got the right answer by checking that A \\cdot B = I A \\cdot B = I . So that's our answer. We've found the identity matrix, B = A^{-1} B = A^{-1} for A A , and we did this by transforming A A into it's identity matrix via elimination and back substitution. Moreover, because B B could be any matrix, we have solved this in the general case. The solution is the same regardless of the number of dimensions, and this leads to a computationally efficient way to invert a matrix. Note There are computationally faster methods of computing the inverse, one such method is known as a decomposition process . In practice, you will simply call the solver of your problem or function, something like inv(A) , and it will pick the best method by inspecting the matrix you give it and return the answer. Conclusions We have figured out how to solve sets of linear equations in the general case, by a procedure we can implement in a computer really easily (known as Gaussian elimination ), and we've generalized this method to the to find the inverse of a matrix, regardless of what is on the right hand side of our system of equations.","title":"From Gaussian elimination to finding the inverse matrix"},{"location":"linear_algebra/week_3/#special-matrices","text":"In the final section of this week, we're going to look at a property of a matrix called the determinant . Note The determinant is a value that can be computed from the elements of a square matrix . The determinant of a matrix A A is denoted det(A) det(A) , det A det A , or \\vert A \\vert \\vert A \\vert . Geometrically, it can be viewed as the scaling factor of the linear transformation described by the matrix. We'll also look at what happens when a matrix doesn't have linearly independent basis vectors. Tip Watch this video before reading this section. For more practice with matrix inverses, see Khan Academy sections here and here .","title":"Special matrices"},{"location":"linear_algebra/week_3/#the-determinant","text":"Let's start by looking at the simple matrix: \\begin{pmatrix} a & 0 \\\\\\ 0 & d\\end{pmatrix} \\begin{pmatrix} a & 0 \\\\\\ 0 & d\\end{pmatrix} If we multiply this matrix by our basis vectors \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 , we get \\hat e_1' = \\begin{bmatrix} a \\\\\\ 0 \\end{bmatrix} \\hat e_1' = \\begin{bmatrix} a \\\\\\ 0 \\end{bmatrix} \\hat e_2' = \\begin{bmatrix} 0 \\\\\\ d \\end{bmatrix} \\hat e_2' = \\begin{bmatrix} 0 \\\\\\ d \\end{bmatrix} So, in plain english, we have stretched our x x -axis by a factor of a a and our y y -axis by a factor of d d and, therefore, scaled the area of the grid cells of the vector space by a factor of ad ad . Note This is a property of the fact that any linear transformation keeps grid lines parallel and evenly spaced. We call this number, ad ad the determinant . Now, if I instead have a matrix: \\begin{pmatrix} a & b \\\\\\ 0 & d\\end{pmatrix} \\begin{pmatrix} a & b \\\\\\ 0 & d\\end{pmatrix} then this is still going to stretch \\hat e_1 \\hat e_1 out by a factor of a a , but on the other axis, I am going to move \\hat e_2 \\hat e_2 hat to: \\hat e_2 = \\begin{pmatrix} b \\\\\\ d\\end{pmatrix} \\hat e_2 = \\begin{pmatrix} b \\\\\\ d\\end{pmatrix} What we have done is taken the original grid and stretched it along the x x -axis by a a and, along the y y -axis by d d and sheared it (parallel to the x x -axis) by b b . Note We've still changed the size, the scale of the space (which is what the determinant really is) by a factor of ad ad . Notice that the area defined by our transformed vectors \\hat e_1' \\hat e_1' and \\hat e_2' \\hat e_2' is still just the base times the perpendicular height, ad ad (the determinant). Lets flesh this out in the general case. Say we have the matrix: A = \\begin{pmatrix} a & b \\\\\\ c & d\\end{pmatrix} A = \\begin{pmatrix} a & b \\\\\\ c & d\\end{pmatrix} Multiplying this matrix by our basis vectors yields a parallelogram (stretched by a a and d d and sheared by b b and c c ). We can actual compute the area of this parallelogram as follows: Note We find the area of this parallelogram by finding the area of the whole box the encloses it, and subtracting off combined area of the the little bits around it. The exact method for solving the area is not important (although it is pretty trivial). What is important, is that the determinant of A A can be computed as \\vert A \\vert = ad - bc \\vert A \\vert = ad - bc , and that this computation has a geometric interpretation . Note This is the formula for the determinant of a square 2 \\times 2 2 \\times 2 matrix. See here for the formula for higher dimensional matrices. Now, in school when you looked at matrices, you probably saw that you could find the inverse in the following way. For a matrix: A = \\begin{pmatrix} a & b \\\\\\ c & d\\end{pmatrix} A = \\begin{pmatrix} a & b \\\\\\ c & d\\end{pmatrix} Exchange a a and the d d , and switch the sign on the b b and the c c Multiply A A by this matrix Scale the transformation by \\frac{1}{ad - bc} \\frac{1}{ad - bc} In the general case, this looks like: \\frac{1}{ad - bc} \\begin{pmatrix} a & b \\\\\\ c & d\\end{pmatrix} \\begin{pmatrix} d & -b \\\\\\ -c & a\\end{pmatrix} = \\frac{1}{ad - bc} \\begin{pmatrix} ad - bc & 0 \\\\\\ 0 & ad - bc\\end{pmatrix} = I \\frac{1}{ad - bc} \\begin{pmatrix} a & b \\\\\\ c & d\\end{pmatrix} \\begin{pmatrix} d & -b \\\\\\ -c & a\\end{pmatrix} = \\frac{1}{ad - bc} \\begin{pmatrix} ad - bc & 0 \\\\\\ 0 & ad - bc\\end{pmatrix} = I This demonstrates that \\frac{1}{ad - bc} \\begin{pmatrix} d & -b \\\\\\ -c & a\\end{pmatrix} \\frac{1}{ad - bc} \\begin{pmatrix} d & -b \\\\\\ -c & a\\end{pmatrix} is in fact the inverse of the matrix A A . This helps capture what the determinant really is. It's the amount by which the original matrix scaled vector space. In the above example, dividing by the determinant normalizes the space back to its original size. Note We could spend a lot of time talking about how to solve for the determinant. However, knowing how to do the operations isn't really a useful skill. Many programming libraries (e.g. python ) have linear algebra libraries (e.g. Numpy ) which makes computing the determinant as easy, for example, as calling det(A) . If you really want to know how to compute determinants by hand, then look up a QR decomposition online.","title":"The determinant"},{"location":"linear_algebra/week_3/#a-determinant-of-zero","text":"Now, let's think about the matrix A = \\begin{pmatrix} 1 & 2 \\\\\\ 1 & 2\\end{pmatrix} A = \\begin{pmatrix} 1 & 2 \\\\\\ 1 & 2\\end{pmatrix} If we multiply this matrix by our basis vectors \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 , we get \\hat e_1' = \\begin{bmatrix} 1 \\\\\\ 1 \\end{bmatrix} \\hat e_1' = \\begin{bmatrix} 1 \\\\\\ 1 \\end{bmatrix} \\hat e_2' = \\begin{bmatrix} 2 \\\\\\ 2 \\end{bmatrix} \\hat e_2' = \\begin{bmatrix} 2 \\\\\\ 2 \\end{bmatrix} So this matrix, when applied to our vector space, actually collapses it onto a line . All our y y 's are mapped to the vector \\begin{bmatrix}2 \\\\\\ 2 \\end{bmatrix} \\begin{bmatrix}2 \\\\\\ 2 \\end{bmatrix} and our x x 's to \\begin{bmatrix}1 \\\\\\ 1 \\end{bmatrix} \\begin{bmatrix}1 \\\\\\ 1 \\end{bmatrix} Correspondingly, we notice that \\vert A \\vert = 0 \\vert A \\vert = 0 . Therefore, the area enclosed by the new basis vectors is zero .","title":"A determinant of zero"},{"location":"linear_algebra/week_3/#a-negative-determinant","text":"A negative determinant simply means that the transformation has flipped the orientation of our vector space. This is much easier to see than to explain ; check out this video which presents some awesome visualizations of the determinant.","title":"A negative determinant"},{"location":"linear_algebra/week_3/#what-the-determinant-means-numerically","text":"To drive home the numerical interpretation of the determinant, lets start with this set of simultaneous equations: \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 2 & 3 & 7\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 17 \\\\\\ 29\\end{pmatrix} \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 2 & 3 & 7\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 17 \\\\\\ 29\\end{pmatrix} You'll notice that both the rows and the columns are linearly dependent . Thinking about the columns of this matrix as the basis vectors of some 3-dimensional space, we note that this transformation collapses our vector space from being 3D to 2D (by collapsing every point in space onto a plane). Let's see what this means numerically by trying to reduce our matrix to row-echelon form: \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 2 & 3 & 7\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 17 \\\\\\ 29\\end{pmatrix} \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 1 & 2 & 4 \\\\\\ 2 & 3 & 7\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 17 \\\\\\ 29\\end{pmatrix} Subtract row 1 from row 2, \\Rightarrow \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 2 & 3 & 7\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 5 \\\\\\ 29\\end{pmatrix} \\Rightarrow \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 2 & 3 & 7\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 5 \\\\\\ 29\\end{pmatrix} Subtract row 1 plus row 2 from row 3, \\Rightarrow \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & 0\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 5 \\\\\\ 0\\end{pmatrix} \\Rightarrow \\begin{pmatrix} 1 & 1 & 3 \\\\\\ 0 & 1 & 1 \\\\\\ 0 & 0 & 0\\end{pmatrix} \\begin{pmatrix} a \\\\\\ b \\\\\\ c\\end{pmatrix} = \\begin{pmatrix} 12 \\\\\\ 5 \\\\\\ 0\\end{pmatrix} while the matrix is now is row-echelon form, we don't have a final entry of the matrix ( 0\\cdot c = 0 0\\cdot c = 0 ). Because we don't have a solution for c c we can't back substitute, and we can't solve our system of equations. Note The reason we can't solve this system is because we don't have enough information. In keeping with our apples and bananas problem, imagine that when we went in to buy apples and bananas and carrots the third time, we ordered just a the sum of first two orders. Therefore, we didn't get any new information and thus don't have enough data to find out the solution for how much apples and bananas and carrots cost. This third order wasn't linearly independent from our first two, in the language of matrices and vectors. So, when the basis vectors describing a matrix aren't linearly independent, then the determinant is zero , and we can't solve the system. The loss of information when we map from n n -dimensional to (n-x) (n-x) -dimensional space (where x \\ge 1 x \\ge 1 ) means we cannot possibly know what the inverse matrix is (as it is impossible to map a lower dimensional space back to the original, higher dimensional space). When a matrix has no inverse, we say that it is singular . Note There are situations where we might want to do a transformation that collapses the number of dimensions in a space, but it means that we cannot possibly reverse the mapping, meaning the matrix has no inverse. This also means we cannot solve a system of linear equations defined by a singular matrix using Gaussian elimination and back substitution.","title":"What the determinant means numerically"},{"location":"linear_algebra/week_3/#summary","text":"In the last section of this week, we took a look at the determinant, which is how much a given transformation scales our space. In 2-dimensions, this can be thought as the scalar multiple appleid to any area of our space, and in 3-dimensions any volume of our space. We also looked at the special case where the determinant is zero and found that this means that the basis vectors aren't linearly independent, which in turn means that the inverse doesn't exist. To summarize Week 3 , we introduced matrices as objects that transforms space. looked at different archetypes of matrices, like rotations , inverses , stretches , and shears , how to combine matrices by doing successive transformations, known as m atrix multiplication or composition how to solve systems of linear equations by elimination and how to find inverses and finally, we introduced determinants and showed how that relates to the concept of linear independence .","title":"Summary"},{"location":"linear_algebra/week_4/","text":"Week 4: Matrices Make Linear Mappings In Module 4, we continue our discussion of matrices; first we think about how to code up matrix multiplication and matrix operations using the Einstein Summation Convention, which is a widely used notation in more advanced linear algebra courses. Then, we look at how matrices can transform a description of a vector from one basis (set of axes) to another. This will allow us to, for example, manipulate images. We'll also look at how to construct a convenient basis vector set in order to do such transformations. Then, we'll write some code to do these transformations and apply this work computationally. Learning Objectives Identify matrices as operators Relate the transformation matrix to a set of new basis vectors Formulate code for mappings based on these transformation matrices Write code to find an orthonormal basis set computationally Matrices as objects that map one vector onto another Introduction to Einstein summation convention and the symmetry of the dot product There is a different, important way to write matrix transformations that we have not yet discussed. It's called the Einstein's Summation Convention . In this convention, we write down the actual operations on the elements of a matrix, which is useful when you're coding or programming. It also lets us see something neat about the dot product, and it lets us deal with non-square matrices. When we started, we said that multiplying a matrix by a vector or with another matrix is a process of taking every element in each row of the first matrix multiplied with the corresponding element in each column of the other matrix, summing the products and putting them in place. In Einstein's Summation Convention, we represent the matrix product C = AB C = AB : \\begin{pmatrix} a_{11} & a_{12} & . & . & . & a_{1n} \\\\\\ a_{21} & a_{22} & & & & . &\\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ a_{n1} & . & . & . & . & a_{nm}\\end{pmatrix} \\begin{pmatrix} b_{11} & b_{12} & . & . & . & b_{1m} \\\\\\ b_{21} & b_{22} & & & & . &\\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ b_{m1} & . & . & . & . & b_{mp}\\end{pmatrix} = AB \\begin{pmatrix} a_{11} & a_{12} & . & . & . & a_{1n} \\\\\\ a_{21} & a_{22} & & & & . &\\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ a_{n1} & . & . & . & . & a_{nm}\\end{pmatrix} \\begin{pmatrix} b_{11} & b_{12} & . & . & . & b_{1m} \\\\\\ b_{21} & b_{22} & & & & . &\\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ b_{m1} & . & . & . & . & b_{mp}\\end{pmatrix} = AB as, c_{ij} = a_{i1}b_{1j} + ... + a_{im}b_{mj} = \\sum_{k=1}^m a_{ik}b_{kj} c_{ij} = a_{i1}b_{1j} + ... + a_{im}b_{mj} = \\sum_{k=1}^m a_{ik}b_{kj} Note To be clear, its the c_{ij} c_{ij} itself that is written Einstein's Summation Convention, not everything that comes to the right of the = = sign. For i = 1, ..., n i = 1, ..., n and j = 1, ..., p j = 1, ..., p , where A A is a n \\times m n \\times m matrix, B B is a m \\times p m \\times p matrix, and C C is a n \\times p n \\times p matrix. This is useful when we are implementing matrix multiplication in code, because it makes it obvious exactly what operations we need to perform. In this case, it should be obvious that we can run three loops over i i , j j and k k , and then use an accumulator on the k k 's to find the elements of the product matrix AB AB . Note We haven't talked about this yet, but now we can see it clearly. There's no reason, so long as the matrices have the same number of entries in k k , that the matrices we multiply need to be the same shape! Let's revisit the dot product in light of the Einstein Summation Convention. If we've got two vectors, let's call them u u and v v , where u u is a column vector having elements u_i u_i and v v is another column vector having elements v_i v_i . \\begin{pmatrix}u_i \\\\\\ \\vdots \\\\\\ u_{n}\\end{pmatrix} \\cdot \\begin{pmatrix}v_i \\\\\\ \\vdots \\\\\\ v_n \\end{pmatrix} \\begin{pmatrix}u_i \\\\\\ \\vdots \\\\\\ u_{n}\\end{pmatrix} \\cdot \\begin{pmatrix}v_i \\\\\\ \\vdots \\\\\\ v_n \\end{pmatrix} When we dot them together, we are computing the following: u \\cdot v = \\sum^n_{i=1}u_iv_i = u_1v_1 + u_2v_2 + ... + u_nv_n u \\cdot v = \\sum^n_{i=1}u_iv_i = u_1v_1 + u_2v_2 + ... + u_nv_n Notice that, \\begin{pmatrix}u_i \\\\\\ \\vdots \\\\\\ u_{n}\\end{pmatrix} \\cdot \\begin{pmatrix}v_i \\\\\\ \\vdots \\\\\\ v_n \\end{pmatrix} = \\begin{pmatrix}u_i & \\cdots & u_{n}\\end{pmatrix} \\cdot \\begin{pmatrix}v_i \\\\\\ \\vdots \\\\\\ v_n \\end{pmatrix} \\begin{pmatrix}u_i \\\\\\ \\vdots \\\\\\ u_{n}\\end{pmatrix} \\cdot \\begin{pmatrix}v_i \\\\\\ \\vdots \\\\\\ v_n \\end{pmatrix} = \\begin{pmatrix}u_i & \\cdots & u_{n}\\end{pmatrix} \\cdot \\begin{pmatrix}v_i \\\\\\ \\vdots \\\\\\ v_n \\end{pmatrix} and so we notice that there's some equivalence between a matrix transformation (or multiplication ) and the dot product . Lets explore this in more detail. Symmetry of the dot product Say we have the vector \\hat u \\hat u , with components u_1 u_1 and u_2 u_2 . Let's imagine what happens if we dot \\hat u \\hat u with the basis vector \\hat e_1 \\hat e_1 . We know from previous sections that this gives us the length of the projection of \\hat u_1 \\hat u_1 on \\hat e_1 \\hat e_1 multiplied by the norm of \\hat e_1 \\hat e_1 . But what if we do the reverse? What if we dot \\hat e_1 \\hat e_1 with \\hat u \\hat u ? We already know that numerically, the result will be the same, as the dot product is commutative . So geometrically, we can imagine drawing a line of symmetry between the point where the two projections cross: Previously, we stated (without proof, although the numerical proof is trivial) that the dot product is commutative, and now, we have shown geometrically why that is true. Conclusions In this section, we introduced Einstein's Summation Convention , which is a compact and computationally useful (but not very visually intuitive) way to write down matrix operations. This led to a discussion on the similarities between the dot product and matrix multiplication, where we noticed a connection between matrix multiplication , and the dot product , which itself has a geometric understanding as the concept of projection , i.e. projecting one vector onto another. This allows us to think about matrix multiplication with a vector as being the projection of that vector onto the vectors composing the matrix (i.e. the columns of the matrix). Matrices transform into the new basis vector set Tip Watch this video before reading this section. For more practice with changing basis, see this Khan Academy section. Matrices changing basis We have said before that the columns of a transformation matrix are the axes of the new basis vectors after applying the mapping. We're now going to spend a little time looking at how to transform a vector from one set of basis vectors to another . Let's say we have two sets of basis vectors, which define a first coordinate system ( \\text{CS}_1) \\text{CS}_1) and a second coordinate system ( \\text{CS}_2) \\text{CS}_2) . Let the basis vectors of \\text{CS}_2 \\text{CS}_2 , from the perspective of \\text{CS}_1 \\text{CS}_1 be: \\hat b_1 = \\begin{bmatrix}3 \\\\\\ 1\\end{bmatrix}, \\hat b_2 = \\begin{bmatrix}1 \\\\\\ 1\\end{bmatrix} \\hat b_1 = \\begin{bmatrix}3 \\\\\\ 1\\end{bmatrix}, \\hat b_2 = \\begin{bmatrix}1 \\\\\\ 1\\end{bmatrix} Lets package these basis vectors into a matrix \\text{CS}_{21} \\text{CS}_{21} , for convenience \\text{CS}_{21} = \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\text{CS}_{21} = \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} Think of these as the basis vectors of \\text{CS}_2 \\text{CS}_2 as they would appear in \\text{CS}_1 \\text{CS}_1 . If we wanted to change the basis of any vectors in \\text{CS}_2 \\text{CS}_2 to \\text{CS}_1 \\text{CS}_1 , we simply do: \\text{CS}_{21} \\cdot \\text{vector in CS}_2 = \\text{vector in CS}_1 \\text{CS}_{21} \\cdot \\text{vector in CS}_2 = \\text{vector in CS}_1 E.g., for the vector \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} defined in terms of \\text{CS}_2 \\text{CS}_2 \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} = \\begin{bmatrix}5 \\\\\\ 2\\end{bmatrix} \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} = \\begin{bmatrix}5 \\\\\\ 2\\end{bmatrix} That is, a vector \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} described in \\text{CS}_2 \\text{CS}_2 , is described as \\begin{bmatrix}5 \\\\\\ 2\\end{bmatrix} \\begin{bmatrix}5 \\\\\\ 2\\end{bmatrix} in \\text{CS}_1 \\text{CS}_1 . Why does this make sense? Well, you can think of \\text{CS}_{21} \\text{CS}_{21} as the transformation that takes the basis vectors of \\text{CS}_1 \\text{CS}_1 and moves them to the positions of the basis vectors of \\text{CS}_2 \\text{CS}_2 . Applying this transformation to a vector in \\text{CS}_2 \\text{CS}_2 , therefore, gives us the corresponding vector in \\text{CS}_1 \\text{CS}_1 . Now, how do we do the reverse? How do we translate a vector in \\text{CS}_1 \\text{CS}_1 to a vector in \\text{CS}_2 \\text{CS}_2 ? All we need to do to change basis in the reverse case is to multiply a vector in one coordinate system by the inverse of the matrix containing the basis vectors of another: \\text{CS}^{-1}_{21} \\cdot \\text{vector in CS}_1 = \\text{vector in CS}_2 \\text{CS}^{-1}_{21} \\cdot \\text{vector in CS}_1 = \\text{vector in CS}_2 E.g., \\frac{1}{2}\\begin{bmatrix}1 & -1 \\\\\\ -1 & 3\\end{bmatrix}\\begin{bmatrix}5 \\\\\\ 2\\end{bmatrix} = \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} \\frac{1}{2}\\begin{bmatrix}1 & -1 \\\\\\ -1 & 3\\end{bmatrix}\\begin{bmatrix}5 \\\\\\ 2\\end{bmatrix} = \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} Notice that this process gave us the coordinates of the vector \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} in \\text{CS}_2 \\text{CS}_2 , which is equal to the vector that we started with. Generalizing This is a little bit tricky, so lets generalize. Imagine we have two coordinate systems, \"ours\" and \"theirs\". We can translate vectors in \"their\" coordinate system to \"our\" coordinate system by applying a transformation A A , where A A is a matrix whose columns contain the basis vectors of \"their\" coordinate system as they appear in \"our coordinate system\": \\text{(1) } A \\cdot \\text{vector in \"their\" coordinate system} = \\text{vector in \"our\" coordinate system} \\text{(1) } A \\cdot \\text{vector in \"their\" coordinate system} = \\text{vector in \"our\" coordinate system} To do the reverse, i.e. take a vector in \"our\" coordinate system and translate it to \"their\" coordinate system, we simply multiply \"our\" vector by the inverse of A \\text{(2) } \\text{vector in \"their\" coordinate system} = A^{-1} \\cdot \\text{vector in \"our\" coordinate system} \\text{(2) } \\text{vector in \"their\" coordinate system} = A^{-1} \\cdot \\text{vector in \"our\" coordinate system} It should be obvious now why this is the case, we simply moved A A over in the (1) to get (2)! Orthonormal basis set When we discussed vectors and projections, we said that if the new basis vectors were orthogonal then we could use projection to easily change basis. Note see Changing basis for a fleshed out example. Summary Not orthogonal, use matrix multiplication. Orthogonal, use projection product. Doing a transformation in a changed basis Tip Watch the last little bit of this video first. Lets discuss the process of applying a transformation in a changed basis. Say again that the basis vectors of coordinate system \\text{CS}_2 \\text{CS}_2 from the perspective of coordinate system \\text{CS}_1 \\text{CS}_1 are: \\text{CS}_{21} = \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\text{CS}_{21} = \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} Say further that we have a vector in \\text{CS}_2 \\text{CS}_2 that we want to transform: {c_2} = \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} {c_2} = \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} And the tranformation we want to apply is: N = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\\\ 1 & 1\\end{bmatrix} N = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\\\ 1 & 1\\end{bmatrix} Note This rotates the vector space 45^0 45^0 counter-clockwise. How do we apply the transformation N N to a vector defined by the coordinate system \\text{CS}_2 \\text{CS}_2 ? The first thing to do is take the vector {c_2} {c_2} and multiply it by \\text{CS}_{21} \\text{CS}_{21} , that is, change the basis of the vector {c_2} {c_2} from \\text{CS}_2 \\text{CS}_2 to \\text{CS}_1 \\text{CS}_1 : {c_1} = \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} {c_1} = \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} Then, we can apply the transformation: {c_1}' = R \\cdot \\text{CS}_{21} \\cdot {c_2} {c_1}' = R \\cdot \\text{CS}_{21} \\cdot {c_2} {c_1}' = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} {c_1}' = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}2 & 0 \\\\\\ 4 & 2\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}2 & 0 \\\\\\ 4 & 2\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} What if, once we obtained this output vector, we wanted to change its basis back to \\text{CS}_2 \\text{CS}_2 ? Recall from the last section that we multiply the whole thing by the inverse of \\text{CS}_{21}^{-1} \\text{CS}_{21}^{-1} {c_2}' = \\text{CS}_{21}^{-1} \\cdot R \\cdot \\text{CS}_{21} \\cdot {c_2} {c_2}' = \\text{CS}_{21}^{-1} \\cdot R \\cdot \\text{CS}_{21} \\cdot {c_2} {c_2}' = \\frac{1}{2} \\begin{bmatrix}1 & -1 \\\\\\ -1 & 3\\end{bmatrix} \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} {c_2}' = \\frac{1}{2} \\begin{bmatrix}1 & -1 \\\\\\ -1 & 3\\end{bmatrix} \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} = \\frac{1}{2} \\begin{bmatrix}1 & -1 \\\\\\ -1 & 3\\end{bmatrix}\\frac{1}{\\sqrt{2}}\\begin{bmatrix}2 & 0 \\\\\\ 4 & 2\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} = \\frac{1}{2} \\begin{bmatrix}1 & -1 \\\\\\ -1 & 3\\end{bmatrix}\\frac{1}{\\sqrt{2}}\\begin{bmatrix}2 & 0 \\\\\\ 4 & 2\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}-1 & -1 \\\\\\ 5 & 3\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}-1 & -1 \\\\\\ 5 & 3\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} This operation essentially builds on the previous operation to return the transformed output vector, {c_1}' {c_1}' relative to \\text{CS}_2 \\text{CS}_2 , that is it returns {c_2}' {c_2}' , where {c_2}' {c_2}' is where c_2 c_2 ends up after in the basis \\text{CS}_2 \\text{CS}_2 after some transformation M M has been applied. Generalizing Again, this is tricky, so lets generalize. Imagine we have two coordinate systems, \"ours\" and \"theirs\". If we are given: a vector with its basis in \"their\" coordinate system A A , a matrix whose columns are the basis vectors of \"their\" coordinate system as they appear in \"our\" coordinate system M M some transformation, with its basis in \"our\" coordinate system Then, Change the vector in \"their\" basis to \"ours\": A \\cdot \\text{vector in \"their\" coordinate system} A \\cdot \\text{vector in \"their\" coordinate system} Apply the transformation in \"our\" coordinate system: M \\cdot A \\cdot \\text{vector in \"their\" coordinate system} M \\cdot A \\cdot \\text{vector in \"their\" coordinate system} Change the resulting vector back to \"their\" coordinate system: A^{-1} \\cdot M \\cdot A \\cdot \\text{vector in \"their\" coordinate system} A^{-1} \\cdot M \\cdot A \\cdot \\text{vector in \"their\" coordinate system} In sum, the transformation A^{-1}MA A^{-1}MA will take any vector in \"their\" coordinate system, apply some transformation in \"our\" coordinate system, and return the resulting vector in \"their\" coordinate system. Making multiple mappings, deciding if these are reversible Orthogonal matrices It is very useful to compose a transformation matrix whose column and row vectors make up a new basis , with the additional constraint of making all of these component vectors orthogonal. Such a square matrix of orthonormal columns and rows is known as an orthogonal matrix . In this section we are going to look at how to construct such a matrix, and why it's useful. (Aside) Transpose First, we need to define a new matrix operation called the transpose . The transpose of a matrix is an operator which flips a matrix over its diagonal , that is it switches the row and column indices of the matrix by producing another matrix denoted as A^T A^T . It is achieved by any one of the following equivalent actions: reflect A A over its main diagonal (which runs from top-left to bottom-right) to obtain A^T A^T , write the rows of A A as the columns of A^T A^T , write the columns of A A as the rows of A^T A^T . Formally, the i -th row, j -th column element of A^T A^T is the j -th row, i -th column element of A A : [A^T]_{ij} = [A]_{ji} [A^T]_{ij} = [A]_{ji} Now let's imagine I have a n \\times n n \\times n matrix A A , with a series of column vectors which are going to be the basis vectors of the some new transformed vector space: A = \\begin{pmatrix} \\begin{pmatrix}\\vdots \\\\\\ \\hat a_1 \\\\\\ \\vdots\\end{pmatrix} & \\begin{pmatrix} \\vdots \\\\\\ \\hat a_2\\\\\\ \\vdots \\end{pmatrix} & \\dots \\begin{pmatrix} \\vdots \\\\\\ \\hat a_n\\\\\\ \\vdots \\end{pmatrix} \\end{pmatrix} A = \\begin{pmatrix} \\begin{pmatrix}\\vdots \\\\\\ \\hat a_1 \\\\\\ \\vdots\\end{pmatrix} & \\begin{pmatrix} \\vdots \\\\\\ \\hat a_2\\\\\\ \\vdots \\end{pmatrix} & \\dots \\begin{pmatrix} \\vdots \\\\\\ \\hat a_n\\\\\\ \\vdots \\end{pmatrix} \\end{pmatrix} Lets place two more constaints on this matrix A A : First, the column vectors a_i a_i have unit length Second, the column vectors a_i a_i are orthogonal to each other. Note That is, \\hat a_i \\cdot \\hat a_j = 0 \\hat a_i \\cdot \\hat a_j = 0 for \\forall i \\ne j \\forall i \\ne j and \\hat a_i \\cdot \\hat a_i = 1 \\hat a_i \\cdot \\hat a_i = 1 . Lets think about what happens when we multiply A A by its transpose, A^T A^T : A^TA = \\begin{pmatrix} \\begin{pmatrix}\\dots & \\hat a_1 & \\dots\\end{pmatrix} \\\\\\ \\begin{pmatrix} \\dots & \\hat a_2 & \\dots \\end{pmatrix} \\\\\\ \\vdots \\\\\\ \\begin{pmatrix} \\dots & \\hat a_n & \\dots \\end{pmatrix} \\end{pmatrix} \\begin{pmatrix} \\begin{pmatrix}\\vdots \\\\\\ \\hat a_1 \\\\\\ \\vdots\\end{pmatrix} & \\begin{pmatrix} \\vdots \\\\\\ \\hat a_2\\\\\\ \\vdots \\end{pmatrix} & \\dots \\begin{pmatrix} \\vdots \\\\\\ \\hat a_n\\\\\\ \\vdots \\end{pmatrix} \\end{pmatrix} = I_n A^TA = \\begin{pmatrix} \\begin{pmatrix}\\dots & \\hat a_1 & \\dots\\end{pmatrix} \\\\\\ \\begin{pmatrix} \\dots & \\hat a_2 & \\dots \\end{pmatrix} \\\\\\ \\vdots \\\\\\ \\begin{pmatrix} \\dots & \\hat a_n & \\dots \\end{pmatrix} \\end{pmatrix} \\begin{pmatrix} \\begin{pmatrix}\\vdots \\\\\\ \\hat a_1 \\\\\\ \\vdots\\end{pmatrix} & \\begin{pmatrix} \\vdots \\\\\\ \\hat a_2\\\\\\ \\vdots \\end{pmatrix} & \\dots \\begin{pmatrix} \\vdots \\\\\\ \\hat a_n\\\\\\ \\vdots \\end{pmatrix} \\end{pmatrix} = I_n So what we notice is that in the case where A A is composed of vectors that are normal to each other and have unit length , (i.e. when they're orthonormal ), then A^TA = I A^TA = I . Stated another way, A^T A^T in this situation is actually the inverse of A A ! This special case is known as an orthogonal matrix . Another thing to note is that because all the basis vectors are of unit length, it must scale space by a factor of one . Stated another way, the determinant of an orthogonal matrix must be either plus or minus one. \\vert A \\vert = \\pm 1 \\vert A \\vert = \\pm 1 Where the minus one arises if the new basis vector set flip space around (from right-handed to left-handed or vice versa). Notice that if A^T A^T , the inverse of A A , then by the definition of the inverse: A^TA = AA^T = I_n A^TA = AA^T = I_n So, we could pre- or post- multiply and still get the identity. This means that the rows of the orthogonal matrix are also orthonormal to each other! So, the transpose matrix of an orthogonal basis set, is itself another orthogonal basis set. Now, remember that in the last module on vectors, we said that transforming a vector onto a new coordinate system was as easy as taking the projection or dot product of that vector onto each of the new bases vectors, as long as they were orthogonal to each other . So, if we have a vector r r and we want to project r r into a new set of axes, let's call them \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 , as long as these vectors are orthogonal to each other, then we can project into the new vector space just by taking the dot product of r r with \\hat e_2 \\hat e_2 , and the dot product of r r with \\hat e_1 \\hat e_1 , and then we'd have its components in the new set of axis. Note See Changing basis for a walked-through example. Conclusions In this section, we introduced the most convenient basis vector set of all, the orthonormal bases vector set . Wherever possible we want to use an orthonormal basis vector set represented as a orthogonal matrix A A . This set of vectors has the following properties and consequences that make it easy to work with: the transpose of such a matrix will be its inverse , which makes the inverse incredibly easy to compute, the transformation will be reversible because space doesn't get collapsed by any dimensions, the projections (i.e. the result of computing the projection of a vector onto the matrix A A ) are just the dot products. One final note. If we arrange the bases vectors in the correct order, then the determinant will be one . \\vert A \\vert = 1 \\vert A \\vert = 1 An easy way to check if they aren't in the right order, is to check if the determinant is minus one. This means we've transformed our space from right to left handed orientation. All we have to do to remedy this is to exchange a pair of vectors in A A such that \\vert A \\vert = 1 \\vert A \\vert = 1 . Recognizing mapping matrices and applying these to data The Gram\u2013Schmidt process In the last section, we motivated the idea that life is much easier if we can construct an orthonormal basis vector set, but we haven't talked about how to do it. In this section, we will explore just that. We'll start from the assumption that we already have some linearly independent vectors that span the space we're interested in. Say we have some such vectors V = \\{v_1, v_2, ..., v_n\\} V = \\{v_1, v_2, ..., v_n\\} , Note If you want to check linear independence, you can write down your vectors as the the columns in a matrix and check that the determinant of that matrix isn't zero. but they aren't orthogonal to each other or of unit length. Our life would probably be easier if we could construct some orthonormal basis. As it turns out, there's a process for doing just that which is called the Gram-Schmidt process . Let's take the first vector in the set to be v_1 v_1 . In this first step, we're just going to normalize v_1 v_1 to get our eventual first basis vector e_1 e_1 e_1 = \\frac{v_1}{\\vert v_1 \\vert} e_1 = \\frac{v_1}{\\vert v_1 \\vert} Now, we can think of v_2 v_2 as being composed of two things: a component that is in the direction of e_1 e_1 and a component that's perpendicular to e_1 e_1 . We can find the component that's in the direction of e_1 e_1 by taking the vector projection v_2 v_2 onto e_1 e_1 : v_2 = (v_2 \\cdot e_1) \\frac{e_1}{\\vert e_1 \\vert} v_2 = (v_2 \\cdot e_1) \\frac{e_1}{\\vert e_1 \\vert} Note |e_1| |e_1| is 1 so we could actually omit it. If we subtract this vector projection from v_2 v_2 we get u_2 u_2 , a vector which is orthogonal to e_1 e_1 : u_2 = v_2 - (v_2 \\cdot e_1) e_1 u_2 = v_2 - (v_2 \\cdot e_1) e_1 Finally, dividing u_2 u_2 by its length gives us e_2 e_2 , the unit vector orthogonal to e_1 e_1 : e_2 = \\frac{u_2}{\\vert u_2 \\vert} e_2 = \\frac{u_2}{\\vert u_2 \\vert} We could continue this process for all vectors in our set V V . The general formula (in pseudocode) is: # For all vectors in our set V for i in | V | : # For all vectors in our set V that come before v_i for j in i : # Subtract the component of v_i in the direction of the previous vectors v_j v_i = v_i - v_i . dot ( v_j ) * v_j # If |v_i| is not zero, normalize it to unit length. Otherwise it is linearly dependent on a # previous vector, so set it equal to the zero vector. if | v_i | !!! note 0 : v_i = v_i / | v_i | else : v_i = zero_vector Conclusions So that's how we construct an orthonormal basis set, which makes our lives much easier for all the reasons we discussed here . Reflecting in a plane This is a rather involved example, and is probably best if you just watch it yourself here . If I can find the time, i'll make notes for the video!","title":"Week 4"},{"location":"linear_algebra/week_4/#week-4-matrices-make-linear-mappings","text":"In Module 4, we continue our discussion of matrices; first we think about how to code up matrix multiplication and matrix operations using the Einstein Summation Convention, which is a widely used notation in more advanced linear algebra courses. Then, we look at how matrices can transform a description of a vector from one basis (set of axes) to another. This will allow us to, for example, manipulate images. We'll also look at how to construct a convenient basis vector set in order to do such transformations. Then, we'll write some code to do these transformations and apply this work computationally. Learning Objectives Identify matrices as operators Relate the transformation matrix to a set of new basis vectors Formulate code for mappings based on these transformation matrices Write code to find an orthonormal basis set computationally","title":"Week 4: Matrices Make Linear Mappings"},{"location":"linear_algebra/week_4/#matrices-as-objects-that-map-one-vector-onto-another","text":"","title":"Matrices as objects that map one vector onto another"},{"location":"linear_algebra/week_4/#introduction-to-einstein-summation-convention-and-the-symmetry-of-the-dot-product","text":"There is a different, important way to write matrix transformations that we have not yet discussed. It's called the Einstein's Summation Convention . In this convention, we write down the actual operations on the elements of a matrix, which is useful when you're coding or programming. It also lets us see something neat about the dot product, and it lets us deal with non-square matrices. When we started, we said that multiplying a matrix by a vector or with another matrix is a process of taking every element in each row of the first matrix multiplied with the corresponding element in each column of the other matrix, summing the products and putting them in place. In Einstein's Summation Convention, we represent the matrix product C = AB C = AB : \\begin{pmatrix} a_{11} & a_{12} & . & . & . & a_{1n} \\\\\\ a_{21} & a_{22} & & & & . &\\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ a_{n1} & . & . & . & . & a_{nm}\\end{pmatrix} \\begin{pmatrix} b_{11} & b_{12} & . & . & . & b_{1m} \\\\\\ b_{21} & b_{22} & & & & . &\\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ b_{m1} & . & . & . & . & b_{mp}\\end{pmatrix} = AB \\begin{pmatrix} a_{11} & a_{12} & . & . & . & a_{1n} \\\\\\ a_{21} & a_{22} & & & & . &\\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ a_{n1} & . & . & . & . & a_{nm}\\end{pmatrix} \\begin{pmatrix} b_{11} & b_{12} & . & . & . & b_{1m} \\\\\\ b_{21} & b_{22} & & & & . &\\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ . & & & & & . \\\\\\ b_{m1} & . & . & . & . & b_{mp}\\end{pmatrix} = AB as, c_{ij} = a_{i1}b_{1j} + ... + a_{im}b_{mj} = \\sum_{k=1}^m a_{ik}b_{kj} c_{ij} = a_{i1}b_{1j} + ... + a_{im}b_{mj} = \\sum_{k=1}^m a_{ik}b_{kj} Note To be clear, its the c_{ij} c_{ij} itself that is written Einstein's Summation Convention, not everything that comes to the right of the = = sign. For i = 1, ..., n i = 1, ..., n and j = 1, ..., p j = 1, ..., p , where A A is a n \\times m n \\times m matrix, B B is a m \\times p m \\times p matrix, and C C is a n \\times p n \\times p matrix. This is useful when we are implementing matrix multiplication in code, because it makes it obvious exactly what operations we need to perform. In this case, it should be obvious that we can run three loops over i i , j j and k k , and then use an accumulator on the k k 's to find the elements of the product matrix AB AB . Note We haven't talked about this yet, but now we can see it clearly. There's no reason, so long as the matrices have the same number of entries in k k , that the matrices we multiply need to be the same shape! Let's revisit the dot product in light of the Einstein Summation Convention. If we've got two vectors, let's call them u u and v v , where u u is a column vector having elements u_i u_i and v v is another column vector having elements v_i v_i . \\begin{pmatrix}u_i \\\\\\ \\vdots \\\\\\ u_{n}\\end{pmatrix} \\cdot \\begin{pmatrix}v_i \\\\\\ \\vdots \\\\\\ v_n \\end{pmatrix} \\begin{pmatrix}u_i \\\\\\ \\vdots \\\\\\ u_{n}\\end{pmatrix} \\cdot \\begin{pmatrix}v_i \\\\\\ \\vdots \\\\\\ v_n \\end{pmatrix} When we dot them together, we are computing the following: u \\cdot v = \\sum^n_{i=1}u_iv_i = u_1v_1 + u_2v_2 + ... + u_nv_n u \\cdot v = \\sum^n_{i=1}u_iv_i = u_1v_1 + u_2v_2 + ... + u_nv_n Notice that, \\begin{pmatrix}u_i \\\\\\ \\vdots \\\\\\ u_{n}\\end{pmatrix} \\cdot \\begin{pmatrix}v_i \\\\\\ \\vdots \\\\\\ v_n \\end{pmatrix} = \\begin{pmatrix}u_i & \\cdots & u_{n}\\end{pmatrix} \\cdot \\begin{pmatrix}v_i \\\\\\ \\vdots \\\\\\ v_n \\end{pmatrix} \\begin{pmatrix}u_i \\\\\\ \\vdots \\\\\\ u_{n}\\end{pmatrix} \\cdot \\begin{pmatrix}v_i \\\\\\ \\vdots \\\\\\ v_n \\end{pmatrix} = \\begin{pmatrix}u_i & \\cdots & u_{n}\\end{pmatrix} \\cdot \\begin{pmatrix}v_i \\\\\\ \\vdots \\\\\\ v_n \\end{pmatrix} and so we notice that there's some equivalence between a matrix transformation (or multiplication ) and the dot product . Lets explore this in more detail. Symmetry of the dot product Say we have the vector \\hat u \\hat u , with components u_1 u_1 and u_2 u_2 . Let's imagine what happens if we dot \\hat u \\hat u with the basis vector \\hat e_1 \\hat e_1 . We know from previous sections that this gives us the length of the projection of \\hat u_1 \\hat u_1 on \\hat e_1 \\hat e_1 multiplied by the norm of \\hat e_1 \\hat e_1 . But what if we do the reverse? What if we dot \\hat e_1 \\hat e_1 with \\hat u \\hat u ? We already know that numerically, the result will be the same, as the dot product is commutative . So geometrically, we can imagine drawing a line of symmetry between the point where the two projections cross: Previously, we stated (without proof, although the numerical proof is trivial) that the dot product is commutative, and now, we have shown geometrically why that is true. Conclusions In this section, we introduced Einstein's Summation Convention , which is a compact and computationally useful (but not very visually intuitive) way to write down matrix operations. This led to a discussion on the similarities between the dot product and matrix multiplication, where we noticed a connection between matrix multiplication , and the dot product , which itself has a geometric understanding as the concept of projection , i.e. projecting one vector onto another. This allows us to think about matrix multiplication with a vector as being the projection of that vector onto the vectors composing the matrix (i.e. the columns of the matrix).","title":"Introduction to Einstein summation convention and the symmetry of the dot product"},{"location":"linear_algebra/week_4/#matrices-transform-into-the-new-basis-vector-set","text":"Tip Watch this video before reading this section. For more practice with changing basis, see this Khan Academy section.","title":"Matrices transform into the new basis vector set"},{"location":"linear_algebra/week_4/#matrices-changing-basis","text":"We have said before that the columns of a transformation matrix are the axes of the new basis vectors after applying the mapping. We're now going to spend a little time looking at how to transform a vector from one set of basis vectors to another . Let's say we have two sets of basis vectors, which define a first coordinate system ( \\text{CS}_1) \\text{CS}_1) and a second coordinate system ( \\text{CS}_2) \\text{CS}_2) . Let the basis vectors of \\text{CS}_2 \\text{CS}_2 , from the perspective of \\text{CS}_1 \\text{CS}_1 be: \\hat b_1 = \\begin{bmatrix}3 \\\\\\ 1\\end{bmatrix}, \\hat b_2 = \\begin{bmatrix}1 \\\\\\ 1\\end{bmatrix} \\hat b_1 = \\begin{bmatrix}3 \\\\\\ 1\\end{bmatrix}, \\hat b_2 = \\begin{bmatrix}1 \\\\\\ 1\\end{bmatrix} Lets package these basis vectors into a matrix \\text{CS}_{21} \\text{CS}_{21} , for convenience \\text{CS}_{21} = \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\text{CS}_{21} = \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} Think of these as the basis vectors of \\text{CS}_2 \\text{CS}_2 as they would appear in \\text{CS}_1 \\text{CS}_1 . If we wanted to change the basis of any vectors in \\text{CS}_2 \\text{CS}_2 to \\text{CS}_1 \\text{CS}_1 , we simply do: \\text{CS}_{21} \\cdot \\text{vector in CS}_2 = \\text{vector in CS}_1 \\text{CS}_{21} \\cdot \\text{vector in CS}_2 = \\text{vector in CS}_1 E.g., for the vector \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} defined in terms of \\text{CS}_2 \\text{CS}_2 \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} = \\begin{bmatrix}5 \\\\\\ 2\\end{bmatrix} \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} = \\begin{bmatrix}5 \\\\\\ 2\\end{bmatrix} That is, a vector \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} described in \\text{CS}_2 \\text{CS}_2 , is described as \\begin{bmatrix}5 \\\\\\ 2\\end{bmatrix} \\begin{bmatrix}5 \\\\\\ 2\\end{bmatrix} in \\text{CS}_1 \\text{CS}_1 . Why does this make sense? Well, you can think of \\text{CS}_{21} \\text{CS}_{21} as the transformation that takes the basis vectors of \\text{CS}_1 \\text{CS}_1 and moves them to the positions of the basis vectors of \\text{CS}_2 \\text{CS}_2 . Applying this transformation to a vector in \\text{CS}_2 \\text{CS}_2 , therefore, gives us the corresponding vector in \\text{CS}_1 \\text{CS}_1 . Now, how do we do the reverse? How do we translate a vector in \\text{CS}_1 \\text{CS}_1 to a vector in \\text{CS}_2 \\text{CS}_2 ? All we need to do to change basis in the reverse case is to multiply a vector in one coordinate system by the inverse of the matrix containing the basis vectors of another: \\text{CS}^{-1}_{21} \\cdot \\text{vector in CS}_1 = \\text{vector in CS}_2 \\text{CS}^{-1}_{21} \\cdot \\text{vector in CS}_1 = \\text{vector in CS}_2 E.g., \\frac{1}{2}\\begin{bmatrix}1 & -1 \\\\\\ -1 & 3\\end{bmatrix}\\begin{bmatrix}5 \\\\\\ 2\\end{bmatrix} = \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} \\frac{1}{2}\\begin{bmatrix}1 & -1 \\\\\\ -1 & 3\\end{bmatrix}\\begin{bmatrix}5 \\\\\\ 2\\end{bmatrix} = \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} Notice that this process gave us the coordinates of the vector \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} \\begin{bmatrix}\\frac{3}{2} \\\\\\ \\frac{1}{2}\\end{bmatrix} in \\text{CS}_2 \\text{CS}_2 , which is equal to the vector that we started with.","title":"Matrices changing basis"},{"location":"linear_algebra/week_4/#generalizing","text":"This is a little bit tricky, so lets generalize. Imagine we have two coordinate systems, \"ours\" and \"theirs\". We can translate vectors in \"their\" coordinate system to \"our\" coordinate system by applying a transformation A A , where A A is a matrix whose columns contain the basis vectors of \"their\" coordinate system as they appear in \"our coordinate system\": \\text{(1) } A \\cdot \\text{vector in \"their\" coordinate system} = \\text{vector in \"our\" coordinate system} \\text{(1) } A \\cdot \\text{vector in \"their\" coordinate system} = \\text{vector in \"our\" coordinate system} To do the reverse, i.e. take a vector in \"our\" coordinate system and translate it to \"their\" coordinate system, we simply multiply \"our\" vector by the inverse of A \\text{(2) } \\text{vector in \"their\" coordinate system} = A^{-1} \\cdot \\text{vector in \"our\" coordinate system} \\text{(2) } \\text{vector in \"their\" coordinate system} = A^{-1} \\cdot \\text{vector in \"our\" coordinate system} It should be obvious now why this is the case, we simply moved A A over in the (1) to get (2)! Orthonormal basis set When we discussed vectors and projections, we said that if the new basis vectors were orthogonal then we could use projection to easily change basis. Note see Changing basis for a fleshed out example. Summary Not orthogonal, use matrix multiplication. Orthogonal, use projection product.","title":"Generalizing"},{"location":"linear_algebra/week_4/#doing-a-transformation-in-a-changed-basis","text":"Tip Watch the last little bit of this video first. Lets discuss the process of applying a transformation in a changed basis. Say again that the basis vectors of coordinate system \\text{CS}_2 \\text{CS}_2 from the perspective of coordinate system \\text{CS}_1 \\text{CS}_1 are: \\text{CS}_{21} = \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\text{CS}_{21} = \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} Say further that we have a vector in \\text{CS}_2 \\text{CS}_2 that we want to transform: {c_2} = \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} {c_2} = \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} And the tranformation we want to apply is: N = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\\\ 1 & 1\\end{bmatrix} N = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\\\ 1 & 1\\end{bmatrix} Note This rotates the vector space 45^0 45^0 counter-clockwise. How do we apply the transformation N N to a vector defined by the coordinate system \\text{CS}_2 \\text{CS}_2 ? The first thing to do is take the vector {c_2} {c_2} and multiply it by \\text{CS}_{21} \\text{CS}_{21} , that is, change the basis of the vector {c_2} {c_2} from \\text{CS}_2 \\text{CS}_2 to \\text{CS}_1 \\text{CS}_1 : {c_1} = \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} {c_1} = \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} Then, we can apply the transformation: {c_1}' = R \\cdot \\text{CS}_{21} \\cdot {c_2} {c_1}' = R \\cdot \\text{CS}_{21} \\cdot {c_2} {c_1}' = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} {c_1}' = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}2 & 0 \\\\\\ 4 & 2\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}2 & 0 \\\\\\ 4 & 2\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} What if, once we obtained this output vector, we wanted to change its basis back to \\text{CS}_2 \\text{CS}_2 ? Recall from the last section that we multiply the whole thing by the inverse of \\text{CS}_{21}^{-1} \\text{CS}_{21}^{-1} {c_2}' = \\text{CS}_{21}^{-1} \\cdot R \\cdot \\text{CS}_{21} \\cdot {c_2} {c_2}' = \\text{CS}_{21}^{-1} \\cdot R \\cdot \\text{CS}_{21} \\cdot {c_2} {c_2}' = \\frac{1}{2} \\begin{bmatrix}1 & -1 \\\\\\ -1 & 3\\end{bmatrix} \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} {c_2}' = \\frac{1}{2} \\begin{bmatrix}1 & -1 \\\\\\ -1 & 3\\end{bmatrix} \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}3 & 1 \\\\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} = \\frac{1}{2} \\begin{bmatrix}1 & -1 \\\\\\ -1 & 3\\end{bmatrix}\\frac{1}{\\sqrt{2}}\\begin{bmatrix}2 & 0 \\\\\\ 4 & 2\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} = \\frac{1}{2} \\begin{bmatrix}1 & -1 \\\\\\ -1 & 3\\end{bmatrix}\\frac{1}{\\sqrt{2}}\\begin{bmatrix}2 & 0 \\\\\\ 4 & 2\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}-1 & -1 \\\\\\ 5 & 3\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}-1 & -1 \\\\\\ 5 & 3\\end{bmatrix} \\begin{bmatrix}x \\\\\\ y\\end{bmatrix} This operation essentially builds on the previous operation to return the transformed output vector, {c_1}' {c_1}' relative to \\text{CS}_2 \\text{CS}_2 , that is it returns {c_2}' {c_2}' , where {c_2}' {c_2}' is where c_2 c_2 ends up after in the basis \\text{CS}_2 \\text{CS}_2 after some transformation M M has been applied.","title":"Doing a transformation in a changed basis"},{"location":"linear_algebra/week_4/#generalizing_1","text":"Again, this is tricky, so lets generalize. Imagine we have two coordinate systems, \"ours\" and \"theirs\". If we are given: a vector with its basis in \"their\" coordinate system A A , a matrix whose columns are the basis vectors of \"their\" coordinate system as they appear in \"our\" coordinate system M M some transformation, with its basis in \"our\" coordinate system Then, Change the vector in \"their\" basis to \"ours\": A \\cdot \\text{vector in \"their\" coordinate system} A \\cdot \\text{vector in \"their\" coordinate system} Apply the transformation in \"our\" coordinate system: M \\cdot A \\cdot \\text{vector in \"their\" coordinate system} M \\cdot A \\cdot \\text{vector in \"their\" coordinate system} Change the resulting vector back to \"their\" coordinate system: A^{-1} \\cdot M \\cdot A \\cdot \\text{vector in \"their\" coordinate system} A^{-1} \\cdot M \\cdot A \\cdot \\text{vector in \"their\" coordinate system} In sum, the transformation A^{-1}MA A^{-1}MA will take any vector in \"their\" coordinate system, apply some transformation in \"our\" coordinate system, and return the resulting vector in \"their\" coordinate system.","title":"Generalizing"},{"location":"linear_algebra/week_4/#making-multiple-mappings-deciding-if-these-are-reversible","text":"","title":"Making multiple mappings, deciding if these are reversible"},{"location":"linear_algebra/week_4/#orthogonal-matrices","text":"It is very useful to compose a transformation matrix whose column and row vectors make up a new basis , with the additional constraint of making all of these component vectors orthogonal. Such a square matrix of orthonormal columns and rows is known as an orthogonal matrix . In this section we are going to look at how to construct such a matrix, and why it's useful. (Aside) Transpose First, we need to define a new matrix operation called the transpose . The transpose of a matrix is an operator which flips a matrix over its diagonal , that is it switches the row and column indices of the matrix by producing another matrix denoted as A^T A^T . It is achieved by any one of the following equivalent actions: reflect A A over its main diagonal (which runs from top-left to bottom-right) to obtain A^T A^T , write the rows of A A as the columns of A^T A^T , write the columns of A A as the rows of A^T A^T . Formally, the i -th row, j -th column element of A^T A^T is the j -th row, i -th column element of A A : [A^T]_{ij} = [A]_{ji} [A^T]_{ij} = [A]_{ji} Now let's imagine I have a n \\times n n \\times n matrix A A , with a series of column vectors which are going to be the basis vectors of the some new transformed vector space: A = \\begin{pmatrix} \\begin{pmatrix}\\vdots \\\\\\ \\hat a_1 \\\\\\ \\vdots\\end{pmatrix} & \\begin{pmatrix} \\vdots \\\\\\ \\hat a_2\\\\\\ \\vdots \\end{pmatrix} & \\dots \\begin{pmatrix} \\vdots \\\\\\ \\hat a_n\\\\\\ \\vdots \\end{pmatrix} \\end{pmatrix} A = \\begin{pmatrix} \\begin{pmatrix}\\vdots \\\\\\ \\hat a_1 \\\\\\ \\vdots\\end{pmatrix} & \\begin{pmatrix} \\vdots \\\\\\ \\hat a_2\\\\\\ \\vdots \\end{pmatrix} & \\dots \\begin{pmatrix} \\vdots \\\\\\ \\hat a_n\\\\\\ \\vdots \\end{pmatrix} \\end{pmatrix} Lets place two more constaints on this matrix A A : First, the column vectors a_i a_i have unit length Second, the column vectors a_i a_i are orthogonal to each other. Note That is, \\hat a_i \\cdot \\hat a_j = 0 \\hat a_i \\cdot \\hat a_j = 0 for \\forall i \\ne j \\forall i \\ne j and \\hat a_i \\cdot \\hat a_i = 1 \\hat a_i \\cdot \\hat a_i = 1 . Lets think about what happens when we multiply A A by its transpose, A^T A^T : A^TA = \\begin{pmatrix} \\begin{pmatrix}\\dots & \\hat a_1 & \\dots\\end{pmatrix} \\\\\\ \\begin{pmatrix} \\dots & \\hat a_2 & \\dots \\end{pmatrix} \\\\\\ \\vdots \\\\\\ \\begin{pmatrix} \\dots & \\hat a_n & \\dots \\end{pmatrix} \\end{pmatrix} \\begin{pmatrix} \\begin{pmatrix}\\vdots \\\\\\ \\hat a_1 \\\\\\ \\vdots\\end{pmatrix} & \\begin{pmatrix} \\vdots \\\\\\ \\hat a_2\\\\\\ \\vdots \\end{pmatrix} & \\dots \\begin{pmatrix} \\vdots \\\\\\ \\hat a_n\\\\\\ \\vdots \\end{pmatrix} \\end{pmatrix} = I_n A^TA = \\begin{pmatrix} \\begin{pmatrix}\\dots & \\hat a_1 & \\dots\\end{pmatrix} \\\\\\ \\begin{pmatrix} \\dots & \\hat a_2 & \\dots \\end{pmatrix} \\\\\\ \\vdots \\\\\\ \\begin{pmatrix} \\dots & \\hat a_n & \\dots \\end{pmatrix} \\end{pmatrix} \\begin{pmatrix} \\begin{pmatrix}\\vdots \\\\\\ \\hat a_1 \\\\\\ \\vdots\\end{pmatrix} & \\begin{pmatrix} \\vdots \\\\\\ \\hat a_2\\\\\\ \\vdots \\end{pmatrix} & \\dots \\begin{pmatrix} \\vdots \\\\\\ \\hat a_n\\\\\\ \\vdots \\end{pmatrix} \\end{pmatrix} = I_n So what we notice is that in the case where A A is composed of vectors that are normal to each other and have unit length , (i.e. when they're orthonormal ), then A^TA = I A^TA = I . Stated another way, A^T A^T in this situation is actually the inverse of A A ! This special case is known as an orthogonal matrix . Another thing to note is that because all the basis vectors are of unit length, it must scale space by a factor of one . Stated another way, the determinant of an orthogonal matrix must be either plus or minus one. \\vert A \\vert = \\pm 1 \\vert A \\vert = \\pm 1 Where the minus one arises if the new basis vector set flip space around (from right-handed to left-handed or vice versa). Notice that if A^T A^T , the inverse of A A , then by the definition of the inverse: A^TA = AA^T = I_n A^TA = AA^T = I_n So, we could pre- or post- multiply and still get the identity. This means that the rows of the orthogonal matrix are also orthonormal to each other! So, the transpose matrix of an orthogonal basis set, is itself another orthogonal basis set. Now, remember that in the last module on vectors, we said that transforming a vector onto a new coordinate system was as easy as taking the projection or dot product of that vector onto each of the new bases vectors, as long as they were orthogonal to each other . So, if we have a vector r r and we want to project r r into a new set of axes, let's call them \\hat e_1 \\hat e_1 and \\hat e_2 \\hat e_2 , as long as these vectors are orthogonal to each other, then we can project into the new vector space just by taking the dot product of r r with \\hat e_2 \\hat e_2 , and the dot product of r r with \\hat e_1 \\hat e_1 , and then we'd have its components in the new set of axis. Note See Changing basis for a walked-through example. Conclusions In this section, we introduced the most convenient basis vector set of all, the orthonormal bases vector set . Wherever possible we want to use an orthonormal basis vector set represented as a orthogonal matrix A A . This set of vectors has the following properties and consequences that make it easy to work with: the transpose of such a matrix will be its inverse , which makes the inverse incredibly easy to compute, the transformation will be reversible because space doesn't get collapsed by any dimensions, the projections (i.e. the result of computing the projection of a vector onto the matrix A A ) are just the dot products. One final note. If we arrange the bases vectors in the correct order, then the determinant will be one . \\vert A \\vert = 1 \\vert A \\vert = 1 An easy way to check if they aren't in the right order, is to check if the determinant is minus one. This means we've transformed our space from right to left handed orientation. All we have to do to remedy this is to exchange a pair of vectors in A A such that \\vert A \\vert = 1 \\vert A \\vert = 1 .","title":"Orthogonal matrices"},{"location":"linear_algebra/week_4/#recognizing-mapping-matrices-and-applying-these-to-data","text":"","title":"Recognizing mapping matrices and applying these to data"},{"location":"linear_algebra/week_4/#the-gramschmidt-process","text":"In the last section, we motivated the idea that life is much easier if we can construct an orthonormal basis vector set, but we haven't talked about how to do it. In this section, we will explore just that. We'll start from the assumption that we already have some linearly independent vectors that span the space we're interested in. Say we have some such vectors V = \\{v_1, v_2, ..., v_n\\} V = \\{v_1, v_2, ..., v_n\\} , Note If you want to check linear independence, you can write down your vectors as the the columns in a matrix and check that the determinant of that matrix isn't zero. but they aren't orthogonal to each other or of unit length. Our life would probably be easier if we could construct some orthonormal basis. As it turns out, there's a process for doing just that which is called the Gram-Schmidt process . Let's take the first vector in the set to be v_1 v_1 . In this first step, we're just going to normalize v_1 v_1 to get our eventual first basis vector e_1 e_1 e_1 = \\frac{v_1}{\\vert v_1 \\vert} e_1 = \\frac{v_1}{\\vert v_1 \\vert} Now, we can think of v_2 v_2 as being composed of two things: a component that is in the direction of e_1 e_1 and a component that's perpendicular to e_1 e_1 . We can find the component that's in the direction of e_1 e_1 by taking the vector projection v_2 v_2 onto e_1 e_1 : v_2 = (v_2 \\cdot e_1) \\frac{e_1}{\\vert e_1 \\vert} v_2 = (v_2 \\cdot e_1) \\frac{e_1}{\\vert e_1 \\vert} Note |e_1| |e_1| is 1 so we could actually omit it. If we subtract this vector projection from v_2 v_2 we get u_2 u_2 , a vector which is orthogonal to e_1 e_1 : u_2 = v_2 - (v_2 \\cdot e_1) e_1 u_2 = v_2 - (v_2 \\cdot e_1) e_1 Finally, dividing u_2 u_2 by its length gives us e_2 e_2 , the unit vector orthogonal to e_1 e_1 : e_2 = \\frac{u_2}{\\vert u_2 \\vert} e_2 = \\frac{u_2}{\\vert u_2 \\vert} We could continue this process for all vectors in our set V V . The general formula (in pseudocode) is: # For all vectors in our set V for i in | V | : # For all vectors in our set V that come before v_i for j in i : # Subtract the component of v_i in the direction of the previous vectors v_j v_i = v_i - v_i . dot ( v_j ) * v_j # If |v_i| is not zero, normalize it to unit length. Otherwise it is linearly dependent on a # previous vector, so set it equal to the zero vector. if | v_i | !!! note 0 : v_i = v_i / | v_i | else : v_i = zero_vector Conclusions So that's how we construct an orthonormal basis set, which makes our lives much easier for all the reasons we discussed here .","title":"The Gram\u2013Schmidt process"},{"location":"linear_algebra/week_4/#reflecting-in-a-plane","text":"This is a rather involved example, and is probably best if you just watch it yourself here . If I can find the time, i'll make notes for the video!","title":"Reflecting in a plane"},{"location":"linear_algebra/week_5/","text":"Week 5: Eigenvalues and Eigenvectors Eigenvectors are particular vectors that are unrotated by a transformation matrix (i.e., they remain on their own span ) and eigenvalues are the amount by which the eigenvectors are scaled. These special 'eigen-things' are very useful in linear algebra and will let us examine Google's famous PageRank algorithm for presenting web search results. Then we'll apply this in code, which will wrap up the course. Tip It is best to watch this video first, then return to and read through this section. If you want even more exposure to these ideas, try the first three sections of the Khan Academy course here . Learning Objectives Identify geometrically what an eigenvector/value is Apply mathematical formulation in simple cases Build an intuition of larger dimensional eigensystems Write code to solve a large dimensional eigen problem What are eigen-things? What are eigenvalues and eigenvectors? The word, \"eigen\" is perhaps most usefully translated from German as meaning characteristic . So when we talk about an eigenproblem , we're talking about finding the characteristic properties of something . But characteristic of what? This module, like the previous weeks, will try and explain this concept of \"eigen-ness\" primarily through a geometric interpretation, which allows us to discuss images rather than immediately getting tangled up in the math. Note This topic is often considered by students to be quite tricky. But once you know how to sketch these problems, the rest is just algebra. As you've seen from previous weeks, it's possible to express the concept of linear transformations using matrices . These operations can include scalings , rotations , and shears . Often, when applying these transformations, we are thinking about what they might do to a specific vector . However, it can also be useful to think about what it might look like when they are applied to every vector in this space. This is most easily visualized by drawing a square centered at the origin, and then observing how the square is distorted when you apply the transformation. For example, if we apply a scaling of 2 in the vertical direction, the square would become a rectangle. Whereas, if we applied a horizontal shear to this space, it would become a trapezoid: Now, here's the key concept. Notice that, after the transformation is applied, some vectors end up lying on the same line that they started on whereas, others do not. To highlight this, lets draw three specific vectors onto our initial square. Now, consider our vertical scaling again, and think about what will happen to these three vectors. As you can see, the horizontal green vector is unchanged, i.e., it is pointing in the same direction and having the same length. The vertical pink vector is also still pointing in the same direction as before but its length is doubled. Lastly, the diagonal orange vector used to be exactly 45 degrees to the axis, but it's angle has now increased as has its length. Besides the horizontal and vertical vectors, any other vectors' direction would have been changed by this vertical scaling. So in some sense, the horizontal and vertical vectors are special , they are characteristic of this particular transformation. These are our eigenvectors , and the value they are scaled by is know as an eigenvalue . Note From a conceptual perspective, that's about it for 2D eigen-problems, we simply take a transformation and we look for the vectors who are still laying on the same span as before, and then we measure how much their length has changed. This is basically what eigenvectors and their corresponding eigenvalues are. Let's look at two more classic examples to make sure that we can generalize what we've learned. First, let look look at pure shear , where pure means that we aren't performing any scaling or rotation in addition, so the area is unchanged: Notice that it's only the green horizontal line that is still laying along its original span, and all the other vectors will be shifted Finally, let's look at rotation . Clearly, this thing has got no eigenvectors at all, as all of the vectors have been rotated off their original span: Conclusions In this lecture, we've already covered almost all of what you need to know about eigenvectors and eigenvalues. Although we've only been working in two dimensions so far, the concept is exactly the same in three or more dimensions. In the rest of the module, we'll have a look at some special cases, as well as discussing how to describe what we've observed in more mathematical terms. Getting into the detail of eigenproblems Special eigen-cases As we saw previously, eigenvectors are those which lie along the same span both before and after applying a linear transform to a space. Eigenvalues are simply the amount that each of those vectors has been stretched in the process. In this section, we're going to look at three special cases to make sure the intuition we've built so far is robust, and then we're going to try and extend this concept into three dimensions. The first example we're going to consider is that of a uniform scaling, which is where we scale by the same amount in each direction: As you will hopefully have spotted, not only are all three of the vectors that we've highlighted eigenvectors, but in fact, for a uniform scaling, any vector would be an eigenvector. In this second example, we're going to look at rotation. In the previous section, we applied a small rotation, and we found that it had no eigenvectors. However, there is one case of non-zero pure rotation which does have at least some eigenvectors, and that is 180 180 degrees: As you can see, the three eigenvectors are still laying on the same spans as before, but pointing in the opposite direction. This means that once again, all vectors for this transform are eigenvectors, and they all have eigenvalues of -1 -1 , which means that although the eigenvectors haven't changed length, they are all now pointing in the opposite direction. In this third case, we're going to look at a combination of a horizontal shear and a vertical scaling , and it's slightly less obvious than some of the previous examples. Just like the pure shear case we saw previously, the green horizontal vector is an eigenvector and its eigenvalue is still 1 1 . However, despite the fact that neither of the other two vectors shown are eigen, this transformation does have two eigenvectors: Let's apply the inverse transform and watch our parallelogram go back to its original square. But this time, with our other eigenvector visible. Hopefully, you're at least convinced that it is indeed an eigenvector as it stays on its own span: This shows us that while the concept of eigenvectors is fairly straightforward, eigenvectors aren't always easy to spot. This problem is even tougher in three or more dimensions, and many of the uses of eigen theory in machine learning frame the system as being composed of hundreds of dimensions or more. So, clearly, we're going to need a more robust mathematical description of this concept to allow us to proceed. Before we do, let's take a quick look at one example in 3D. Clearly, scaling and shear are all going to operate much the same way in 3D as they do in 2D. However, rotation does take on a neat new meaning. As you can see from the image, although both the pink and green vectors have changed direction, the orange vector has not moved. This means that the orange vector is an eigenvector, but it also tells us, as a physical interpretation, that if we find the eigenvector of a 3D rotation, it means we've also found the axis of rotation . In this video, we've covered a range of special cases, which I hope have prompted the questions in your mind about how we're going to go about writing a formal definition of an eigen-problem. Calculating eigenvectors At this point, we should now have a reasonable feeling for what an eigen-problem looks like, at least geometrically. In this section, we're going to formalize this concept into an algebraic expression, which will allow us to calculate eigenvalues and eigenvectors whenever they exist. Consider a transformation A A . An eigenvector of this transformation is any vector that can be written as a scaled version of itself after the transformation is applied, i.e., Ax = \\lambda x Ax = \\lambda x This expression captures the idea that applying the transformation A A to an eigenvector x x is the same as scaling that eigenvector x x by some number, \\lambda \\lambda (the eigenvalue). In order to solve for the eigenvectors of the transformation A A , we need to find values of x x that make the two sides equal. To help us find the solutions to this expression, we can rewrite it by putting all the terms on one side and then factorizing (A - \\lambda I) x = 0 (A - \\lambda I) x = 0 Note If you're wondering where the I I term came from, it's just an n \\times n n \\times n identity matrix. We didn't need this in the first expression we wrote, as multiplying vectors by scalars is defined. However, subtracting scalars from matrices is not defined , so the I I just tidies up the math, without changing the meaning. Now that we have this expression, we can see that for the left-hand side to equal 0 0 , either the contents of the brackets must be 0 0 or the vector x x must be 0 0 . As it turns out, we're not interested in the case where the vector x x is 0 0 , i.e., when it has no length or direction, as this represents a trivial solution . Instead, we are interested in the case where the term in brackets is 0 0 . Referring back to the material in the previous parts of the course, we can test if a matrix operation will result in a 0 0 output by calculating its determinant det (A - \\lambda I) = 0 det (A - \\lambda I) = 0 Calculating the determinants manually is a lot of work for high dimensional matrices. So let's try applying this to an arbitrary 2 \\times 2 2 \\times 2 transformation. Let A = \\begin{pmatrix}a & b \\\\ c & d\\end{pmatrix} A = \\begin{pmatrix}a & b \\\\ c & d\\end{pmatrix} substituting this into our eigen-finding expression gives the following: det \\Biggl( \\begin{pmatrix}a & b \\\\\\ c & d\\end{pmatrix}- \\begin{pmatrix}\\lambda & 0 \\\\\\ 0 & \\lambda\\end{pmatrix} \\Biggl ) = 0 det \\Biggl( \\begin{pmatrix}a & b \\\\\\ c & d\\end{pmatrix}- \\begin{pmatrix}\\lambda & 0 \\\\\\ 0 & \\lambda\\end{pmatrix} \\Biggl ) = 0 Evaluating this determinant, we get what is referred to as the characteristic polynomial , which looks like this \\lambda^2 - (a + d) \\lambda + ad - bc = 0 \\lambda^2 - (a + d) \\lambda + ad - bc = 0 Our eigenvalues are simply the solutions of this equation. Once we solve for them, we can then plug them back into the original expression to calculate our eigenvectors. Click the dropdown below for a fully-worked out solution to computing eigenvalues and eigenvectors. Example Let's take the case of a vertical scaling by a factor of 2 2 , which is represented by the transformation matrix A = \\begin{pmatrix} 1 & 0 \\\\\\ 0 & 2\\end{pmatrix} A = \\begin{pmatrix} 1 & 0 \\\\\\ 0 & 2\\end{pmatrix} We start with our equation for finding eigenvalues Ax = \\lambda x Ax = \\lambda x Rearranging, we get (A-\\lambda I)x = 0 (A-\\lambda I)x = 0 Solving (A-\\lambda I) = 0 (A-\\lambda I) = 0 is equivalent to asking when the determinant of the matrix is 0 0 det((A- \\lambda I)) = 0 det((A- \\lambda I)) = 0 Subbing in our matrix A A and solving the resulting characteristic polynomial det \\begin{pmatrix} 1 - \\lambda & 0 \\\\\\ 0 & 2- \\lambda \\end{pmatrix} = (1 - \\lambda)(2-\\lambda) = 0 det \\begin{pmatrix} 1 - \\lambda & 0 \\\\\\ 0 & 2- \\lambda \\end{pmatrix} = (1 - \\lambda)(2-\\lambda) = 0 This means that our equation must have solutions at \\lambda = 1 \\lambda = 1 and \\lambda = 2 \\lambda = 2 . Thinking back to our original eigen-finding formula, (A - \\lambda I)x = 0 (A - \\lambda I)x = 0 , we can now sub these two solutions back in. Thinking about the case where \\lambda = 1 \\lambda = 1 , @\\lambda = 1: \\begin{pmatrix} 1 - 1 & 0 \\\\\\ 0 & 2 - 1 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} 0 \\\\\\ x_2\\end{pmatrix} = 0 @\\lambda = 1: \\begin{pmatrix} 1 - 1 & 0 \\\\\\ 0 & 2 - 1 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} 0 \\\\\\ x_2\\end{pmatrix} = 0 Now, thinking about the case where \\lambda = 2 \\lambda = 2 , @\\lambda = 2: \\begin{pmatrix} 1 - 2 & 0 \\\\\\ 0 & 2 - 2 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} -1 & 0 \\\\\\ 0 & 0 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} -x_1 \\\\\\ 0 \\end{pmatrix} = 0 @\\lambda = 2: \\begin{pmatrix} 1 - 2 & 0 \\\\\\ 0 & 2 - 2 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} -1 & 0 \\\\\\ 0 & 0 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} -x_1 \\\\\\ 0 \\end{pmatrix} = 0 So what do these two expressions tell us? Well, in the case where our eigenvalue \\lambda = 1 \\lambda = 1 , we've got an eigenvector where the x_2 x_2 term must be zero. But we don't really know anything about the x_1 x_1 term. Well, this is because any vector that points along the horizontal axis could be an eigenvector of this system . We say that by writing @\\lambda = 1: \\begin{pmatrix} t \\\\\\ 0 \\end{pmatrix} @\\lambda = 1: \\begin{pmatrix} t \\\\\\ 0 \\end{pmatrix} using an arbitrary parameter t t . Similarly for the \\lambda = 2 \\lambda = 2 case, we can say that our eigenvector must equal @\\lambda = 2: \\begin{pmatrix} 0 \\\\\\ t \\end{pmatrix} @\\lambda = 2: \\begin{pmatrix} 0 \\\\\\ t \\end{pmatrix} because as long as it doesn't move at all in the horizontal direction, any vector that's purely vertical would be an eigenvector of this system, as they would lie along the same span. So now we have two eigenvalues, and their two corresponding eigenvectors. Conclusions Despite all the fun that we've just been having, the truth is that you will almost certainly never have to perform this calculation by hand. Note Indeed, with libraries like numpy this is as easy as import numpy as np A = np . array ([ 1 , 0 ], [ 0 , 2 ]) eigenvalues , eigenvectors = numpy . linalg . eig ( A ) Furthermore, we saw that our approach required finding the roots of a polynomial of order n n , i.e., the dimension of your matrix, which means that the problem will very quickly stop being possible by analytical methods alone. When a computer finds the eigensolutions of a 100 dimensional problem it's forced to employ iterative numerical methods. Therefore, developing a strong conceptual understanding of eigen problems will be much more useful than being really good at calculating them by hand. In this sections, we translated our geometrical understanding of eigenvectors into a robust mathematical expression, and validated it on a few test cases. But I hope that I've also convinced you that working through lots of eigen-problems, as is often done in engineering undergraduate degrees, is not a good investment of your time if you already understand the underlying concepts. This is what computers are for. Next video, we'll be referring back to the concept of basis change to see what magic happens when you use eigenvectors as your basis. See you then. When changing to the eigenbasis is really useful Changing to the eigenbasis So now that we know what eigenvectors are and how to calculate them, we can combine this idea with a concept of changing basis which was covered earlier in the course. What emerges from this synthesis is a particularly powerful tool for performing efficient matrix operations called diagonalisation . Sometimes, we need to apply the same matrix multiplication many times. For example, imagine a transformation matrix T T represents the change in location of a particle after a single time step. So we can write that our initial position, described by vector v_0 v_0 , multiplied by the transformation T T gives us our new location, v_1 v_1 . To work out where our particle will be after two time steps, we can find v_2 v_2 by simply multiplying v_1 v_1 by T T , which is of course the same thing as multiplying v_0 v_0 by T T two times. So v_2 = T^2 v_0 v_2 = T^2 v_0 . Now imagine that we expect the same linear transformation to occur every time step for n n time steps. We can write this transformation as v_n = T^n v_0 v_n = T^n v_0 You've already seen how much work it takes to apply a single 3D matrix multiplication. So if we were to imagine that T T tells us what happens in one second, but we'd like to know where our particle is in two weeks from now, then n n is going to be around 1.2 million, i.e., we'd need to multiply T T by itself more than a million times, which may take quite a while. If all the terms in the matrix are zero except for those along the leading diagonal, we refer to it as a diagonal matrix . When raising matrices to powers, diagonal matrices make things a lot easier. All you need to do is put each of the terms on the diagonal to the power of n n and you've got the answer. So in this case, T^n = \\begin{pmatrix} a^n & 0 & 0 \\\\\\ 0 & b^n & 0 \\\\\\ 0 & 0 & c^n\\end{pmatrix} T^n = \\begin{pmatrix} a^n & 0 & 0 \\\\\\ 0 & b^n & 0 \\\\\\ 0 & 0 & c^n\\end{pmatrix} It's simple enough, but what if T T is not a diagonal matrix? Well, as you may have guessed, the answer comes from eigen-analysis. Essentially, what we're going to do is simply change to a basis where our transformation T T becomes diagonal, which is what we call an eigen-basis. We can then easily apply our power of n n to the diagonalized form, and finally transform the resulting matrix back again, giving us T^n T^n , but avoiding much of the work. As we saw in the section on changing basis, each column of our transform matrix simply represents the new location of the transformed unit vectors. So, to build our eigen-basis conversion matrix, we just plug in each of our eigenvectors as columns: C = \\begin{pmatrix}x_1 & x_2 & x_3 \\\\\\ \\vdots & \\vdots & \\vdots\\end{pmatrix} C = \\begin{pmatrix}x_1 & x_2 & x_3 \\\\\\ \\vdots & \\vdots & \\vdots\\end{pmatrix} Note However, don't forget that some of these maybe complex, so not easy to spot using the purely geometrical approach, but they are appear in the math just like the others. Applying this transform, we find ourselves in a world where multiplying by T T is effectively just a pure scaling, which is another way of saying that it can now be represented by a diagonal matrix. Crucially, this diagonal matrix, D D , contains the corresponding eigenvalues of the matrix T T . So, D = \\begin{pmatrix} \\lambda_1 & 0 & 0 \\\\\\ 0 & \\lambda_2 & 0 \\\\\\ 0 & 0 & \\lambda_3\\end{pmatrix} D = \\begin{pmatrix} \\lambda_1 & 0 & 0 \\\\\\ 0 & \\lambda_2 & 0 \\\\\\ 0 & 0 & \\lambda_3\\end{pmatrix} We're so close now to unleashing the power of eigen. The final link that we need to see is the following. Bringing together everything we've just said, it should now be clear that applying the transformation T T is just the same as converting to our eigenbasis, applying the diagonalized matrix, and then converting back again. So T = CDC^{-1} T = CDC^{-1} <span><span class=\"MathJax_Preview\">T = CDC^{-1}</span><script type=\"math/tex\">T = CDC^{-1} which suggests that T^2 = CDC^{-1}CDC^{-1} = CDDC^{-1} = CD^2C^{-1} T^2 = CDC^{-1}CDC^{-1} = CDDC^{-1} = CD^2C^{-1} <span><span class=\"MathJax_Preview\">T^2 = CDC^{-1}CDC^{-1} = CDDC^{-1} = CD^2C^{-1}</span><script type=\"math/tex\">T^2 = CDC^{-1}CDC^{-1} = CDDC^{-1} = CD^2C^{-1} So hopefully you've spotted that in the middle of our expression on the right-hand side, you've got C C multiplied by C inverse. But multiplying a matrix and then by its inverse is just the same as doing nothing at all. So we can simply remove this operation. And then we can finish this expression by saying, well this must be CD^2C^{-1} CD^2C^{-1} . We can of course then generalize this to any power of T T we'd like. So finally we can say that, T^n = CD^nC^{-1} T^n = CD^nC^{-1} We now have a method which lets us apply a transformation matrix as many times as we'd like without paying a large computational cost. Conclusions This result brings together many of the ideas that we've encountered so far in this course. Check out this video, where we'll work through a short example just to ensure that this approach lines up with our expectations when applied to a simple case. Making the PageRank algorithm PageRank The final topic of this module on Eigenproblems, as well as the final topic of this course as a whole, will focus on an algorithm called PageRank . This algorithm was famously published by and named after Google founder Larry Page and colleagues in 1998. And was used by Google to help them decide which order to display their websites when they returned from search. The central assumption underpinning page rank is that the importance of a website is related to its links to and from other websites, and somehow Eigen theory comes up. This bubble diagram represents a model mini Internet, where each bubble is a webpage and each arrow from A, B, C, and D represents a link on that webpage which takes you to one of the others. We're trying to build an expression that tells us, based on this network structure, which of these webpages is most relevant to the person who made the search. As such, we're going to use the concept of Procrastinating Pat who is an imaginary person who goes on the Internet and just randomly click links to avoid doing their work. By mapping all the possible links, we can build a model to estimate the amount of time we would expect Pat to spend on each webpage. We can describe the links present on page A as a vector, where each row is either a one or a zero based on whether there is a link to the corresponding page. And then normalise the vector by the total number of the links, such that they can be used to describe a probability for that page. For example, the vector of links from page A will be \\begin{bmatrix} 0 & 1 & 1 & 1\\end{bmatrix} \\begin{bmatrix} 0 & 1 & 1 & 1\\end{bmatrix} because vector A has links to sites B, to C, and to D, but it doesn't have a link to itself. Also, because there are three links in this page in total, we would normalize by a factor of a third. So the total click probability sums to one. So we can write, L_A = \\begin{bmatrix}0 & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3}\\end{bmatrix} L_A = \\begin{bmatrix}0 & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3}\\end{bmatrix} Following the same logic, the link vectors in the next two sites are shown here: L_B = \\begin{bmatrix} \\frac{1}{2} & 0 & 0 & \\frac{1}{2}\\end{bmatrix}$$ $$L_C = \\begin{bmatrix}0 & 0 & 0 & 1\\end{bmatrix} L_B = \\begin{bmatrix} \\frac{1}{2} & 0 & 0 & \\frac{1}{2}\\end{bmatrix}$$ $$L_C = \\begin{bmatrix}0 & 0 & 0 & 1\\end{bmatrix} and finally, for page D, we can write L_D = \\begin{bmatrix} 0 & \\frac{1}{2} & \\frac{1}{2} & 0 \\end{bmatrix} L_D = \\begin{bmatrix} 0 & \\frac{1}{2} & \\frac{1}{2} & 0 \\end{bmatrix} We can now build our link matrix L L by using each of our linked vectors as a column, which you can see will form a square matrix. L = \\begin{bmatrix} 0 & \\frac{1}{2} & 0 & 0 \\\\\\ \\frac{1}{3} & 0 & 0 & \\frac{1}{2} \\\\\\ \\frac{1}{3} & 0 & 0 & \\frac{1}{2} \\\\\\ \\frac{1}{3} & \\frac{1}{2} & 1 & 0 \\end{bmatrix} L = \\begin{bmatrix} 0 & \\frac{1}{2} & 0 & 0 \\\\\\ \\frac{1}{3} & 0 & 0 & \\frac{1}{2} \\\\\\ \\frac{1}{3} & 0 & 0 & \\frac{1}{2} \\\\\\ \\frac{1}{3} & \\frac{1}{2} & 1 & 0 \\end{bmatrix} What we're trying to represent here with our matrix L L is the probability of ending up on each of the pages. For example, the only way to get to A is by being at B. So you then need to know the probability of being at B, which you could've got to from either A or D. As you can see, this problem is self-referential, as the ranks on all the pages depend on all the others. Although we built our matrix from columns of outward links, we can see that the rows describe inward links normalized with respect to their page of origin. We can now write an expression which summarises the approach. We're going to use the vector r r to store the rank of all webpages. To calculate the rank of page A, you need to know three things about all other pages on the Internet. What's your rank? Do you link to page A? And how many outgoing links do you have in total? The following expression combines these three pieces of information for webpage A only. r_a = \\sum_{j=1}^n L_{a, j}r_j r_a = \\sum_{j=1}^n L_{a, j}r_j So this is going to scroll through each of our webpages. Which means that the rank of A is the sum of the ranks of all the pages which link to it, weighted by their specific link probability taken from matrix L L . Now we want to be able to write this expression for all pages and solve them simultaneously. Thinking back to our linear algebra, we can rewrite the above expression applied to all webpages as a simple matrix multiplication. So r = Lr r = Lr Clearly, we start off not knowing r r . So we simply assume that all the ranks are equally and normalise them by the total number of webpages in our analysis, which in this case is 4. So r = \\begin{bmatrix}\\frac{1}{4} \\\\ \\frac{1}{4} \\\\ \\frac{1}{4} \\\\ \\frac{1}{4}\\end{bmatrix} r = \\begin{bmatrix}\\frac{1}{4} \\\\ \\frac{1}{4} \\\\ \\frac{1}{4} \\\\ \\frac{1}{4}\\end{bmatrix} Then, each time you multiply r r by our matrix L L , this gives us an updated value for r r . So we can say that r^{i+1} = Lr^i r^{i+1} = Lr^i Applying this expression repeatedly means that we are solving this problem iteratively. Each time we do this, we update the values in r r until, eventually, r r stops changing. So now r r really does equal Lr Lr . Thinking back to the previous videos in this module, this implies that r r is now an eigenvector of matrix L L , with an eigenvalue of 1. At this point, you might well be thinking, if we want to multiply r r by L L many times, perhaps this will be best tackled by applying the diagonalization method that we saw in the last video. But don't forget, this would require us to already know all of the Eigen vectors, which is what we're trying to find in the first place. So now that we have an equation, and hopefully some idea of where it came from, we can ask our computer to iteratively apply it until it converges to find our rank vector. You can see that although it takes about ten iterations for the numbers to settle down, the order is already established after the first iteration. However, this is just an artifact of our system being so tiny. So now we have our result, which says that as Procrastinating Pat randomly clicks around our network, we'd expect them to spend about 40% of their time on page D. But only about 12% of their time on page A, with 24% on each of pages B and C. We now have our ranking, with D at the top and A at the bottom, and B and C equal in the middle. As it turns out, although there are many approaches for efficiently calculating eigenvectors that have been developed over the years, repeatedly multiplying a randomly selected initial guest vector by a matrix, which is called the power method , is still very effective for the page rank problem for two reasons. Firstly, although the power method will clearly only give you one eigenvector, when we know that there will be n n for an n n webpage system, it turns out that because of the way we've structured our link matrix, the vector it gives you will always be the one that you're looking for, with an eigenvalue of 1. Secondly, although this is not true for the full webpage mini Internet, when looking at the real Internet you can imagine that almost every entry in the link matrix will be zero, i.e,, most pages don't connect to most other pages. This is referred to as a sparse matrix . And algorithms exist such that multiplications can be performed very efficiently. One key aspect of the page rank algorithm that we haven't discussed so far is called the damping factor, d d . This adds an additional term to our iterative formula. So r^{i + 1} r^{i + 1} is now going to equal r^{i + 1}= d Lr^i + \\frac{1 - d}{n} r^{i + 1}= d Lr^i + \\frac{1 - d}{n} where d d is something between 0 and 1. And you can think of it as 1 minus the probability with which procrastinating Pat suddenly, randomly types in a web address, rather than clicking on a link on his current page. The effect that this has on the actual calculation is about finding a compromise between speed and stability of the iterative convergence process. There are over one billion websites on the Internet today, compared with just a few million when the page rank algorithm was first published in 1998. And so the methods for search and ranking have had to evolve to maximize efficiency, although the core concept has remained unchanged for many years. Conclusions This brings us to the end of our introduction to the page rank algorithm. There are, of course, many details which we didn't cover in this video. But I hope this has allowed you to come away with some insight and understanding into how the page rank works, and hopefully the confidence to apply this to some larger networks yourself. Summary This brings us to the end of the fifth module and also, to the end of this course on linear algebra for machine learning. We've covered a lot of ground in the past five modules, but I hope that we've managed to balance, the speed with the level of detail to ensure that you've stayed with us throughout. There is a tension at the heart of mathematics teaching in the computer age. Classical teaching approaches focused around working through lots of examples by hand without much emphasis on building intuition. However, computers now do nearly all of the calculation work for us, and it's not typical for the methods appropriate to hand calculation to be the same as those employed by a computer. This can mean that, despite doing lots of work, students can come away from a classical education missing both the detailed view of the computational methods, but also the high level view of what each method is really doing. The concepts that you've been exposed to over the last five modules cover the core of linear algebra. That you will need as you progress your study of machine learning. And we hope that at the very least, when you get stuck in the future, you'll know the appropriate language. So that you can quickly look up some help when you need it. Which, after all, is the most important skill of a professional coder.","title":"Week 5"},{"location":"linear_algebra/week_5/#week-5-eigenvalues-and-eigenvectors","text":"Eigenvectors are particular vectors that are unrotated by a transformation matrix (i.e., they remain on their own span ) and eigenvalues are the amount by which the eigenvectors are scaled. These special 'eigen-things' are very useful in linear algebra and will let us examine Google's famous PageRank algorithm for presenting web search results. Then we'll apply this in code, which will wrap up the course. Tip It is best to watch this video first, then return to and read through this section. If you want even more exposure to these ideas, try the first three sections of the Khan Academy course here .","title":"Week 5: Eigenvalues and Eigenvectors"},{"location":"linear_algebra/week_5/#learning-objectives","text":"Identify geometrically what an eigenvector/value is Apply mathematical formulation in simple cases Build an intuition of larger dimensional eigensystems Write code to solve a large dimensional eigen problem","title":"Learning Objectives"},{"location":"linear_algebra/week_5/#what-are-eigen-things","text":"","title":"What are eigen-things?"},{"location":"linear_algebra/week_5/#what-are-eigenvalues-and-eigenvectors","text":"The word, \"eigen\" is perhaps most usefully translated from German as meaning characteristic . So when we talk about an eigenproblem , we're talking about finding the characteristic properties of something . But characteristic of what? This module, like the previous weeks, will try and explain this concept of \"eigen-ness\" primarily through a geometric interpretation, which allows us to discuss images rather than immediately getting tangled up in the math. Note This topic is often considered by students to be quite tricky. But once you know how to sketch these problems, the rest is just algebra. As you've seen from previous weeks, it's possible to express the concept of linear transformations using matrices . These operations can include scalings , rotations , and shears . Often, when applying these transformations, we are thinking about what they might do to a specific vector . However, it can also be useful to think about what it might look like when they are applied to every vector in this space. This is most easily visualized by drawing a square centered at the origin, and then observing how the square is distorted when you apply the transformation. For example, if we apply a scaling of 2 in the vertical direction, the square would become a rectangle. Whereas, if we applied a horizontal shear to this space, it would become a trapezoid: Now, here's the key concept. Notice that, after the transformation is applied, some vectors end up lying on the same line that they started on whereas, others do not. To highlight this, lets draw three specific vectors onto our initial square. Now, consider our vertical scaling again, and think about what will happen to these three vectors. As you can see, the horizontal green vector is unchanged, i.e., it is pointing in the same direction and having the same length. The vertical pink vector is also still pointing in the same direction as before but its length is doubled. Lastly, the diagonal orange vector used to be exactly 45 degrees to the axis, but it's angle has now increased as has its length. Besides the horizontal and vertical vectors, any other vectors' direction would have been changed by this vertical scaling. So in some sense, the horizontal and vertical vectors are special , they are characteristic of this particular transformation. These are our eigenvectors , and the value they are scaled by is know as an eigenvalue . Note From a conceptual perspective, that's about it for 2D eigen-problems, we simply take a transformation and we look for the vectors who are still laying on the same span as before, and then we measure how much their length has changed. This is basically what eigenvectors and their corresponding eigenvalues are. Let's look at two more classic examples to make sure that we can generalize what we've learned. First, let look look at pure shear , where pure means that we aren't performing any scaling or rotation in addition, so the area is unchanged: Notice that it's only the green horizontal line that is still laying along its original span, and all the other vectors will be shifted Finally, let's look at rotation . Clearly, this thing has got no eigenvectors at all, as all of the vectors have been rotated off their original span: Conclusions In this lecture, we've already covered almost all of what you need to know about eigenvectors and eigenvalues. Although we've only been working in two dimensions so far, the concept is exactly the same in three or more dimensions. In the rest of the module, we'll have a look at some special cases, as well as discussing how to describe what we've observed in more mathematical terms.","title":"What are eigenvalues and eigenvectors?"},{"location":"linear_algebra/week_5/#getting-into-the-detail-of-eigenproblems","text":"","title":"Getting into the detail of eigenproblems"},{"location":"linear_algebra/week_5/#special-eigen-cases","text":"As we saw previously, eigenvectors are those which lie along the same span both before and after applying a linear transform to a space. Eigenvalues are simply the amount that each of those vectors has been stretched in the process. In this section, we're going to look at three special cases to make sure the intuition we've built so far is robust, and then we're going to try and extend this concept into three dimensions. The first example we're going to consider is that of a uniform scaling, which is where we scale by the same amount in each direction: As you will hopefully have spotted, not only are all three of the vectors that we've highlighted eigenvectors, but in fact, for a uniform scaling, any vector would be an eigenvector. In this second example, we're going to look at rotation. In the previous section, we applied a small rotation, and we found that it had no eigenvectors. However, there is one case of non-zero pure rotation which does have at least some eigenvectors, and that is 180 180 degrees: As you can see, the three eigenvectors are still laying on the same spans as before, but pointing in the opposite direction. This means that once again, all vectors for this transform are eigenvectors, and they all have eigenvalues of -1 -1 , which means that although the eigenvectors haven't changed length, they are all now pointing in the opposite direction. In this third case, we're going to look at a combination of a horizontal shear and a vertical scaling , and it's slightly less obvious than some of the previous examples. Just like the pure shear case we saw previously, the green horizontal vector is an eigenvector and its eigenvalue is still 1 1 . However, despite the fact that neither of the other two vectors shown are eigen, this transformation does have two eigenvectors: Let's apply the inverse transform and watch our parallelogram go back to its original square. But this time, with our other eigenvector visible. Hopefully, you're at least convinced that it is indeed an eigenvector as it stays on its own span: This shows us that while the concept of eigenvectors is fairly straightforward, eigenvectors aren't always easy to spot. This problem is even tougher in three or more dimensions, and many of the uses of eigen theory in machine learning frame the system as being composed of hundreds of dimensions or more. So, clearly, we're going to need a more robust mathematical description of this concept to allow us to proceed. Before we do, let's take a quick look at one example in 3D. Clearly, scaling and shear are all going to operate much the same way in 3D as they do in 2D. However, rotation does take on a neat new meaning. As you can see from the image, although both the pink and green vectors have changed direction, the orange vector has not moved. This means that the orange vector is an eigenvector, but it also tells us, as a physical interpretation, that if we find the eigenvector of a 3D rotation, it means we've also found the axis of rotation . In this video, we've covered a range of special cases, which I hope have prompted the questions in your mind about how we're going to go about writing a formal definition of an eigen-problem.","title":"Special eigen-cases"},{"location":"linear_algebra/week_5/#calculating-eigenvectors","text":"At this point, we should now have a reasonable feeling for what an eigen-problem looks like, at least geometrically. In this section, we're going to formalize this concept into an algebraic expression, which will allow us to calculate eigenvalues and eigenvectors whenever they exist. Consider a transformation A A . An eigenvector of this transformation is any vector that can be written as a scaled version of itself after the transformation is applied, i.e., Ax = \\lambda x Ax = \\lambda x This expression captures the idea that applying the transformation A A to an eigenvector x x is the same as scaling that eigenvector x x by some number, \\lambda \\lambda (the eigenvalue). In order to solve for the eigenvectors of the transformation A A , we need to find values of x x that make the two sides equal. To help us find the solutions to this expression, we can rewrite it by putting all the terms on one side and then factorizing (A - \\lambda I) x = 0 (A - \\lambda I) x = 0 Note If you're wondering where the I I term came from, it's just an n \\times n n \\times n identity matrix. We didn't need this in the first expression we wrote, as multiplying vectors by scalars is defined. However, subtracting scalars from matrices is not defined , so the I I just tidies up the math, without changing the meaning. Now that we have this expression, we can see that for the left-hand side to equal 0 0 , either the contents of the brackets must be 0 0 or the vector x x must be 0 0 . As it turns out, we're not interested in the case where the vector x x is 0 0 , i.e., when it has no length or direction, as this represents a trivial solution . Instead, we are interested in the case where the term in brackets is 0 0 . Referring back to the material in the previous parts of the course, we can test if a matrix operation will result in a 0 0 output by calculating its determinant det (A - \\lambda I) = 0 det (A - \\lambda I) = 0 Calculating the determinants manually is a lot of work for high dimensional matrices. So let's try applying this to an arbitrary 2 \\times 2 2 \\times 2 transformation. Let A = \\begin{pmatrix}a & b \\\\ c & d\\end{pmatrix} A = \\begin{pmatrix}a & b \\\\ c & d\\end{pmatrix} substituting this into our eigen-finding expression gives the following: det \\Biggl( \\begin{pmatrix}a & b \\\\\\ c & d\\end{pmatrix}- \\begin{pmatrix}\\lambda & 0 \\\\\\ 0 & \\lambda\\end{pmatrix} \\Biggl ) = 0 det \\Biggl( \\begin{pmatrix}a & b \\\\\\ c & d\\end{pmatrix}- \\begin{pmatrix}\\lambda & 0 \\\\\\ 0 & \\lambda\\end{pmatrix} \\Biggl ) = 0 Evaluating this determinant, we get what is referred to as the characteristic polynomial , which looks like this \\lambda^2 - (a + d) \\lambda + ad - bc = 0 \\lambda^2 - (a + d) \\lambda + ad - bc = 0 Our eigenvalues are simply the solutions of this equation. Once we solve for them, we can then plug them back into the original expression to calculate our eigenvectors. Click the dropdown below for a fully-worked out solution to computing eigenvalues and eigenvectors. Example Let's take the case of a vertical scaling by a factor of 2 2 , which is represented by the transformation matrix A = \\begin{pmatrix} 1 & 0 \\\\\\ 0 & 2\\end{pmatrix} A = \\begin{pmatrix} 1 & 0 \\\\\\ 0 & 2\\end{pmatrix} We start with our equation for finding eigenvalues Ax = \\lambda x Ax = \\lambda x Rearranging, we get (A-\\lambda I)x = 0 (A-\\lambda I)x = 0 Solving (A-\\lambda I) = 0 (A-\\lambda I) = 0 is equivalent to asking when the determinant of the matrix is 0 0 det((A- \\lambda I)) = 0 det((A- \\lambda I)) = 0 Subbing in our matrix A A and solving the resulting characteristic polynomial det \\begin{pmatrix} 1 - \\lambda & 0 \\\\\\ 0 & 2- \\lambda \\end{pmatrix} = (1 - \\lambda)(2-\\lambda) = 0 det \\begin{pmatrix} 1 - \\lambda & 0 \\\\\\ 0 & 2- \\lambda \\end{pmatrix} = (1 - \\lambda)(2-\\lambda) = 0 This means that our equation must have solutions at \\lambda = 1 \\lambda = 1 and \\lambda = 2 \\lambda = 2 . Thinking back to our original eigen-finding formula, (A - \\lambda I)x = 0 (A - \\lambda I)x = 0 , we can now sub these two solutions back in. Thinking about the case where \\lambda = 1 \\lambda = 1 , @\\lambda = 1: \\begin{pmatrix} 1 - 1 & 0 \\\\\\ 0 & 2 - 1 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} 0 \\\\\\ x_2\\end{pmatrix} = 0 @\\lambda = 1: \\begin{pmatrix} 1 - 1 & 0 \\\\\\ 0 & 2 - 1 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} 0 \\\\\\ x_2\\end{pmatrix} = 0 Now, thinking about the case where \\lambda = 2 \\lambda = 2 , @\\lambda = 2: \\begin{pmatrix} 1 - 2 & 0 \\\\\\ 0 & 2 - 2 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} -1 & 0 \\\\\\ 0 & 0 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} -x_1 \\\\\\ 0 \\end{pmatrix} = 0 @\\lambda = 2: \\begin{pmatrix} 1 - 2 & 0 \\\\\\ 0 & 2 - 2 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} -1 & 0 \\\\\\ 0 & 0 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\\\ x_2\\end{pmatrix} = \\begin{pmatrix} -x_1 \\\\\\ 0 \\end{pmatrix} = 0 So what do these two expressions tell us? Well, in the case where our eigenvalue \\lambda = 1 \\lambda = 1 , we've got an eigenvector where the x_2 x_2 term must be zero. But we don't really know anything about the x_1 x_1 term. Well, this is because any vector that points along the horizontal axis could be an eigenvector of this system . We say that by writing @\\lambda = 1: \\begin{pmatrix} t \\\\\\ 0 \\end{pmatrix} @\\lambda = 1: \\begin{pmatrix} t \\\\\\ 0 \\end{pmatrix} using an arbitrary parameter t t . Similarly for the \\lambda = 2 \\lambda = 2 case, we can say that our eigenvector must equal @\\lambda = 2: \\begin{pmatrix} 0 \\\\\\ t \\end{pmatrix} @\\lambda = 2: \\begin{pmatrix} 0 \\\\\\ t \\end{pmatrix} because as long as it doesn't move at all in the horizontal direction, any vector that's purely vertical would be an eigenvector of this system, as they would lie along the same span. So now we have two eigenvalues, and their two corresponding eigenvectors.","title":"Calculating eigenvectors"},{"location":"linear_algebra/week_5/#conclusions","text":"Despite all the fun that we've just been having, the truth is that you will almost certainly never have to perform this calculation by hand. Note Indeed, with libraries like numpy this is as easy as import numpy as np A = np . array ([ 1 , 0 ], [ 0 , 2 ]) eigenvalues , eigenvectors = numpy . linalg . eig ( A ) Furthermore, we saw that our approach required finding the roots of a polynomial of order n n , i.e., the dimension of your matrix, which means that the problem will very quickly stop being possible by analytical methods alone. When a computer finds the eigensolutions of a 100 dimensional problem it's forced to employ iterative numerical methods. Therefore, developing a strong conceptual understanding of eigen problems will be much more useful than being really good at calculating them by hand. In this sections, we translated our geometrical understanding of eigenvectors into a robust mathematical expression, and validated it on a few test cases. But I hope that I've also convinced you that working through lots of eigen-problems, as is often done in engineering undergraduate degrees, is not a good investment of your time if you already understand the underlying concepts. This is what computers are for. Next video, we'll be referring back to the concept of basis change to see what magic happens when you use eigenvectors as your basis. See you then.","title":"Conclusions"},{"location":"linear_algebra/week_5/#when-changing-to-the-eigenbasis-is-really-useful","text":"","title":"When changing to the eigenbasis is really useful"},{"location":"linear_algebra/week_5/#changing-to-the-eigenbasis","text":"So now that we know what eigenvectors are and how to calculate them, we can combine this idea with a concept of changing basis which was covered earlier in the course. What emerges from this synthesis is a particularly powerful tool for performing efficient matrix operations called diagonalisation . Sometimes, we need to apply the same matrix multiplication many times. For example, imagine a transformation matrix T T represents the change in location of a particle after a single time step. So we can write that our initial position, described by vector v_0 v_0 , multiplied by the transformation T T gives us our new location, v_1 v_1 . To work out where our particle will be after two time steps, we can find v_2 v_2 by simply multiplying v_1 v_1 by T T , which is of course the same thing as multiplying v_0 v_0 by T T two times. So v_2 = T^2 v_0 v_2 = T^2 v_0 . Now imagine that we expect the same linear transformation to occur every time step for n n time steps. We can write this transformation as v_n = T^n v_0 v_n = T^n v_0 You've already seen how much work it takes to apply a single 3D matrix multiplication. So if we were to imagine that T T tells us what happens in one second, but we'd like to know where our particle is in two weeks from now, then n n is going to be around 1.2 million, i.e., we'd need to multiply T T by itself more than a million times, which may take quite a while. If all the terms in the matrix are zero except for those along the leading diagonal, we refer to it as a diagonal matrix . When raising matrices to powers, diagonal matrices make things a lot easier. All you need to do is put each of the terms on the diagonal to the power of n n and you've got the answer. So in this case, T^n = \\begin{pmatrix} a^n & 0 & 0 \\\\\\ 0 & b^n & 0 \\\\\\ 0 & 0 & c^n\\end{pmatrix} T^n = \\begin{pmatrix} a^n & 0 & 0 \\\\\\ 0 & b^n & 0 \\\\\\ 0 & 0 & c^n\\end{pmatrix} It's simple enough, but what if T T is not a diagonal matrix? Well, as you may have guessed, the answer comes from eigen-analysis. Essentially, what we're going to do is simply change to a basis where our transformation T T becomes diagonal, which is what we call an eigen-basis. We can then easily apply our power of n n to the diagonalized form, and finally transform the resulting matrix back again, giving us T^n T^n , but avoiding much of the work. As we saw in the section on changing basis, each column of our transform matrix simply represents the new location of the transformed unit vectors. So, to build our eigen-basis conversion matrix, we just plug in each of our eigenvectors as columns: C = \\begin{pmatrix}x_1 & x_2 & x_3 \\\\\\ \\vdots & \\vdots & \\vdots\\end{pmatrix} C = \\begin{pmatrix}x_1 & x_2 & x_3 \\\\\\ \\vdots & \\vdots & \\vdots\\end{pmatrix} Note However, don't forget that some of these maybe complex, so not easy to spot using the purely geometrical approach, but they are appear in the math just like the others. Applying this transform, we find ourselves in a world where multiplying by T T is effectively just a pure scaling, which is another way of saying that it can now be represented by a diagonal matrix. Crucially, this diagonal matrix, D D , contains the corresponding eigenvalues of the matrix T T . So, D = \\begin{pmatrix} \\lambda_1 & 0 & 0 \\\\\\ 0 & \\lambda_2 & 0 \\\\\\ 0 & 0 & \\lambda_3\\end{pmatrix} D = \\begin{pmatrix} \\lambda_1 & 0 & 0 \\\\\\ 0 & \\lambda_2 & 0 \\\\\\ 0 & 0 & \\lambda_3\\end{pmatrix} We're so close now to unleashing the power of eigen. The final link that we need to see is the following. Bringing together everything we've just said, it should now be clear that applying the transformation T T is just the same as converting to our eigenbasis, applying the diagonalized matrix, and then converting back again. So T = CDC^{-1} T = CDC^{-1} <span><span class=\"MathJax_Preview\">T = CDC^{-1}</span><script type=\"math/tex\">T = CDC^{-1} which suggests that T^2 = CDC^{-1}CDC^{-1} = CDDC^{-1} = CD^2C^{-1} T^2 = CDC^{-1}CDC^{-1} = CDDC^{-1} = CD^2C^{-1} <span><span class=\"MathJax_Preview\">T^2 = CDC^{-1}CDC^{-1} = CDDC^{-1} = CD^2C^{-1}</span><script type=\"math/tex\">T^2 = CDC^{-1}CDC^{-1} = CDDC^{-1} = CD^2C^{-1} So hopefully you've spotted that in the middle of our expression on the right-hand side, you've got C C multiplied by C inverse. But multiplying a matrix and then by its inverse is just the same as doing nothing at all. So we can simply remove this operation. And then we can finish this expression by saying, well this must be CD^2C^{-1} CD^2C^{-1} . We can of course then generalize this to any power of T T we'd like. So finally we can say that, T^n = CD^nC^{-1} T^n = CD^nC^{-1} We now have a method which lets us apply a transformation matrix as many times as we'd like without paying a large computational cost. Conclusions This result brings together many of the ideas that we've encountered so far in this course. Check out this video, where we'll work through a short example just to ensure that this approach lines up with our expectations when applied to a simple case.","title":"Changing to the eigenbasis"},{"location":"linear_algebra/week_5/#making-the-pagerank-algorithm","text":"","title":"Making the PageRank algorithm"},{"location":"linear_algebra/week_5/#pagerank","text":"The final topic of this module on Eigenproblems, as well as the final topic of this course as a whole, will focus on an algorithm called PageRank . This algorithm was famously published by and named after Google founder Larry Page and colleagues in 1998. And was used by Google to help them decide which order to display their websites when they returned from search. The central assumption underpinning page rank is that the importance of a website is related to its links to and from other websites, and somehow Eigen theory comes up. This bubble diagram represents a model mini Internet, where each bubble is a webpage and each arrow from A, B, C, and D represents a link on that webpage which takes you to one of the others. We're trying to build an expression that tells us, based on this network structure, which of these webpages is most relevant to the person who made the search. As such, we're going to use the concept of Procrastinating Pat who is an imaginary person who goes on the Internet and just randomly click links to avoid doing their work. By mapping all the possible links, we can build a model to estimate the amount of time we would expect Pat to spend on each webpage. We can describe the links present on page A as a vector, where each row is either a one or a zero based on whether there is a link to the corresponding page. And then normalise the vector by the total number of the links, such that they can be used to describe a probability for that page. For example, the vector of links from page A will be \\begin{bmatrix} 0 & 1 & 1 & 1\\end{bmatrix} \\begin{bmatrix} 0 & 1 & 1 & 1\\end{bmatrix} because vector A has links to sites B, to C, and to D, but it doesn't have a link to itself. Also, because there are three links in this page in total, we would normalize by a factor of a third. So the total click probability sums to one. So we can write, L_A = \\begin{bmatrix}0 & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3}\\end{bmatrix} L_A = \\begin{bmatrix}0 & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3}\\end{bmatrix} Following the same logic, the link vectors in the next two sites are shown here: L_B = \\begin{bmatrix} \\frac{1}{2} & 0 & 0 & \\frac{1}{2}\\end{bmatrix}$$ $$L_C = \\begin{bmatrix}0 & 0 & 0 & 1\\end{bmatrix} L_B = \\begin{bmatrix} \\frac{1}{2} & 0 & 0 & \\frac{1}{2}\\end{bmatrix}$$ $$L_C = \\begin{bmatrix}0 & 0 & 0 & 1\\end{bmatrix} and finally, for page D, we can write L_D = \\begin{bmatrix} 0 & \\frac{1}{2} & \\frac{1}{2} & 0 \\end{bmatrix} L_D = \\begin{bmatrix} 0 & \\frac{1}{2} & \\frac{1}{2} & 0 \\end{bmatrix} We can now build our link matrix L L by using each of our linked vectors as a column, which you can see will form a square matrix. L = \\begin{bmatrix} 0 & \\frac{1}{2} & 0 & 0 \\\\\\ \\frac{1}{3} & 0 & 0 & \\frac{1}{2} \\\\\\ \\frac{1}{3} & 0 & 0 & \\frac{1}{2} \\\\\\ \\frac{1}{3} & \\frac{1}{2} & 1 & 0 \\end{bmatrix} L = \\begin{bmatrix} 0 & \\frac{1}{2} & 0 & 0 \\\\\\ \\frac{1}{3} & 0 & 0 & \\frac{1}{2} \\\\\\ \\frac{1}{3} & 0 & 0 & \\frac{1}{2} \\\\\\ \\frac{1}{3} & \\frac{1}{2} & 1 & 0 \\end{bmatrix} What we're trying to represent here with our matrix L L is the probability of ending up on each of the pages. For example, the only way to get to A is by being at B. So you then need to know the probability of being at B, which you could've got to from either A or D. As you can see, this problem is self-referential, as the ranks on all the pages depend on all the others. Although we built our matrix from columns of outward links, we can see that the rows describe inward links normalized with respect to their page of origin. We can now write an expression which summarises the approach. We're going to use the vector r r to store the rank of all webpages. To calculate the rank of page A, you need to know three things about all other pages on the Internet. What's your rank? Do you link to page A? And how many outgoing links do you have in total? The following expression combines these three pieces of information for webpage A only. r_a = \\sum_{j=1}^n L_{a, j}r_j r_a = \\sum_{j=1}^n L_{a, j}r_j So this is going to scroll through each of our webpages. Which means that the rank of A is the sum of the ranks of all the pages which link to it, weighted by their specific link probability taken from matrix L L . Now we want to be able to write this expression for all pages and solve them simultaneously. Thinking back to our linear algebra, we can rewrite the above expression applied to all webpages as a simple matrix multiplication. So r = Lr r = Lr Clearly, we start off not knowing r r . So we simply assume that all the ranks are equally and normalise them by the total number of webpages in our analysis, which in this case is 4. So r = \\begin{bmatrix}\\frac{1}{4} \\\\ \\frac{1}{4} \\\\ \\frac{1}{4} \\\\ \\frac{1}{4}\\end{bmatrix} r = \\begin{bmatrix}\\frac{1}{4} \\\\ \\frac{1}{4} \\\\ \\frac{1}{4} \\\\ \\frac{1}{4}\\end{bmatrix} Then, each time you multiply r r by our matrix L L , this gives us an updated value for r r . So we can say that r^{i+1} = Lr^i r^{i+1} = Lr^i Applying this expression repeatedly means that we are solving this problem iteratively. Each time we do this, we update the values in r r until, eventually, r r stops changing. So now r r really does equal Lr Lr . Thinking back to the previous videos in this module, this implies that r r is now an eigenvector of matrix L L , with an eigenvalue of 1. At this point, you might well be thinking, if we want to multiply r r by L L many times, perhaps this will be best tackled by applying the diagonalization method that we saw in the last video. But don't forget, this would require us to already know all of the Eigen vectors, which is what we're trying to find in the first place. So now that we have an equation, and hopefully some idea of where it came from, we can ask our computer to iteratively apply it until it converges to find our rank vector. You can see that although it takes about ten iterations for the numbers to settle down, the order is already established after the first iteration. However, this is just an artifact of our system being so tiny. So now we have our result, which says that as Procrastinating Pat randomly clicks around our network, we'd expect them to spend about 40% of their time on page D. But only about 12% of their time on page A, with 24% on each of pages B and C. We now have our ranking, with D at the top and A at the bottom, and B and C equal in the middle. As it turns out, although there are many approaches for efficiently calculating eigenvectors that have been developed over the years, repeatedly multiplying a randomly selected initial guest vector by a matrix, which is called the power method , is still very effective for the page rank problem for two reasons. Firstly, although the power method will clearly only give you one eigenvector, when we know that there will be n n for an n n webpage system, it turns out that because of the way we've structured our link matrix, the vector it gives you will always be the one that you're looking for, with an eigenvalue of 1. Secondly, although this is not true for the full webpage mini Internet, when looking at the real Internet you can imagine that almost every entry in the link matrix will be zero, i.e,, most pages don't connect to most other pages. This is referred to as a sparse matrix . And algorithms exist such that multiplications can be performed very efficiently. One key aspect of the page rank algorithm that we haven't discussed so far is called the damping factor, d d . This adds an additional term to our iterative formula. So r^{i + 1} r^{i + 1} is now going to equal r^{i + 1}= d Lr^i + \\frac{1 - d}{n} r^{i + 1}= d Lr^i + \\frac{1 - d}{n} where d d is something between 0 and 1. And you can think of it as 1 minus the probability with which procrastinating Pat suddenly, randomly types in a web address, rather than clicking on a link on his current page. The effect that this has on the actual calculation is about finding a compromise between speed and stability of the iterative convergence process. There are over one billion websites on the Internet today, compared with just a few million when the page rank algorithm was first published in 1998. And so the methods for search and ranking have had to evolve to maximize efficiency, although the core concept has remained unchanged for many years. Conclusions This brings us to the end of our introduction to the page rank algorithm. There are, of course, many details which we didn't cover in this video. But I hope this has allowed you to come away with some insight and understanding into how the page rank works, and hopefully the confidence to apply this to some larger networks yourself.","title":"PageRank"},{"location":"linear_algebra/week_5/#summary","text":"This brings us to the end of the fifth module and also, to the end of this course on linear algebra for machine learning. We've covered a lot of ground in the past five modules, but I hope that we've managed to balance, the speed with the level of detail to ensure that you've stayed with us throughout. There is a tension at the heart of mathematics teaching in the computer age. Classical teaching approaches focused around working through lots of examples by hand without much emphasis on building intuition. However, computers now do nearly all of the calculation work for us, and it's not typical for the methods appropriate to hand calculation to be the same as those employed by a computer. This can mean that, despite doing lots of work, students can come away from a classical education missing both the detailed view of the computational methods, but also the high level view of what each method is really doing. The concepts that you've been exposed to over the last five modules cover the core of linear algebra. That you will need as you progress your study of machine learning. And we hope that at the very least, when you get stuck in the future, you'll know the appropriate language. So that you can quickly look up some help when you need it. Which, after all, is the most important skill of a professional coder.","title":"Summary"}]}